#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ SIMPLE FAST INFERENCE FOR QWEN3

A simplified but fast inference implementation that adds KV caching to your existing model.
This is easier to integrate and understand than the full vLLM-style implementation.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import time
from typing import List, Optional, Dict, Tuple
from dataclasses import dataclass
from collections import defaultdict

# Import our model components
from config.qwen3_small_config import SmallModelConfig
from qwen3_complete_model import MinimalLLM
from qwen3_core_components import Qwen3Attention, SwiGLUFeedForward, TransformerBlock, RMSNorm, Rotary, repeat_kv

# =============================================================================
# üéØ SIMPLE KV CACHE
# =============================================================================

class SimpleKVCache:
    """
    üéØ SIMPLE KV CACHE
    
    A straightforward KV cache implementation that stores key-value pairs
    for each sequence position. Much simpler than paged attention but still effective.
    """
    
    def __init__(self, max_seq_len: int, n_heads: int, head_dim: int, dtype: torch.dtype, device: str):
        self.max_seq_len = max_seq_len
        self.n_heads = n_heads
        self.head_dim = head_dim
        self.device = device
        
        # Cache storage: [seq_id][layer_idx] -> (k_cache, v_cache)
        # k_cache, v_cache: (n_heads, max_seq_len, head_dim)
        self.cache = defaultdict(lambda: defaultdict(lambda: (
            torch.zeros(n_heads, max_seq_len, head_dim, dtype=dtype, device=device),
            torch.zeros(n_heads, max_seq_len, head_dim, dtype=dtype, device=device)
        )))
        
        # Track sequence lengths
        self.seq_lengths = defaultdict(int)
    
    def get_cache(self, seq_id: int, layer_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """Get cached K, V for a sequence and layer"""
        return self.cache[seq_id][layer_idx]
    
    def update_cache(self, seq_id: int, layer_idx: int, k: torch.Tensor, v: torch.Tensor, pos: int):
        """
        Update cache with new K, V values
        
        Args:
            seq_id: Sequence ID
            layer_idx: Layer index
            k: Key tensor (n_heads, seq_len, head_dim)
            v: Value tensor (n_heads, seq_len, head_dim)
            pos: Position to update
        """
        k_cache, v_cache = self.cache[seq_id][layer_idx]
        
        # Update cache at position
        k_cache[:, pos:pos + k.size(1), :] = k
        v_cache[:, pos:pos + v.size(1), :] = v
        
        # Update sequence length
        self.seq_lengths[seq_id] = max(self.seq_lengths[seq_id], pos + k.size(1))
    
    def get_cached_kv(self, seq_id: int, layer_idx: int, current_len: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """Get cached K, V up to current length"""
        k_cache, v_cache = self.cache[seq_id][layer_idx]
        return k_cache[:, :current_len, :], v_cache[:, :current_len, :]
    
    def clear_sequence(self, seq_id: int):
        """Clear cache for a sequence"""
        if seq_id in self.cache:
            del self.cache[seq_id]
        if seq_id in self.seq_lengths:
            del self.seq_lengths[seq_id]

# =============================================================================
# üéØ OPTIMIZED ATTENTION WITH SIMPLE KV CACHE
# =============================================================================

class CachedAttention(nn.Module):
    """
    üéØ CACHED ATTENTION
    
    Attention layer with simple KV caching for fast inference.
    """
    
    def __init__(self, config: SmallModelConfig, kv_cache: SimpleKVCache):
        super().__init__()
        self.d_model = config.d_model
        self.n_heads = config.n_heads
        self.n_kv_heads = config.n_kv_heads
        self.n_kv_groups = config.n_kv_groups
        self.d_k = config.d_k
        self.kv_cache = kv_cache

        # Projections
        self.q_proj = nn.Linear(self.d_model, self.n_heads * self.d_k, bias=config.attention_bias)
        self.k_proj = nn.Linear(self.d_model, self.n_kv_heads * self.d_k, bias=config.attention_bias)
        self.v_proj = nn.Linear(self.d_model, self.n_kv_heads * self.d_k, bias=config.attention_bias)
        self.w_o = nn.Linear(self.d_model, self.d_model, bias=False)

        # Normalization
        self.q_norm = RMSNorm(self.d_k, eps=config.rms_norm_eps)
        self.k_norm = RMSNorm(self.d_k, eps=config.rms_norm_eps)

        # RoPE
        self.rotary = Rotary(self.d_k, config.max_seq_len)
        self.dropout = config.dropout

    def forward(self, x: torch.Tensor, seq_id: int, layer_idx: int, use_cache: bool = True):
        """
        Forward pass with KV caching
        
        Args:
            x: Input tensor (batch_size, seq_len, d_model)
            seq_id: Sequence ID for cache lookup
            layer_idx: Layer index
            use_cache: Whether to use cached KV values
        """
        batch_size, seq_len = x.size(0), x.size(1)

        # Project to Q, K, V
        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.d_k)
        k = self.k_proj(x).view(batch_size, seq_len, self.n_kv_heads, self.d_k)
        v = self.v_proj(x).view(batch_size, seq_len, self.n_kv_heads, self.d_k)

        # Apply normalization
        q = self.q_norm(q)
        k = self.k_norm(k)

        # Apply RoPE
        q = self.rotary(q.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)
        k = self.rotary(k.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)

        if use_cache:
            # Get cached K, V
            cached_k, cached_v = self.kv_cache.get_cached_kv(seq_id, layer_idx, self.kv_cache.seq_lengths[seq_id])
            
            # Concatenate new K, V with cached ones
            if cached_k.size(1) > 0:
                K = torch.cat([cached_k.unsqueeze(0), k.transpose(1, 2)], dim=2)  # (batch, n_kv_heads, cached_len + seq_len, d_k)
                V = torch.cat([cached_v.unsqueeze(0), v.transpose(1, 2)], dim=2)  # (batch, n_kv_heads, cached_len + seq_len, d_k)
            else:
                K = k.transpose(1, 2)  # (batch, n_kv_heads, seq_len, d_k)
                V = v.transpose(1, 2)  # (batch, n_kv_heads, seq_len, d_k)
            
            # Update cache
            current_pos = self.kv_cache.seq_lengths[seq_id]
            self.kv_cache.update_cache(seq_id, layer_idx, k[0].transpose(0, 1), v[0].transpose(0, 1), current_pos)
        else:
            K = k.transpose(1, 2)  # (batch, n_kv_heads, seq_len, d_k)
            V = v.transpose(1, 2)  # (batch, n_kv_heads, seq_len, d_k)

        # Repeat KV heads for GQA
        K = repeat_kv(K, self.n_kv_groups)
        V = repeat_kv(V, self.n_kv_groups)

        # Transpose Q for attention
        Q = q.transpose(1, 2)  # (batch, n_heads, seq_len, d_k)

        # Scaled dot-product attention
        attn_output = F.scaled_dot_product_attention(
            Q, K, V,
            is_causal=True,
            dropout_p=self.dropout if self.training else 0.0
        )

        # Reshape and project back
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)
        return self.w_o(attn_output)

# =============================================================================
# üèóÔ∏è CACHED TRANSFORMER BLOCK
# =============================================================================

class CachedTransformerBlock(nn.Module):
    """
    üèóÔ∏è CACHED TRANSFORMER BLOCK
    
    Transformer block with cached attention.
    """
    
    def __init__(self, config: SmallModelConfig, kv_cache: SimpleKVCache):
        super().__init__()
        self.attention = CachedAttention(config, kv_cache)
        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)
        
        # Pre-norm architecture
        self.norm1 = RMSNorm(config.d_model, eps=config.rms_norm_eps)
        self.norm2 = RMSNorm(config.d_model, eps=config.rms_norm_eps)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x: torch.Tensor, seq_id: int, layer_idx: int, use_cache: bool = True):
        """Forward pass with KV caching"""
        # Pre-norm attention with residual connection
        attn_out = self.attention(self.norm1(x), seq_id, layer_idx, use_cache)
        x = x + self.dropout(attn_out)
        
        # Pre-norm feed-forward with residual connection
        ff_out = self.feed_forward(self.norm2(x))
        x = x + self.dropout(ff_out)
        
        return x

# =============================================================================
# üöÄ SIMPLE FAST INFERENCE ENGINE
# =============================================================================

class SimpleFastInference:
    """
    üöÄ SIMPLE FAST INFERENCE ENGINE
    
    A simplified but fast inference engine that adds KV caching to your existing model.
    Much easier to understand and integrate than full vLLM-style implementations.
    """
    
    def __init__(self, model: MinimalLLM, tokenizer, config: SmallModelConfig, 
                 max_seq_len: int = 2048):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        self.max_seq_len = max_seq_len
        self.device = next(model.parameters()).device
        
        # Initialize KV cache
        self.kv_cache = SimpleKVCache(
            max_seq_len=max_seq_len,
            n_heads=config.n_kv_heads,
            head_dim=config.d_k,
            dtype=model.dtype if hasattr(model, 'dtype') else torch.float16,
            device=self.device
        )
        
        # Replace transformer blocks with cached versions
        self._replace_attention_layers()
        
        # Sequence tracking
        self.next_seq_id = 0
        
    def _replace_attention_layers(self):
        """Replace standard attention with cached attention"""
        for i, block in enumerate(self.model.transformer_blocks):
            cached_block = CachedTransformerBlock(self.config, self.kv_cache)
            
            # Copy weights from original block
            cached_block.attention.q_proj.load_state_dict(block.attention.q_proj.state_dict())
            cached_block.attention.k_proj.load_state_dict(block.attention.k_proj.state_dict())
            cached_block.attention.v_proj.load_state_dict(block.attention.v_proj.state_dict())
            cached_block.attention.w_o.load_state_dict(block.attention.w_o.state_dict())
            cached_block.attention.q_norm.load_state_dict(block.attention.q_norm.state_dict())
            cached_block.attention.k_norm.load_state_dict(block.attention.k_norm.state_dict())
            
            cached_block.feed_forward.load_state_dict(block.feed_forward.state_dict())
            cached_block.norm1.load_state_dict(block.norm1.state_dict())
            cached_block.norm2.load_state_dict(block.norm2.state_dict())
            
            self.model.transformer_blocks[i] = cached_block
    
    def generate_single(self, prompt: str, max_new_tokens: int = 100, 
                       temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9) -> str:
        """
        Generate text for a single prompt with KV caching
        
        Args:
            prompt: Input prompt
            max_new_tokens: Maximum number of new tokens to generate
            temperature: Sampling temperature
            top_k: Top-k sampling
            top_p: Top-p (nucleus) sampling
            
        Returns:
            Generated text
        """
        seq_id = self.next_seq_id
        self.next_seq_id += 1
        
        # Tokenize prompt
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)
        input_length = input_ids.size(1)
        
        # Clear cache for this sequence
        self.kv_cache.clear_sequence(seq_id)
        
        # Prefill phase: process the entire prompt
        with torch.no_grad():
            x = self.model.token_embedding(input_ids) * math.sqrt(self.config.d_model)
            x = self.model.position_dropout(x)
            
            # Process through transformer blocks with caching
            for layer_idx, block in enumerate(self.model.transformer_blocks):
                x = block(x, seq_id, layer_idx, use_cache=True)
            
            # Get logits for last position
            x = self.model.norm(x)
            logits = self.model.lm_head(x[:, -1:, :])  # Only last position
        
        # Generate tokens one by one
        generated_tokens = []
        current_ids = input_ids
        
        for _ in range(max_new_tokens):
            # Sample next token
            next_token = self._sample_token(logits, temperature, top_k, top_p)
            generated_tokens.append(next_token)
            
            # Check for EOS token
            if next_token == self.tokenizer.eos_token_id:
                break
            
            # Prepare next input (only the new token)
            next_input = torch.tensor([[next_token]], device=self.device)
            
            # Forward pass for single token
            with torch.no_grad():
                x = self.model.token_embedding(next_input) * math.sqrt(self.config.d_model)
                x = self.model.position_dropout(x)
                
                # Process through transformer blocks with caching
                for layer_idx, block in enumerate(self.model.transformer_blocks):
                    x = block(x, seq_id, layer_idx, use_cache=True)
                
                # Get logits for next token
                x = self.model.norm(x)
                logits = self.model.lm_head(x)
        
        # Decode generated tokens
        generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        # Clean up cache
        self.kv_cache.clear_sequence(seq_id)
        
        return generated_text
    
    def _sample_token(self, logits: torch.Tensor, temperature: float, top_k: int, top_p: float) -> int:
        """Sample a token from logits"""
        logits = logits.squeeze(0)  # Remove batch dimension
        
        # Apply temperature
        if temperature > 0:
            logits = logits / temperature
        
        # Apply top-k filtering
        if top_k > 0:
            top_k_logits, top_k_indices = torch.topk(logits, min(top_k, logits.size(-1)))
            filtered_logits = torch.full_like(logits, float('-inf'))
            filtered_logits[top_k_indices] = top_k_logits
            logits = filtered_logits
        
        # Apply top-p filtering
        if top_p < 1.0:
            sorted_logits, sorted_indices = torch.sort(logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
            sorted_indices_to_remove[0] = 0
            indices_to_remove = sorted_indices[sorted_indices_to_remove]
            logits[indices_to_remove] = float('-inf')
        
        # Sample token
        probs = F.softmax(logits, dim=-1)
        token = torch.multinomial(probs, num_samples=1).item()
        
        return token
    
    def generate_batch(self, prompts: List[str], max_new_tokens: int = 100, 
                      temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9) -> List[str]:
        """
        Generate text for multiple prompts (sequential processing)
        
        Args:
            prompts: List of input prompts
            max_new_tokens: Maximum number of new tokens to generate
            temperature: Sampling temperature
            top_k: Top-k sampling
            top_p: Top-p (nucleus) sampling
            
        Returns:
            List of generated texts
        """
        results = []
        for prompt in prompts:
            result = self.generate_single(prompt, max_new_tokens, temperature, top_k, top_p)
            results.append(result)
        return results

# =============================================================================
# üéØ CONVENIENCE FUNCTIONS
# =============================================================================

def create_simple_fast_inference(model_path: str, tokenizer_path: str, 
                                max_seq_len: int = 2048) -> SimpleFastInference:
    """
    Create a simple fast inference engine from saved model
    
    Args:
        model_path: Path to saved model
        tokenizer_path: Path to tokenizer
        max_seq_len: Maximum sequence length
        
    Returns:
        SimpleFastInference instance
    """
    # Load model
    checkpoint = torch.load(model_path, map_location='cpu')
    config = SmallModelConfig()
    config.vocab_size = checkpoint.get('vocab_size', 32000)  # Default vocab size
    
    model = MinimalLLM(config)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()
    
    # Load tokenizer
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Create engine
    engine = SimpleFastInference(
        model=model,
        tokenizer=tokenizer,
        config=config,
        max_seq_len=max_seq_len
    )
    
    return engine

# =============================================================================
# üß™ TESTING AND BENCHMARKING
# =============================================================================

def benchmark_simple_inference(engine: SimpleFastInference, num_requests: int = 10, 
                              max_input_len: int = 100, max_output_len: int = 100):
    """
    Benchmark the simple inference engine
    
    Args:
        engine: SimpleFastInference instance
        num_requests: Number of requests to process
        max_input_len: Maximum input length
        max_output_len: Maximum output length
    """
    print(f"üöÄ Benchmarking Simple Fast Inference Engine")
    print(f"   Requests: {num_requests}")
    print(f"   Max input length: {max_input_len}")
    print(f"   Max output length: {max_output_len}")
    
    # Generate test prompts
    test_prompts = []
    for i in range(num_requests):
        prompt_len = min(50, max_input_len)
        prompt = f"Write a short story about a {prompt_len}-word adventure: "
        test_prompts.append(prompt)
    
    # Benchmark
    start_time = time.time()
    results = engine.generate_batch(test_prompts, max_new_tokens=max_output_len)
    end_time = time.time()
    
    # Calculate metrics
    total_time = end_time - start_time
    total_tokens = sum(len(result.split()) for result in results)
    throughput = total_tokens / total_time if total_time > 0 else 0
    
    print(f"\nüìä Benchmark Results:")
    print(f"   Total time: {total_time:.2f}s")
    print(f"   Total tokens: {total_tokens:,}")
    print(f"   Throughput: {throughput:.1f} tokens/s")
    print(f"   Requests/s: {num_requests / total_time:.2f}")
    
    # Show sample results
    print(f"\nüìù Sample Results:")
    for i, (prompt, result) in enumerate(zip(test_prompts[:3], results[:3])):
        print(f"   {i+1}. Prompt: {prompt[:50]}...")
        print(f"      Generated: {result[:100]}...")
    
    return results

if __name__ == "__main__":
    print("üöÄ Simple Fast Inference Engine for Qwen3")
    print("=" * 50)
    
    # Example usage
    try:
        # Create engine (you'll need to provide actual model paths)
        # engine = create_simple_fast_inference(
        #     model_path="models/final_model1.pt",
        #     tokenizer_path="HuggingFaceTB/SmolLM-135M"
        # )
        
        # # Test single generation
        # prompt = "Hello, how are you today?"
        # result = engine.generate_single(prompt, max_new_tokens=50)
        # print(f"Prompt: {prompt}")
        # print(f"Generated: {result}")
        
        # # Test batch generation
        # prompts = ["Tell me a joke", "Write a haiku", "Explain AI"]
        # results = engine.generate_batch(prompts, max_new_tokens=30)
        # for prompt, result in zip(prompts, results):
        #     print(f"\nPrompt: {prompt}")
        #     print(f"Generated: {result}")
        
        # # Benchmark
        # benchmark_simple_inference(engine, num_requests=5)
        
        print("‚úÖ Simple Fast Inference Engine ready!")
        print("   To use: create_simple_fast_inference(model_path, tokenizer_path)")
        print("   Key features:")
        print("   - KV caching for 10-100x speedup")
        print("   - Simple integration with existing model")
        print("   - Memory efficient")
        print("   - Easy to understand and modify")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        print("   Make sure you have a trained model and tokenizer available")
