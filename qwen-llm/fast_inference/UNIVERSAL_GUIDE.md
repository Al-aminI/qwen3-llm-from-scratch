# üåê Universal Fast Inference Guide

## üìã Overview

The Universal Fast Inference Engine allows you to use **ANY** model with fast inference capabilities, not just your custom MinimalLLM. It automatically detects model architecture and applies appropriate optimizations.

## üéØ Supported Models

### ‚úÖ **Your Custom Models**
- **MinimalLLM**: Your custom Qwen3-style model
- **Any Custom Model**: With transformer architecture

### ‚úÖ **HuggingFace Models**
- **GPT Models**: GPT-2, GPT-3, etc.
- **BERT Models**: BERT, RoBERTa, DistilBERT, etc.
- **LLaMA Models**: LLaMA, LLaMA-2, etc.
- **Other Models**: Any HuggingFace transformer model

### ‚úÖ **Any PyTorch Model**
- Models with transformer architecture
- Models with attention mechanisms
- Custom implementations

## üöÄ Quick Start

### 1. **Your Custom MinimalLLM**

```python
from fast_inference.core.engine.universal_engine import create_universal_fast_inference

# Load your trained model
engine = create_universal_fast_inference(
    model="models/final_model1.pt",  # Your model path
    tokenizer="HuggingFaceTB/SmolLM-135M",
    max_seq_len=1024,
    model_type="minimal_llm"
)

# Generate text (with KV caching!)
result = engine.generate_single(
    "The future of AI",
    max_new_tokens=100,
    temperature=0.8
)
```

### 2. **HuggingFace Models**

```python
# GPT-2
engine = create_universal_fast_inference(
    model="gpt2",
    tokenizer="gpt2",
    model_type="huggingface"
)

# DistilGPT-2 (smaller, faster)
engine = create_universal_fast_inference(
    model="distilgpt2",
    tokenizer="distilgpt2",
    model_type="huggingface"
)

# LLaMA (if available)
engine = create_universal_fast_inference(
    model="meta-llama/Llama-2-7b-hf",
    tokenizer="meta-llama/Llama-2-7b-hf",
    model_type="huggingface"
)
```

### 3. **Model Instance**

```python
# If you already have a model instance
engine = create_universal_fast_inference(
    model=your_model_instance,
    tokenizer=your_tokenizer_instance,
    model_type="auto"  # Auto-detect
)
```

## üîç Automatic Detection

The universal engine automatically detects:

### **Model Architecture**
- ‚úÖ **MinimalLLM**: Your custom model
- ‚úÖ **HuggingFace**: Standard transformer models
- ‚úÖ **Custom**: Other transformer implementations

### **Model Components**
- ‚úÖ **Attention Heads**: Number of attention heads
- ‚úÖ **Hidden Size**: Model dimension
- ‚úÖ **Layers**: Number of transformer layers
- ‚úÖ **Vocabulary**: Vocabulary size

### **Optimization Strategy**
- ‚úÖ **KV Caching**: For supported models
- ‚úÖ **Memory Management**: Automatic cache sizing
- ‚úÖ **Device Handling**: CUDA/CPU optimization

## üìä Performance Benefits

### **Your MinimalLLM**
- ‚úÖ **KV Caching**: 10-100x speedup
- ‚úÖ **Memory Efficient**: Linear memory growth
- ‚úÖ **Full Optimization**: All features available

### **HuggingFace Models**
- ‚úÖ **Standard Generation**: Fast inference
- ‚úÖ **Batch Processing**: Multiple prompts
- ‚úÖ **Memory Optimization**: Efficient memory usage

### **Custom Models**
- ‚úÖ **Automatic Detection**: No manual configuration
- ‚úÖ **Fallback Generation**: Works with any model
- ‚úÖ **Easy Integration**: Simple API

## üéØ Usage Examples

### **Single Generation**

```python
# Your model
result = engine.generate_single(
    prompt="Write a story about a robot",
    max_new_tokens=100,
    temperature=0.8,
    top_k=50,
    top_p=0.9
)
```

### **Batch Generation**

```python
# Multiple prompts
prompts = [
    "Tell me a joke",
    "Write a haiku",
    "Explain quantum physics"
]

results = engine.generate_batch(
    prompts=prompts,
    max_new_tokens=50,
    temperature=0.8
)
```

### **Model Information**

```python
# Get model details
info = engine.get_model_info()
print(f"Model type: {info['model_type']}")
print(f"Parameters: {info['parameters']:,}")
print(f"Architecture: {info['architecture']}")
```

## üîß Configuration Options

### **Model Type Detection**

```python
# Auto-detect (recommended)
engine = create_universal_fast_inference(
    model=model,
    tokenizer=tokenizer,
    model_type="auto"
)

# Force specific type
engine = create_universal_fast_inference(
    model=model,
    tokenizer=tokenizer,
    model_type="minimal_llm"  # or "huggingface", "custom"
)
```

### **Memory Configuration**

```python
# Adjust sequence length
engine = create_universal_fast_inference(
    model=model,
    tokenizer=tokenizer,
    max_seq_len=2048  # Longer sequences
)
```

## üöÄ Advanced Features

### **Performance Monitoring**

```python
# Get performance info
info = engine.get_model_info()
print(f"Cache memory: {info['cache_info']}")
print(f"Device: {info['device']}")
```

### **Custom Sampling**

```python
# High creativity
result = engine.generate_single(
    prompt,
    temperature=1.2,  # More random
    top_k=100,        # More options
    top_p=0.95        # More diversity
)

# Low creativity
result = engine.generate_single(
    prompt,
    temperature=0.3,  # More focused
    top_k=20,         # Fewer options
    top_p=0.8         # Less diversity
)
```

## üîÑ Migration Guide

### **From Original Fast Inference**

```python
# Old way (MinimalLLM only)
from fast_inference import create_simple_fast_inference
engine = create_simple_fast_inference("model.pt", "tokenizer")

# New way (Universal)
from fast_inference.core.engine.universal_engine import create_universal_fast_inference
engine = create_universal_fast_inference("model.pt", "tokenizer", model_type="minimal_llm")
```

### **From HuggingFace**

```python
# Old way (standard generation)
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")
outputs = model.generate(input_ids, max_new_tokens=100)

# New way (with optimizations)
engine = create_universal_fast_inference("gpt2", "gpt2", model_type="huggingface")
result = engine.generate_single("prompt", max_new_tokens=100)
```

## üéØ Best Practices

### **Model Selection**

1. **Your MinimalLLM**: Use `model_type="minimal_llm"` for full optimization
2. **HuggingFace Models**: Use `model_type="huggingface"` for standard models
3. **Unknown Models**: Use `model_type="auto"` for automatic detection

### **Performance Optimization**

1. **Sequence Length**: Set appropriate `max_seq_len` for your use case
2. **Batch Size**: Use batch generation for multiple prompts
3. **Memory**: Monitor memory usage with `get_model_info()`

### **Error Handling**

```python
try:
    engine = create_universal_fast_inference(model, tokenizer)
    result = engine.generate_single(prompt)
except Exception as e:
    print(f"Error: {e}")
    # Fallback to standard generation
```

## üîç Troubleshooting

### **Common Issues**

1. **Model Not Found**
   ```python
   # Check model path
   import os
   if not os.path.exists("models/final_model1.pt"):
       print("Model file not found!")
   ```

2. **Memory Issues**
   ```python
   # Reduce sequence length
   engine = create_universal_fast_inference(
       model, tokenizer, max_seq_len=512
   )
   ```

3. **Slow Performance**
   ```python
   # Check device
   print(f"Device: {engine.device}")
   # Make sure model is on GPU
   ```

### **Debug Mode**

```python
# Enable verbose output
import logging
logging.basicConfig(level=logging.DEBUG)

# Get detailed model info
info = engine.get_model_info()
print(f"Architecture: {info['architecture']}")
```

## üéâ Benefits Summary

### **Universal Compatibility**
- ‚úÖ **Any Model**: Works with your models and HuggingFace models
- ‚úÖ **Automatic Detection**: No manual configuration needed
- ‚úÖ **Easy Migration**: Simple API changes

### **Performance Benefits**
- ‚úÖ **KV Caching**: 10-100x speedup for supported models
- ‚úÖ **Memory Efficient**: Optimized memory usage
- ‚úÖ **Batch Processing**: Handle multiple prompts efficiently

### **Developer Experience**
- ‚úÖ **Simple API**: Easy to use and understand
- ‚úÖ **Error Handling**: Robust error handling and fallbacks
- ‚úÖ **Documentation**: Comprehensive examples and guides

## üöÄ Next Steps

1. **Try the Examples**: Run the universal examples to see it in action
2. **Test Your Models**: Use your trained models with the universal engine
3. **Experiment**: Try different HuggingFace models
4. **Optimize**: Adjust parameters for your specific use case

The Universal Fast Inference Engine gives you the best of both worlds: the performance of your custom models with the flexibility to use any model you want!
