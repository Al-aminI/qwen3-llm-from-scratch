"""
ğŸŒ Universal Fast Inference Example

This example demonstrates how to use the universal fast inference engine
with different types of models.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from fast_inference.core.engine.universal_engine import create_universal_fast_inference

def example_minimal_llm():
    """
    ğŸ¯ EXAMPLE: Your Custom MinimalLLM Model
    """
    print("ğŸ¯ Example 1: Your Custom MinimalLLM Model")
    print("=" * 50)
    
    # Load your trained model
    model_path = "models/final_model1.pt"
    tokenizer_path = "HuggingFaceTB/SmolLM-135M"
    
    if not os.path.exists(model_path):
        print(f"âŒ Model file {model_path} not found!")
        print("ğŸ’¡ Please train a model first with the pretraining package")
        return
    
    # Create universal engine
    engine = create_universal_fast_inference(
        model=model_path,
        tokenizer=tokenizer_path,
        max_seq_len=1024,
        model_type="minimal_llm"
    )
    
    # Get model info
    info = engine.get_model_info()
    print(f"ğŸ“Š Model Info:")
    print(f"   Type: {info['model_type']}")
    print(f"   Parameters: {info['parameters']:,}")
    print(f"   Architecture: {info['architecture']['model_type']}")
    
    # Generate text
    prompts = [
        "The future of artificial intelligence",
        "Once upon a time in a distant galaxy",
        "The most important thing to remember is"
    ]
    
    for i, prompt in enumerate(prompts, 1):
        print(f"\nğŸ¯ Prompt {i}: '{prompt}'")
        print("-" * 30)
        
        result = engine.generate_single(
            prompt=prompt,
            max_new_tokens=50,
            temperature=0.8,
            top_k=50,
            top_p=0.9
        )
        
        print(f"ğŸ“ Generated: {result}")
    
    print(f"\nâœ… MinimalLLM example completed!")

def example_huggingface_models():
    """
    ğŸ¯ EXAMPLE: HuggingFace Models
    """
    print("\nğŸ¯ Example 2: HuggingFace Models")
    print("=" * 50)
    
    # List of HuggingFace models to try
    models_to_try = [
        "microsoft/DialoGPT-small",
        "gpt2",
        "distilgpt2"
    ]
    
    for model_name in models_to_try:
        try:
            print(f"\nğŸ”„ Trying model: {model_name}")
            
            # Create universal engine
            engine = create_universal_fast_inference(
                model=model_name,
                tokenizer=model_name,
                max_seq_len=512,
                model_type="huggingface"
            )
            
            # Get model info
            info = engine.get_model_info()
            print(f"ğŸ“Š Model Info:")
            print(f"   Type: {info['model_type']}")
            print(f"   Parameters: {info['parameters']:,}")
            print(f"   Architecture: {info['architecture']['model_type']}")
            
            # Generate text
            prompt = "The future of technology"
            result = engine.generate_single(
                prompt=prompt,
                max_new_tokens=30,
                temperature=0.8
            )
            
            print(f"ğŸ“ Generated: {result}")
            
        except Exception as e:
            print(f"âŒ Error with {model_name}: {e}")
            continue
    
    print(f"\nâœ… HuggingFace models example completed!")

def example_batch_generation():
    """
    ğŸ¯ EXAMPLE: Batch Generation
    """
    print("\nğŸ¯ Example 3: Batch Generation")
    print("=" * 50)
    
    # Use a small model for batch generation
    model_name = "distilgpt2"
    
    try:
        # Create universal engine
        engine = create_universal_fast_inference(
            model=model_name,
            tokenizer=model_name,
            max_seq_len=256,
            model_type="huggingface"
        )
        
        # Batch prompts
        prompts = [
            "Tell me a joke about",
            "Write a haiku about",
            "Explain the concept of",
            "The best way to learn",
            "In the year 2050"
        ]
        
        print(f"ğŸ”„ Generating text for {len(prompts)} prompts...")
        
        # Generate batch
        results = engine.generate_batch(
            prompts=prompts,
            max_new_tokens=20,
            temperature=0.8
        )
        
        # Display results
        for prompt, result in zip(prompts, results):
            print(f"\nğŸ“ {prompt} {result}")
        
    except Exception as e:
        print(f"âŒ Error in batch generation: {e}")
    
    print(f"\nâœ… Batch generation example completed!")

def example_performance_comparison():
    """
    ğŸ¯ EXAMPLE: Performance Comparison
    """
    print("\nğŸ¯ Example 4: Performance Comparison")
    print("=" * 50)
    
    import time
    
    # Test with a small model
    model_name = "distilgpt2"
    
    try:
        # Create universal engine
        engine = create_universal_fast_inference(
            model=model_name,
            tokenizer=model_name,
            max_seq_len=256,
            model_type="huggingface"
        )
        
        # Test prompts
        test_prompts = [
            "The future of artificial intelligence",
            "Once upon a time in a distant galaxy",
            "The most important thing to remember is"
        ]
        
        # Time generation
        start_time = time.time()
        
        for prompt in test_prompts:
            result = engine.generate_single(
                prompt=prompt,
                max_new_tokens=50,
                temperature=0.8
            )
            print(f"ğŸ“ Generated: {result[:50]}...")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"\nâ±ï¸ Performance:")
        print(f"   Total time: {total_time:.2f} seconds")
        print(f"   Average per prompt: {total_time/len(test_prompts):.2f} seconds")
        print(f"   Prompts processed: {len(test_prompts)}")
        
    except Exception as e:
        print(f"âŒ Error in performance test: {e}")
    
    print(f"\nâœ… Performance comparison completed!")

def main():
    """
    ğŸ¯ MAIN FUNCTION
    
    Run all examples to demonstrate universal fast inference.
    """
    print("ğŸŒ UNIVERSAL FAST INFERENCE EXAMPLES")
    print("=" * 60)
    print("This example demonstrates how to use the universal fast inference")
    print("engine with different types of models.")
    print()
    
    # Run examples
    example_minimal_llm()
    example_huggingface_models()
    example_batch_generation()
    example_performance_comparison()
    
    print("\nğŸ‰ All examples completed!")
    print("\nğŸ’¡ Key Takeaways:")
    print("1. âœ… Universal engine works with your MinimalLLM")
    print("2. âœ… Universal engine works with HuggingFace models")
    print("3. âœ… Automatic model detection and optimization")
    print("4. âœ… Easy batch generation")
    print("5. âœ… Performance monitoring")

if __name__ == "__main__":
    main()
