"""
üöÄ Performance Optimization Examples

This module demonstrates various performance optimization techniques using Triton.
"""

import torch
import triton
import triton.language as tl
import time
import numpy as np
from typing import Tuple, Optional, List
from triton_tutorials.utils.benchmarking import BenchmarkSuite
from triton_tutorials.utils.profiling import PerformanceProfiler
from triton_tutorials.utils.performance_analysis import PerformanceAnalyzer

# ============================================================================
# üß† BASIC PERFORMANCE OPTIMIZATION
# ============================================================================

@triton.jit
def basic_vector_add_kernel(
    a_ptr, b_ptr, output_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    """
    üß† BASIC VECTOR ADDITION KERNEL
    
    Basic vector addition implementation.
    """
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    
    # Load data
    a = tl.load(a_ptr + offsets, mask=mask, other=0.0)
    b = tl.load(b_ptr + offsets, mask=mask, other=0.0)
    
    # Perform addition
    output = a + b
    
    # Store result
    tl.store(output_ptr + offsets, output, mask=mask)

def basic_vector_add(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """
    üß† BASIC VECTOR ADDITION WRAPPER
    
    Wrapper function for basic vector addition.
    """
    # Input validation
    assert a.is_cuda and b.is_cuda, "Input tensors must be on GPU!"
    assert a.shape == b.shape, "Input tensors must have the same shape!"
    
    n_elements = a.numel()
    output = torch.empty_like(a)
    
    # Define block size
    BLOCK_SIZE = 128
    
    # Calculate grid size
    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)
    
    # Launch kernel
    basic_vector_add_kernel[grid](
        a, b, output,
        n_elements,
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    return output

# ============================================================================
# üöÄ OPTIMIZED PERFORMANCE
# ============================================================================

@triton.jit
def optimized_vector_add_kernel(
    a_ptr, b_ptr, output_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    """
    üöÄ OPTIMIZED VECTOR ADDITION KERNEL
    
    Optimized vector addition with:
    - Coalesced memory access
    - Optimal block size
    - Efficient memory patterns
    """
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    
    # Load data with coalesced access
    a = tl.load(a_ptr + offsets, mask=mask, other=0.0)
    b = tl.load(b_ptr + offsets, mask=mask, other=0.0)
    
    # Perform addition
    output = a + b
    
    # Store result with coalesced access
    tl.store(output_ptr + offsets, output, mask=mask)

def optimized_vector_add(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """
    üöÄ OPTIMIZED VECTOR ADDITION WRAPPER
    
    Wrapper function for optimized vector addition.
    """
    # Input validation
    assert a.is_cuda and b.is_cuda, "Input tensors must be on GPU!"
    assert a.shape == b.shape, "Input tensors must have the same shape!"
    
    n_elements = a.numel()
    output = torch.empty_like(a)
    
    # Define optimal block size
    BLOCK_SIZE = 256
    
    # Calculate grid size
    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)
    
    # Launch kernel
    optimized_vector_add_kernel[grid](
        a, b, output,
        n_elements,
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    return output

# ============================================================================
# üî• FUSED PERFORMANCE
# ============================================================================

@triton.jit
def fused_vector_operations_kernel(
    a_ptr, b_ptr, c_ptr, output_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    """
    üî• FUSED VECTOR OPERATIONS KERNEL
    
    Fused vector operations:
    output = (a + b) * c + a * 2.0
    """
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    
    # Load data
    a = tl.load(a_ptr + offsets, mask=mask, other=0.0)
    b = tl.load(b_ptr + offsets, mask=mask, other=0.0)
    c = tl.load(c_ptr + offsets, mask=mask, other=0.0)
    
    # Fused operations: (a + b) * c + a * 2.0
    intermediate1 = a + b
    intermediate2 = intermediate1 * c
    intermediate3 = a * 2.0
    output = intermediate2 + intermediate3
    
    # Store result
    tl.store(output_ptr + offsets, output, mask=mask)

def fused_vector_operations(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:
    """
    üî• FUSED VECTOR OPERATIONS WRAPPER
    
    Wrapper function for fused vector operations.
    """
    # Input validation
    assert a.is_cuda and b.is_cuda and c.is_cuda, "Input tensors must be on GPU!"
    assert a.shape == b.shape == c.shape, "Input tensors must have the same shape!"
    
    n_elements = a.numel()
    output = torch.empty_like(a)
    
    # Define block size
    BLOCK_SIZE = 256
    
    # Calculate grid size
    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)
    
    # Launch kernel
    fused_vector_operations_kernel[grid](
        a, b, c, output,
        n_elements,
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    return output

# ============================================================================
# üéØ AUTOTUNED PERFORMANCE
# ============================================================================

@triton.jit
def autotuned_vector_add_kernel(
    a_ptr, b_ptr, output_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
    NUM_WARPS: tl.constexpr,
):
    """
    üéØ AUTOTUNED VECTOR ADDITION KERNEL
    
    Autotuned vector addition with configurable parameters.
    """
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    
    # Load data
    a = tl.load(a_ptr + offsets, mask=mask, other=0.0)
    b = tl.load(b_ptr + offsets, mask=mask, other=0.0)
    
    # Perform addition
    output = a + b
    
    # Store result
    tl.store(output_ptr + offsets, output, mask=mask)

def autotuned_vector_add(a: torch.Tensor, b: torch.Tensor, 
                        block_size: int = 256, num_warps: int = 4) -> torch.Tensor:
    """
    üéØ AUTOTUNED VECTOR ADDITION WRAPPER
    
    Wrapper function for autotuned vector addition.
    """
    # Input validation
    assert a.is_cuda and b.is_cuda, "Input tensors must be on GPU!"
    assert a.shape == b.shape, "Input tensors must have the same shape!"
    
    n_elements = a.numel()
    output = torch.empty_like(a)
    
    # Calculate grid size
    grid = (triton.cdiv(n_elements, block_size),)
    
    # Launch kernel
    autotuned_vector_add_kernel[grid](
        a, b, output,
        n_elements,
        BLOCK_SIZE=block_size,
        NUM_WARPS=num_warps
    )
    
    return output

# ============================================================================
# üß™ TESTING AND VALIDATION
# ============================================================================

def test_performance_optimizations():
    """
    üß™ TEST PERFORMANCE OPTIMIZATIONS
    
    Tests various performance optimization techniques.
    """
    print("üß™ Testing Performance Optimizations:")
    print("=" * 50)
    
    # Test basic vector addition
    print("\nüìä Test: Basic Vector Addition")
    size = 1024
    a = torch.randn(size, device='cuda', dtype=torch.float32)
    b = torch.randn(size, device='cuda', dtype=torch.float32)
    
    triton_result = basic_vector_add(a, b)
    pytorch_result = a + b
    
    is_correct = torch.allclose(triton_result, pytorch_result, rtol=1e-5, atol=1e-6)
    print(f"  Result: {'‚úÖ PASS' if is_correct else '‚ùå FAIL'}")
    
    # Test optimized vector addition
    print("\nüìä Test: Optimized Vector Addition")
    triton_result = optimized_vector_add(a, b)
    pytorch_result = a + b
    
    is_correct = torch.allclose(triton_result, pytorch_result, rtol=1e-5, atol=1e-6)
    print(f"  Result: {'‚úÖ PASS' if is_correct else '‚ùå FAIL'}")
    
    # Test fused vector operations
    print("\nüìä Test: Fused Vector Operations")
    c = torch.randn(size, device='cuda', dtype=torch.float32)
    
    triton_result = fused_vector_operations(a, b, c)
    pytorch_result = (a + b) * c + a * 2.0
    
    is_correct = torch.allclose(triton_result, pytorch_result, rtol=1e-5, atol=1e-6)
    print(f"  Result: {'‚úÖ PASS' if is_correct else '‚ùå FAIL'}")
    
    # Test autotuned vector addition
    print("\nüìä Test: Autotuned Vector Addition")
    triton_result = autotuned_vector_add(a, b, block_size=256, num_warps=4)
    pytorch_result = a + b
    
    is_correct = torch.allclose(triton_result, pytorch_result, rtol=1e-5, atol=1e-6)
    print(f"  Result: {'‚úÖ PASS' if is_correct else '‚ùå FAIL'}")

# ============================================================================
# üìä PERFORMANCE BENCHMARKING
# ============================================================================

def benchmark_performance_optimizations():
    """
    üìä BENCHMARK PERFORMANCE OPTIMIZATIONS
    
    Benchmarks various performance optimization techniques.
    """
    print("\nüìä Benchmarking Performance Optimizations:")
    print("=" * 50)
    
    # Test different sizes
    sizes = [1024, 4096, 16384, 65536, 262144]
    
    for size in sizes:
        print(f"\nüìà Size: {size:,} elements")
        
        # Create test data
        a = torch.randn(size, device='cuda', dtype=torch.float32)
        b = torch.randn(size, device='cuda', dtype=torch.float32)
        c = torch.randn(size, device='cuda', dtype=torch.float32)
        
        # Benchmark basic vector addition
        print("\n  Basic Vector Addition:")
        try:
            # Warmup
            for _ in range(10):
                _ = basic_vector_add(a, b)
                _ = a + b
            
            torch.cuda.synchronize()
            
            # Benchmark Triton
            start_time = time.time()
            for _ in range(100):
                _ = basic_vector_add(a, b)
            torch.cuda.synchronize()
            triton_time = (time.time() - start_time) / 100 * 1000
            
            # Benchmark PyTorch
            start_time = time.time()
            for _ in range(100):
                _ = a + b
            torch.cuda.synchronize()
            pytorch_time = (time.time() - start_time) / 100 * 1000
            
            speedup = pytorch_time / triton_time if triton_time > 0 else 0
            
            print(f"    Triton:  {triton_time:.3f} ms")
            print(f"    PyTorch: {pytorch_time:.3f} ms")
            print(f"    Speedup: {speedup:.2f}x")
        except Exception as e:
            print(f"    Error: {e}")
        
        # Benchmark optimized vector addition
        print("\n  Optimized Vector Addition:")
        try:
            # Warmup
            for _ in range(10):
                _ = optimized_vector_add(a, b)
                _ = a + b
            
            torch.cuda.synchronize()
            
            # Benchmark Triton
            start_time = time.time()
            for _ in range(100):
                _ = optimized_vector_add(a, b)
            torch.cuda.synchronize()
            triton_time = (time.time() - start_time) / 100 * 1000
            
            # Benchmark PyTorch
            start_time = time.time()
            for _ in range(100):
                _ = a + b
            torch.cuda.synchronize()
            pytorch_time = (time.time() - start_time) / 100 * 1000
            
            speedup = pytorch_time / triton_time if triton_time > 0 else 0
            
            print(f"    Triton:  {triton_time:.3f} ms")
            print(f"    PyTorch: {pytorch_time:.3f} ms")
            print(f"    Speedup: {speedup:.2f}x")
        except Exception as e:
            print(f"    Error: {e}")
        
        # Benchmark fused vector operations
        print("\n  Fused Vector Operations:")
        try:
            # Warmup
            for _ in range(10):
                _ = fused_vector_operations(a, b, c)
                _ = (a + b) * c + a * 2.0
            
            torch.cuda.synchronize()
            
            # Benchmark Triton
            start_time = time.time()
            for _ in range(100):
                _ = fused_vector_operations(a, b, c)
            torch.cuda.synchronize()
            triton_time = (time.time() - start_time) / 100 * 1000
            
            # Benchmark PyTorch
            start_time = time.time()
            for _ in range(100):
                _ = (a + b) * c + a * 2.0
            torch.cuda.synchronize()
            pytorch_time = (time.time() - start_time) / 100 * 1000
            
            speedup = pytorch_time / triton_time if triton_time > 0 else 0
            
            print(f"    Triton:  {triton_time:.3f} ms")
            print(f"    PyTorch: {pytorch_time:.3f} ms")
            print(f"    Speedup: {speedup:.2f}x")
        except Exception as e:
            print(f"    Error: {e}")
        
        # Benchmark autotuned vector addition
        print("\n  Autotuned Vector Addition:")
        try:
            # Warmup
            for _ in range(10):
                _ = autotuned_vector_add(a, b, block_size=256, num_warps=4)
                _ = a + b
            
            torch.cuda.synchronize()
            
            # Benchmark Triton
            start_time = time.time()
            for _ in range(100):
                _ = autotuned_vector_add(a, b, block_size=256, num_warps=4)
            torch.cuda.synchronize()
            triton_time = (time.time() - start_time) / 100 * 1000
            
            # Benchmark PyTorch
            start_time = time.time()
            for _ in range(100):
                _ = a + b
            torch.cuda.synchronize()
            pytorch_time = (time.time() - start_time) / 100 * 1000
            
            speedup = pytorch_time / triton_time if triton_time > 0 else 0
            
            print(f"    Triton:  {triton_time:.3f} ms")
            print(f"    PyTorch: {pytorch_time:.3f} ms")
            print(f"    Speedup: {speedup:.2f}x")
        except Exception as e:
            print(f"    Error: {e}")

# ============================================================================
# üîç PROFILING AND ANALYSIS
# ============================================================================

def profile_performance_optimizations():
    """
    üîç PROFILE PERFORMANCE OPTIMIZATIONS
    
    Profiles various performance optimization techniques.
    """
    print("\nüîç Profiling Performance Optimizations:")
    print("=" * 50)
    
    # Create profiler
    profiler = PerformanceProfiler()
    
    # Test configuration
    size = 1024 * 1024  # 1M elements
    a = torch.randn(size, device='cuda', dtype=torch.float32)
    b = torch.randn(size, device='cuda', dtype=torch.float32)
    c = torch.randn(size, device='cuda', dtype=torch.float32)
    
    # Profile basic vector addition
    print("\nüìä Profile: Basic Vector Addition")
    result = profiler.profile_function(
        basic_vector_add,
        "Basic Vector Addition",
        a, b
    )
    
    if result:
        print(f"  Execution Time: {result.execution_time:.3f} ms")
        print(f"  Memory Usage: {result.memory_usage:.3f} GB")
        print(f"  GPU Memory Usage: {result.gpu_memory_usage:.3f} GB")
        print(f"  Status: ‚úÖ")
    else:
        print(f"  Status: ‚ùå")
    
    # Profile optimized vector addition
    print("\nüìä Profile: Optimized Vector Addition")
    result = profiler.profile_function(
        optimized_vector_add,
        "Optimized Vector Addition",
        a, b
    )
    
    if result:
        print(f"  Execution Time: {result.execution_time:.3f} ms")
        print(f"  Memory Usage: {result.memory_usage:.3f} GB")
        print(f"  GPU Memory Usage: {result.gpu_memory_usage:.3f} GB")
        print(f"  Status: ‚úÖ")
    else:
        print(f"  Status: ‚ùå")
    
    # Profile fused vector operations
    print("\nüìä Profile: Fused Vector Operations")
    result = profiler.profile_function(
        fused_vector_operations,
        "Fused Vector Operations",
        a, b, c
    )
    
    if result:
        print(f"  Execution Time: {result.execution_time:.3f} ms")
        print(f"  Memory Usage: {result.memory_usage:.3f} GB")
        print(f"  GPU Memory Usage: {result.gpu_memory_usage:.3f} GB")
        print(f"  Status: ‚úÖ")
    else:
        print(f"  Status: ‚ùå")
    
    # Profile autotuned vector addition
    print("\nüìä Profile: Autotuned Vector Addition")
    result = profiler.profile_function(
        lambda x, y: autotuned_vector_add(x, y, block_size=256, num_warps=4),
        "Autotuned Vector Addition",
        a, b
    )
    
    if result:
        print(f"  Execution Time: {result.execution_time:.3f} ms")
        print(f"  Memory Usage: {result.memory_usage:.3f} GB")
        print(f"  GPU Memory Usage: {result.gpu_memory_usage:.3f} GB")
        print(f"  Status: ‚úÖ")
    else:
        print(f"  Status: ‚ùå")
    
    # Print profiling results
    profiler.print_results()
    
    # Save results
    profiler.save_results("performance_profiling_results.json")

# ============================================================================
# üìà PERFORMANCE ANALYSIS
# ============================================================================

def analyze_performance_optimizations():
    """
    üìà ANALYZE PERFORMANCE OPTIMIZATIONS
    
    Analyzes various performance optimization techniques.
    """
    print("\nüìà Analyzing Performance Optimizations:")
    print("=" * 50)
    
    # Create analyzer
    analyzer = PerformanceAnalyzer()
    
    # Test different sizes
    sizes = [1024, 4096, 16384, 65536, 262144]
    
    for size in sizes:
        print(f"\nüìä Size: {size:,} elements")
        
        # Create test data
        a = torch.randn(size, device='cuda', dtype=torch.float32)
        b = torch.randn(size, device='cuda', dtype=torch.float32)
        
        # Analyze basic vector addition
        result = analyzer.analyze_kernel(
            basic_vector_add,
            f"Basic Vector Addition ({size:,})",
            size,
            torch.float32,
            num_runs=100
        )
        
        if result:
            print(f"  Basic Vector Addition:")
            print(f"    Execution Time: {result.execution_time:.3f} ms")
            print(f"    Memory Bandwidth: {result.memory_bandwidth:.1f} GB/s")
            print(f"    Throughput: {result.throughput:.0f} elements/s")
            print(f"    Efficiency: {result.efficiency:.2f}")
        
        # Analyze optimized vector addition
        result = analyzer.analyze_kernel(
            optimized_vector_add,
            f"Optimized Vector Addition ({size:,})",
            size,
            torch.float32,
            num_runs=100
        )
        
        if result:
            print(f"  Optimized Vector Addition:")
            print(f"    Execution Time: {result.execution_time:.3f} ms")
            print(f"    Memory Bandwidth: {result.memory_bandwidth:.1f} GB/s")
            print(f"    Throughput: {result.throughput:.0f} elements/s")
            print(f"    Efficiency: {result.efficiency:.2f}")
    
    # Print analysis summary
    analyzer.print_summary()
    
    # Generate report
    analyzer.generate_report("performance_analysis_report.json")

# ============================================================================
# üéØ MAIN FUNCTION
# ============================================================================

def main():
    """
    üéØ MAIN FUNCTION
    
    Runs the performance optimization examples.
    """
    print("üöÄ PERFORMANCE OPTIMIZATION EXAMPLES")
    print("=" * 70)
    print("This module demonstrates various performance optimization techniques.")
    
    # Check CUDA availability
    if not torch.cuda.is_available():
        print("\n‚ùå CUDA not available. Please use a GPU-enabled environment.")
        return
    
    # Run the examples
    test_performance_optimizations()
    benchmark_performance_optimizations()
    profile_performance_optimizations()
    analyze_performance_optimizations()
    
    print("\nüéâ Performance Optimization Examples Complete!")
    print("\nüí° Key Takeaways:")
    print("1. ‚úÖ Basic performance optimization techniques")
    print("2. ‚úÖ Optimized memory access patterns")
    print("3. ‚úÖ Fused operations for better performance")
    print("4. ‚úÖ Autotuned kernels for optimal performance")
    print("5. ‚úÖ Performance profiling and analysis")
    print("6. ‚úÖ Memory bandwidth optimization")
    print("7. ‚úÖ Scalable performance optimization")
    
    print("\nüöÄ Next Steps:")
    print("- Experiment with different optimization strategies")
    print("- Try optimizing for different hardware configurations")
    print("- Implement advanced autotuning techniques")
    print("- Add support for different data types and operations")

if __name__ == "__main__":
    main()
