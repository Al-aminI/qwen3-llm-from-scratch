[
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "fire",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fire",
        "description": "fire",
        "detail": "fire",
        "documentation": {}
    },
    {
        "label": "PaliGemmaProcessor",
        "importPath": "processing_paligemma",
        "description": "processing_paligemma",
        "isExtraImport": true,
        "detail": "processing_paligemma",
        "documentation": {}
    },
    {
        "label": "KVCache",
        "importPath": "modeling_gemma",
        "description": "modeling_gemma",
        "isExtraImport": true,
        "detail": "modeling_gemma",
        "documentation": {}
    },
    {
        "label": "PaliGemmaForConditionalGeneration",
        "importPath": "modeling_gemma",
        "description": "modeling_gemma",
        "isExtraImport": true,
        "detail": "modeling_gemma",
        "documentation": {}
    },
    {
        "label": "PaliGemmaForConditionalGeneration",
        "importPath": "modeling_gemma",
        "description": "modeling_gemma",
        "isExtraImport": true,
        "detail": "modeling_gemma",
        "documentation": {}
    },
    {
        "label": "PaliGemmaConfig",
        "importPath": "modeling_gemma",
        "description": "modeling_gemma",
        "isExtraImport": true,
        "detail": "modeling_gemma",
        "documentation": {}
    },
    {
        "label": "load_hf_model",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "SiglipVisionConfig",
        "importPath": "modeling_siglip",
        "description": "modeling_siglip",
        "isExtraImport": true,
        "detail": "modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipVisionModel",
        "importPath": "modeling_siglip",
        "description": "modeling_siglip",
        "isExtraImport": true,
        "detail": "modeling_siglip",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "safe_open",
        "importPath": "safetensors",
        "description": "safetensors",
        "isExtraImport": true,
        "detail": "safetensors",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "torch.utils.data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "BlockMask",
        "importPath": "torch.nn.attention.flex_attention",
        "description": "torch.nn.attention.flex_attention",
        "isExtraImport": true,
        "detail": "torch.nn.attention.flex_attention",
        "documentation": {}
    },
    {
        "label": "create_block_mask",
        "importPath": "torch.nn.attention.flex_attention",
        "description": "torch.nn.attention.flex_attention",
        "isExtraImport": true,
        "detail": "torch.nn.attention.flex_attention",
        "documentation": {}
    },
    {
        "label": "BlockMask",
        "importPath": "torch.nn.attention.flex_attention",
        "description": "torch.nn.attention.flex_attention",
        "isExtraImport": true,
        "detail": "torch.nn.attention.flex_attention",
        "documentation": {}
    },
    {
        "label": "create_block_mask",
        "importPath": "torch.nn.attention.flex_attention",
        "description": "torch.nn.attention.flex_attention",
        "isExtraImport": true,
        "detail": "torch.nn.attention.flex_attention",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "asynccontextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "BenchmarkRunner",
        "importPath": "fast_inference.utils.benchmarking",
        "description": "fast_inference.utils.benchmarking",
        "isExtraImport": true,
        "detail": "fast_inference.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "BenchmarkRunner",
        "importPath": "fast_inference.utils.benchmarking",
        "description": "fast_inference.utils.benchmarking",
        "isExtraImport": true,
        "detail": "fast_inference.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "generate_test_prompts",
        "importPath": "fast_inference.utils.benchmarking",
        "description": "fast_inference.utils.benchmarking",
        "isExtraImport": true,
        "detail": "fast_inference.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "Mock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "Mock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "fast_inference.core.engine",
        "description": "fast_inference.core.engine",
        "isExtraImport": true,
        "detail": "fast_inference.core.engine",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "sample_tokens",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "apply_repetition_penalty",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "apply_top_k_filtering",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "apply_top_p_filtering",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "sample_greedy",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "sample_random",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "SimpleKVCache",
        "importPath": "fast_inference.core.cache",
        "description": "fast_inference.core.cache",
        "isExtraImport": true,
        "detail": "fast_inference.core.cache",
        "documentation": {}
    },
    {
        "label": "PagedKVCache",
        "importPath": "fast_inference.core.cache",
        "description": "fast_inference.core.cache",
        "isExtraImport": true,
        "detail": "fast_inference.core.cache",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "Muon",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "train_model",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "resume_training",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "simple_fast_inference",
        "description": "simple_fast_inference",
        "isExtraImport": true,
        "detail": "simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "importPath": "simple_fast_inference",
        "description": "simple_fast_inference",
        "isExtraImport": true,
        "detail": "simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "psutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil",
        "description": "psutil",
        "detail": "psutil",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "QuantizationExpert",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationBenchmark",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALayer",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALayer",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALinear",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALayer",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALayer",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALinear",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "FineTuneConfig",
        "importPath": "finetune_imdb",
        "description": "finetune_imdb",
        "isExtraImport": true,
        "detail": "finetune_imdb",
        "documentation": {}
    },
    {
        "label": "load_imdb_data",
        "importPath": "finetune_imdb",
        "description": "finetune_imdb",
        "isExtraImport": true,
        "detail": "finetune_imdb",
        "documentation": {}
    },
    {
        "label": "SentimentClassifier",
        "importPath": "finetune_imdb",
        "description": "finetune_imdb",
        "isExtraImport": true,
        "detail": "finetune_imdb",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "move_inputs_to_device",
        "kind": 2,
        "importPath": "pytorch-paligemma.inference",
        "description": "pytorch-paligemma.inference",
        "peekOfCode": "def move_inputs_to_device(model_inputs: dict, device: str):\n    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n    return model_inputs\ndef get_model_inputs(\n    processor: PaliGemmaProcessor, prompt: str, image_file_path: str, device: str\n):\n    image = Image.open(image_file_path)\n    images = [image]\n    prompts = [prompt]\n    model_inputs = processor(text=prompts, images=images)",
        "detail": "pytorch-paligemma.inference",
        "documentation": {}
    },
    {
        "label": "get_model_inputs",
        "kind": 2,
        "importPath": "pytorch-paligemma.inference",
        "description": "pytorch-paligemma.inference",
        "peekOfCode": "def get_model_inputs(\n    processor: PaliGemmaProcessor, prompt: str, image_file_path: str, device: str\n):\n    image = Image.open(image_file_path)\n    images = [image]\n    prompts = [prompt]\n    model_inputs = processor(text=prompts, images=images)\n    model_inputs = move_inputs_to_device(model_inputs, device)\n    return model_inputs\ndef test_inference(",
        "detail": "pytorch-paligemma.inference",
        "documentation": {}
    },
    {
        "label": "test_inference",
        "kind": 2,
        "importPath": "pytorch-paligemma.inference",
        "description": "pytorch-paligemma.inference",
        "peekOfCode": "def test_inference(\n    model: PaliGemmaForConditionalGeneration,\n    processor: PaliGemmaProcessor,\n    device: str,\n    prompt: str,\n    image_file_path: str,\n    max_tokens_to_generate: int,\n    temperature: float,\n    top_p: float,\n    do_sample: bool,",
        "detail": "pytorch-paligemma.inference",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "pytorch-paligemma.inference",
        "description": "pytorch-paligemma.inference",
        "peekOfCode": "def main(\n    model_path: str = None,\n    prompt: str = None,\n    image_file_path: str = None,\n    max_tokens_to_generate: int = 100,\n    temperature: float = 0.8,\n    top_p: float = 0.9,\n    do_sample: bool = False,\n    only_cpu: bool = False,\n):",
        "detail": "pytorch-paligemma.inference",
        "documentation": {}
    },
    {
        "label": "KVCache",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class KVCache():\n    def __init__(self) -> None:\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n    def num_items(self) -> int:\n        if len(self.key_cache) == 0:\n            return 0\n        else:\n            # The shape of the key_cache is [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n            return self.key_cache[0].shape[-2]",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaConfig",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaConfig():\n    def __init__(\n        self,\n        vocab_size,\n        hidden_size,\n        intermediate_size,\n        num_hidden_layers,\n        num_attention_heads,\n        num_key_value_heads,\n        head_dim=256,",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "PaliGemmaConfig",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class PaliGemmaConfig():\n    def __init__(\n        self,\n        vision_config=None,\n        text_config=None,\n        ignore_index=-100,\n        image_token_index=256000,\n        vocab_size=257152,\n        projection_dim=2048,\n        hidden_size=2048,",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaRMSNorm",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaRMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.zeros(dim))\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n    def forward(self, x):\n        output = self._norm(x.float())\n        # Llama does x.to(float16) * w whilst Gemma is (x * w).to(float16)",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaRotaryEmbedding",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n        self.dim = dim # it is set to the head_dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        # Calculate the theta according to the formula theta_i = base^(-2i/dim) where i = 0, 1, 2, ..., dim // 2\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n    @torch.no_grad()",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaMLP",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n    def forward(self, x):",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaAttention",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaAttention(nn.Module):\n    def __init__(self, config: GemmaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = config.head_dim\n        self.num_key_value_heads = config.num_key_value_heads",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaDecoderLayer",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaDecoderLayer(nn.Module):\n    def __init__(self, config: GemmaConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = GemmaAttention(config=config, layer_idx=layer_idx)\n        self.mlp = GemmaMLP(config)\n        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n    def forward(\n        self,",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaModel",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaModel(nn.Module):\n    def __init__(self, config: GemmaConfig):\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList(\n            [GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaForCausalLM",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaForCausalLM(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.model = GemmaModel(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n    def tie_weights(self):",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "PaliGemmaMultiModalProjector",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class PaliGemmaMultiModalProjector(nn.Module):\n    def __init__(self, config: PaliGemmaConfig):\n        super().__init__()\n        self.linear = nn.Linear(config.vision_config.hidden_size, config.vision_config.projection_dim, bias=True)\n    def forward(self, image_features):\n        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Projection_Dim]\n        hidden_states = self.linear(image_features)\n        return hidden_states\nclass PaliGemmaForConditionalGeneration(nn.Module):\n    def __init__(self, config: PaliGemmaConfig):",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "PaliGemmaForConditionalGeneration",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class PaliGemmaForConditionalGeneration(nn.Module):\n    def __init__(self, config: PaliGemmaConfig):\n        super().__init__()\n        self.config = config\n        self.vision_tower = SiglipVisionModel(config.vision_config)\n        self.multi_modal_projector = PaliGemmaMultiModalProjector(config)\n        self.vocab_size = config.vocab_size\n        language_model = GemmaForCausalLM(config.text_config)\n        self.language_model = language_model\n        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "rotate_half",
        "kind": 2,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "def rotate_half(x):\n    # Build the [-x2, x1, -x4, x3, ...] tensor for the sin part of the positional encoding.\n    x1 = x[..., : x.shape[-1] // 2] # Takes the first half of the last dimension\n    x2 = x[..., x.shape[-1] // 2 :] # Takes the second half of the last dimension\n    return torch.cat((-x2, x1), dim=-1)\ndef apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n    cos = cos.unsqueeze(unsqueeze_dim) # Add the head dimension\n    sin = sin.unsqueeze(unsqueeze_dim) # Add the head dimension\n    # Apply the formula (34) of the Rotary Positional Encoding paper.\n    q_embed = (q * cos) + (rotate_half(q) * sin)",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "apply_rotary_pos_emb",
        "kind": 2,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n    cos = cos.unsqueeze(unsqueeze_dim) # Add the head dimension\n    sin = sin.unsqueeze(unsqueeze_dim) # Add the head dimension\n    # Apply the formula (34) of the Rotary Positional Encoding paper.\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\nclass GemmaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\nclass GemmaAttention(nn.Module):\n    def __init__(self, config: GemmaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "SiglipVisionConfig",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipVisionConfig:\n    def __init__(\n        self,\n        hidden_size=768,\n        intermediate_size=3072,\n        num_hidden_layers=12,\n        num_attention_heads=12,\n        num_channels=3,\n        image_size=224,\n        patch_size=16,",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipVisionEmbeddings",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipVisionEmbeddings(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.image_size = config.image_size\n        self.patch_size = config.patch_size\n        self.patch_embedding = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=self.embed_dim,",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipAttention",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n        self.scale = self.head_dim**-0.5 # Equivalent to 1 / sqrt(self.head_dim)\n        self.dropout = config.attention_dropout",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipMLP",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Intermediate_Size]\n        hidden_states = self.fc1(hidden_states)\n        # hidden_states: [Batch_Size, Num_Patches, Intermediate_Size]",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipEncoderLayer",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipEncoderLayer(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.embed_dim = config.hidden_size\n        self.self_attn = SiglipAttention(config)\n        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n        self.mlp = SiglipMLP(config)\n        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n    # Ignore copy\n    def forward(",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipEncoder",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipEncoder(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.layers = nn.ModuleList(\n            [SiglipEncoderLayer(config) for _ in range(config.num_hidden_layers)]\n        )\n    # Ignore copy\n    def forward(\n        self,",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipVisionTransformer",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipVisionTransformer(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        embed_dim = config.hidden_size\n        self.embeddings = SiglipVisionEmbeddings(config)\n        self.encoder = SiglipEncoder(config)\n        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n        # pixel_values: [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipVisionModel",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipVisionModel(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.vision_model = SiglipVisionTransformer(config)\n    def forward(self, pixel_values) -> Tuple:\n        # [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]\n        return self.vision_model(pixel_values=pixel_values)",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "PaliGemmaProcessor",
        "kind": 6,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "class PaliGemmaProcessor:\n    IMAGE_TOKEN = \"<image>\"\n    def __init__(self, tokenizer, num_image_tokens: int, image_size: int):\n        super().__init__()\n        self.image_seq_length = num_image_tokens\n        self.image_size = image_size\n        # Tokenizer described here: https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md#tokenizer\n        tokens_to_add = {\"additional_special_tokens\": [self.IMAGE_TOKEN]}\n        tokenizer.add_special_tokens(tokens_to_add)\n        EXTRA_TOKENS = [",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "add_image_tokens_to_prompt",
        "kind": 2,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "def add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token):\n    # Quoting from the blog (https://huggingface.co/blog/paligemma#detailed-inference-process):\n    #   The input text is tokenized normally.\n    #   A <bos> token is added at the beginning, and an additional newline token (\\n) is appended.\n    #   This newline token is an essential part of the input prompt the model was trained with, so adding it explicitly ensures it's always there.\n    #   The tokenized text is also prefixed with a fixed number of <image> tokens.\n    # NOTE: from the paper it looks like the `\\n` should be tokenized separately, but in the HF implementation this is not done.\n    #       ref to HF implementation: https://github.com/huggingface/transformers/blob/7f79a97399bb52aad8460e1da2f36577d5dccfed/src/transformers/models/paligemma/processing_paligemma.py#L55-L73\n    return f\"{image_token * image_seq_len}{bos_token}{prefix_prompt}\\n\"\ndef rescale(",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "rescale",
        "kind": 2,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "def rescale(\n    image: np.ndarray, scale: float, dtype: np.dtype = np.float32\n) -> np.ndarray:\n    rescaled_image = image * scale\n    rescaled_image = rescaled_image.astype(dtype)\n    return rescaled_image\ndef resize(\n    image: Image,\n    size: Tuple[int, int],\n    resample: Image.Resampling = None,",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 2,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "def resize(\n    image: Image,\n    size: Tuple[int, int],\n    resample: Image.Resampling = None,\n    reducing_gap: Optional[int] = None,\n) -> np.ndarray:\n    height, width = size\n    resized_image = image.resize(\n        (width, height), resample=resample, reducing_gap=reducing_gap\n    )",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "normalize",
        "kind": 2,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "def normalize(\n    image: np.ndarray,\n    mean: Union[float, Iterable[float]],\n    std: Union[float, Iterable[float]],\n) -> np.ndarray:\n    mean = np.array(mean, dtype=image.dtype)\n    std = np.array(std, dtype=image.dtype)\n    image = (image - mean) / std\n    return image\ndef process_images(",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "process_images",
        "kind": 2,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "def process_images(\n    images: List[Image.Image],\n    size: Dict[str, int] = None,\n    resample: Image.Resampling = None,\n    rescale_factor: float = None,\n    image_mean: Optional[Union[float, List[float]]] = None,\n    image_std: Optional[Union[float, List[float]]] = None,\n) -> List[np.ndarray]:\n    height, width = size[0], size[1]\n    images = [",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "IMAGENET_STANDARD_MEAN",
        "kind": 5,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "IMAGENET_STANDARD_MEAN = [0.5, 0.5, 0.5]\nIMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]\ndef add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token):\n    # Quoting from the blog (https://huggingface.co/blog/paligemma#detailed-inference-process):\n    #   The input text is tokenized normally.\n    #   A <bos> token is added at the beginning, and an additional newline token (\\n) is appended.\n    #   This newline token is an essential part of the input prompt the model was trained with, so adding it explicitly ensures it's always there.\n    #   The tokenized text is also prefixed with a fixed number of <image> tokens.\n    # NOTE: from the paper it looks like the `\\n` should be tokenized separately, but in the HF implementation this is not done.\n    #       ref to HF implementation: https://github.com/huggingface/transformers/blob/7f79a97399bb52aad8460e1da2f36577d5dccfed/src/transformers/models/paligemma/processing_paligemma.py#L55-L73",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "IMAGENET_STANDARD_STD",
        "kind": 5,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "IMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]\ndef add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token):\n    # Quoting from the blog (https://huggingface.co/blog/paligemma#detailed-inference-process):\n    #   The input text is tokenized normally.\n    #   A <bos> token is added at the beginning, and an additional newline token (\\n) is appended.\n    #   This newline token is an essential part of the input prompt the model was trained with, so adding it explicitly ensures it's always there.\n    #   The tokenized text is also prefixed with a fixed number of <image> tokens.\n    # NOTE: from the paper it looks like the `\\n` should be tokenized separately, but in the HF implementation this is not done.\n    #       ref to HF implementation: https://github.com/huggingface/transformers/blob/7f79a97399bb52aad8460e1da2f36577d5dccfed/src/transformers/models/paligemma/processing_paligemma.py#L55-L73\n    return f\"{image_token * image_seq_len}{bos_token}{prefix_prompt}\\n\"",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "load_hf_model",
        "kind": 2,
        "importPath": "pytorch-paligemma.utils",
        "description": "pytorch-paligemma.utils",
        "peekOfCode": "def load_hf_model(model_path: str, device: str) -> Tuple[PaliGemmaForConditionalGeneration, AutoTokenizer]:\n    # Load the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n    assert tokenizer.padding_side == \"right\"\n    # Find all the *.safetensors files\n    safetensors_files = glob.glob(os.path.join(model_path, \"*.safetensors\"))\n    # ... and load them one by one in the tensors dictionary\n    tensors = {}\n    for safetensors_file in safetensors_files:\n        with safe_open(safetensors_file, framework=\"pt\", device=\"cpu\") as f:",
        "detail": "pytorch-paligemma.utils",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "kind": 6,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "class SmallModelConfig:\n    \"\"\"\n     SMALL CONFIGURATION FOR FAST TRAINING\n    This configuration is optimized for:\n    - Fast training on CPU/limited GPU\n    - Learning the concepts without waiting hours\n    - Still demonstrating all key components\n    \"\"\"\n    # Model architecture - MUCH SMALLER\n    d_model: int = 128          # Reduced from 384 (3x smaller)",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "Muon",
        "kind": 6,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "class Muon(torch.optim.Optimizer):\n    \"\"\"\n     MUON OPTIMIZER: MomentUm Orthogonalized by Newton-schulz\n    This is a revolutionary optimizer that combines:\n    1. Momentum (like Adam) - remembers past gradients\n    2. Orthogonalization (Newton-Schulz) - makes gradients \"well-behaved\"\n    3. Adaptive learning rates - adjusts based on matrix dimensions\n     Why Muon is special:\n    - 30-50% faster convergence than Adam\n    - More stable training (fewer gradient explosions)",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "kind": 6,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "class TextTokenDataset(Dataset):\n    \"\"\"\n     CUSTOM DATASET FOR LANGUAGE MODELING\n    This creates training examples for our language model:\n     What it does:\n    - Takes a long sequence of tokens\n    - Creates sliding windows of fixed length\n    - Each example: input sequence + target sequence (shifted by 1)\n     Example:\n    Original text: \"The cat sat on the mat\"",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def set_seed(seed: int = 42):\n    \"\"\"Set all random seeds for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f\" Set all seeds to {seed}\")\n@dataclass",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n     KEY COMPONENT: Grouped-Query Attention (GQA)\n    GQA is a memory-efficient attention mechanism where:\n    - We have fewer Key-Value heads than Query heads\n    - Each KV head is \"repeated\" to match the number of Query heads\n    - This reduces memory usage while maintaining performance\n    Example:\n    - 4 Query heads, 2 KV heads  each KV head repeated 2 times\n    - Memory savings: 50% reduction in KV cache memory",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "zeropower_via_newtonschulz5",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n    \"\"\"\n     NEWTON-SCHULZ ORTHOGONALIZATION\n    This is the mathematical heart of the Muon optimizer:\n     What it does:\n    - Takes a matrix G (gradients)\n    - Makes it \"orthogonal\" (like rotating it to be perfectly aligned)\n    - Uses Newton-Schulz iteration (a fast numerical method)\n     Why orthogonalization helps:\n    - Orthogonal matrices preserve vector lengths and angles",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def load_and_cache_data(config: SmallModelConfig, cache_dir: str = \"data_cache\"):\n    \"\"\"\n     SMART DATA LOADING WITH CACHING\n    This function demonstrates modern ML data handling:\n     Key features:\n    1. Caching: Avoids reprocessing the same data\n    2. Streaming: Loads large datasets without memory issues\n    3. Tokenization: Converts text to numbers the model can understand\n    4. Efficient storage: Uses pickle for fast loading\n     The process:",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "CachedAttention",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.attention.cached_attention",
        "description": "qwen-llm.fast_inference.core.attention.cached_attention",
        "peekOfCode": "class CachedAttention(nn.Module):\n    \"\"\"\n     CACHED ATTENTION\n    Attention layer with simple KV caching for fast inference.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'SimpleKVCache'):\n        \"\"\"\n        Initialize cached attention layer.\n        Args:\n            config: Model configuration",
        "detail": "qwen-llm.fast_inference.core.attention.cached_attention",
        "documentation": {}
    },
    {
        "label": "OptimizedAttention",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.attention.optimized_attention",
        "description": "qwen-llm.fast_inference.core.attention.optimized_attention",
        "peekOfCode": "class OptimizedAttention(nn.Module):\n    \"\"\"\n     OPTIMIZED ATTENTION WITH PAGED KV CACHE\n    Advanced attention layer with paged KV caching for maximum performance.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'PagedKVCache'):\n        \"\"\"\n        Initialize optimized attention layer.\n        Args:\n            config: Model configuration",
        "detail": "qwen-llm.fast_inference.core.attention.optimized_attention",
        "documentation": {}
    },
    {
        "label": "PagedKVCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.cache.paged_cache",
        "description": "qwen-llm.fast_inference.core.cache.paged_cache",
        "peekOfCode": "class PagedKVCache(nn.Module):\n    \"\"\"\n     PAGED KV CACHE\n    Efficient memory management for KV cache using page-based allocation.\n    Inspired by vLLM's PagedAttention but simplified for our use case.\n    \"\"\"\n    def __init__(self, n_pages: int, page_size: int, n_heads: int, head_dim: int, dtype: torch.dtype, device: str):\n        \"\"\"\n        Initialize paged KV cache.\n        Args:",
        "detail": "qwen-llm.fast_inference.core.cache.paged_cache",
        "documentation": {}
    },
    {
        "label": "SimpleKVCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.cache.simple_cache",
        "description": "qwen-llm.fast_inference.core.cache.simple_cache",
        "peekOfCode": "class SimpleKVCache:\n    \"\"\"\n     SIMPLE KV CACHE\n    A straightforward KV cache implementation that stores key-value pairs\n    for each sequence position. Much simpler than paged attention but still effective.\n    \"\"\"\n    def __init__(self, max_seq_len: int, n_heads: int, head_dim: int, dtype: torch.dtype, device: str):\n        \"\"\"\n        Initialize simple KV cache.\n        Args:",
        "detail": "qwen-llm.fast_inference.core.cache.simple_cache",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "description": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "peekOfCode": "class Sequence:\n    \"\"\"Represents a single generation sequence.\"\"\"\n    seq_id: int\n    prompt: str\n    input_ids: torch.Tensor\n    output_ids: List[int]\n    sampling_params: 'SamplingParams'\n    batch_idx: int = -1\n    finished: bool = False\n    prefill_done: bool = False",
        "detail": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "documentation": {}
    },
    {
        "label": "FastInferenceEngine",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "description": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "peekOfCode": "class FastInferenceEngine:\n    \"\"\"\n     FAST INFERENCE ENGINE\n    High-performance inference engine with:\n    - Paged KV cache for memory efficiency\n    - Continuous batching for throughput\n    - CUDA graphs for optimization\n    - Dynamic scheduling\n    \"\"\"\n    def __init__(self, model: nn.Module, tokenizer, config, ",
        "detail": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "documentation": {}
    },
    {
        "label": "OptimizedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "description": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "peekOfCode": "class OptimizedTransformerBlock(nn.Module):\n    \"\"\"\n     OPTIMIZED TRANSFORMER BLOCK\n    Transformer block with optimized attention and KV caching.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'PagedKVCache'):\n        super().__init__()\n        self.attention = OptimizedAttention(config, kv_cache)\n        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n        # Pre-norm architecture",
        "detail": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "documentation": {}
    },
    {
        "label": "create_fast_inference_engine",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "description": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "peekOfCode": "def create_fast_inference_engine(model_path: str, tokenizer_path: str, \n                                max_batch_size: int = 32, max_seq_len: int = 2048,\n                                n_pages: int = 1000, page_size: int = 128) -> FastInferenceEngine:\n    \"\"\"\n    Create a fast inference engine from saved model.\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        max_batch_size: Maximum batch size\n        max_seq_len: Maximum sequence length",
        "detail": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.simple_engine",
        "description": "qwen-llm.fast_inference.core.engine.simple_engine",
        "peekOfCode": "class SimpleFastInference:\n    \"\"\"\n     SIMPLE FAST INFERENCE ENGINE\n    A simplified but fast inference engine that adds KV caching to your existing model.\n    Much easier to understand and integrate than full vLLM-style implementations.\n    \"\"\"\n    def __init__(self, model: nn.Module, tokenizer, config, max_seq_len: int = 2048):\n        \"\"\"\n        Initialize simple fast inference engine.\n        Args:",
        "detail": "qwen-llm.fast_inference.core.engine.simple_engine",
        "documentation": {}
    },
    {
        "label": "CachedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.simple_engine",
        "description": "qwen-llm.fast_inference.core.engine.simple_engine",
        "peekOfCode": "class CachedTransformerBlock(nn.Module):\n    \"\"\"\n     CACHED TRANSFORMER BLOCK\n    Transformer block with cached attention.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'SimpleKVCache'):\n        super().__init__()\n        self.attention = CachedAttention(config, kv_cache)\n        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n        # Pre-norm architecture",
        "detail": "qwen-llm.fast_inference.core.engine.simple_engine",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.core.engine.simple_engine",
        "description": "qwen-llm.fast_inference.core.engine.simple_engine",
        "peekOfCode": "def create_simple_fast_inference(model_path: str, tokenizer_path: str, \n                                max_seq_len: int = 2048) -> SimpleFastInference:\n    \"\"\"\n    Create a simple fast inference engine from saved model.\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        max_seq_len: Maximum sequence length\n    Returns:\n        SimpleFastInference instance",
        "detail": "qwen-llm.fast_inference.core.engine.simple_engine",
        "documentation": {}
    },
    {
        "label": "InferenceRequest",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.examples.advanced.production_server",
        "description": "qwen-llm.fast_inference.examples.advanced.production_server",
        "peekOfCode": "class InferenceRequest:\n    \"\"\"Request structure for inference.\"\"\"\n    prompt: str\n    max_new_tokens: int = 100\n    temperature: float = 0.8\n    top_k: int = 50\n    top_p: float = 0.9\n    request_id: Optional[str] = None\n@dataclass\nclass InferenceResponse:",
        "detail": "qwen-llm.fast_inference.examples.advanced.production_server",
        "documentation": {}
    },
    {
        "label": "InferenceResponse",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.examples.advanced.production_server",
        "description": "qwen-llm.fast_inference.examples.advanced.production_server",
        "peekOfCode": "class InferenceResponse:\n    \"\"\"Response structure for inference.\"\"\"\n    request_id: str\n    generated_text: str\n    generation_time: float\n    tokens_generated: int\n    success: bool\n    error_message: Optional[str] = None\nclass InferenceServer:\n    \"\"\"",
        "detail": "qwen-llm.fast_inference.examples.advanced.production_server",
        "documentation": {}
    },
    {
        "label": "InferenceServer",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.examples.advanced.production_server",
        "description": "qwen-llm.fast_inference.examples.advanced.production_server",
        "peekOfCode": "class InferenceServer:\n    \"\"\"\n    Production-ready inference server.\n    This class provides a robust inference server with:\n    - Request queuing and batching\n    - Error handling and recovery\n    - Performance monitoring\n    - Health checks\n    - Graceful shutdown\n    \"\"\"",
        "detail": "qwen-llm.fast_inference.examples.advanced.production_server",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "qwen-llm.fast_inference.examples.advanced.production_server",
        "description": "qwen-llm.fast_inference.examples.advanced.production_server",
        "peekOfCode": "logger = logging.getLogger(__name__)\n@dataclass\nclass InferenceRequest:\n    \"\"\"Request structure for inference.\"\"\"\n    prompt: str\n    max_new_tokens: int = 100\n    temperature: float = 0.8\n    top_k: int = 50\n    top_p: float = 0.9\n    request_id: Optional[str] = None",
        "detail": "qwen-llm.fast_inference.examples.advanced.production_server",
        "documentation": {}
    },
    {
        "label": "naive_generate_text",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "description": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "peekOfCode": "def naive_generate_text(model, tokenizer, prompt: str, max_length: int = 100,\n                       temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9) -> str:\n    \"\"\"\n    Naive text generation without KV caching (for comparison).\n    This is the original text generation method that processes the entire sequence\n    for each new token. Very slow but simple.\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    # Tokenize prompt",
        "detail": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "description": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "peekOfCode": "def main():\n    \"\"\"Main function demonstrating performance comparison.\"\"\"\n    print(\" Performance Comparison Example\")\n    print(\"=\" * 50)\n    # Test prompts\n    test_prompts = [\n        \"Hello, how are you today?\",\n        \"Tell me a joke about programming\",\n        \"Write a short story about\",\n        \"Explain the concept of machine learning\",",
        "detail": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.basic.quick_start",
        "description": "qwen-llm.fast_inference.examples.basic.quick_start",
        "peekOfCode": "def main():\n    \"\"\"Main function demonstrating basic usage.\"\"\"\n    print(\" Fast Inference Quick Start Example\")\n    print(\"=\" * 50)\n    # Example 1: Single text generation\n    print(\"\\n Example 1: Single Text Generation\")\n    print(\"-\" * 40)\n    try:\n        # Create engine (you'll need to provide actual model paths)\n        engine = create_simple_fast_inference(",
        "detail": "qwen-llm.fast_inference.examples.basic.quick_start",
        "documentation": {}
    },
    {
        "label": "TestSimpleFastInference",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.integration.test_engine",
        "description": "qwen-llm.fast_inference.tests.integration.test_engine",
        "peekOfCode": "class TestSimpleFastInference:\n    \"\"\"Integration tests for SimpleFastInference.\"\"\"\n    @pytest.fixture\n    def mock_model(self):\n        \"\"\"Create a mock model for testing.\"\"\"\n        model = Mock()\n        model.parameters.return_value = [torch.tensor([1.0])]\n        model.dtype = torch.float16\n        model.transformer_blocks = [Mock() for _ in range(2)]\n        # Mock transformer block components",
        "detail": "qwen-llm.fast_inference.tests.integration.test_engine",
        "documentation": {}
    },
    {
        "label": "TestSimpleKVCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.unit.test_cache",
        "description": "qwen-llm.fast_inference.tests.unit.test_cache",
        "peekOfCode": "class TestSimpleKVCache:\n    \"\"\"Test cases for SimpleKVCache.\"\"\"\n    def test_initialization(self):\n        \"\"\"Test cache initialization.\"\"\"\n        cache = SimpleKVCache(\n            max_seq_len=100,\n            n_heads=8,\n            head_dim=64,\n            dtype=torch.float16,\n            device=\"cpu\"",
        "detail": "qwen-llm.fast_inference.tests.unit.test_cache",
        "documentation": {}
    },
    {
        "label": "TestPagedKVCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.unit.test_cache",
        "description": "qwen-llm.fast_inference.tests.unit.test_cache",
        "peekOfCode": "class TestPagedKVCache:\n    \"\"\"Test cases for PagedKVCache.\"\"\"\n    def test_initialization(self):\n        \"\"\"Test cache initialization.\"\"\"\n        cache = PagedKVCache(\n            n_pages=10,\n            page_size=128,\n            n_heads=8,\n            head_dim=64,\n            dtype=torch.float16,",
        "detail": "qwen-llm.fast_inference.tests.unit.test_cache",
        "documentation": {}
    },
    {
        "label": "TestSamplingParams",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "description": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "peekOfCode": "class TestSamplingParams:\n    \"\"\"Test cases for SamplingParams.\"\"\"\n    def test_default_initialization(self):\n        \"\"\"Test default parameter initialization.\"\"\"\n        params = SamplingParams()\n        assert params.max_new_tokens == 100\n        assert params.temperature == 0.8\n        assert params.top_k == 50\n        assert params.top_p == 0.9\n        assert params.repetition_penalty == 1.0",
        "detail": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "documentation": {}
    },
    {
        "label": "TestSamplingFunctions",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "description": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "peekOfCode": "class TestSamplingFunctions:\n    \"\"\"Test cases for sampling functions.\"\"\"\n    def test_sample_greedy(self):\n        \"\"\"Test greedy sampling.\"\"\"\n        logits = torch.tensor([[1.0, 2.0, 0.5, 3.0]])\n        tokens = sample_greedy(logits)\n        assert tokens.item() == 3  # Highest logit at index 3\n    def test_sample_random(self):\n        \"\"\"Test random sampling.\"\"\"\n        logits = torch.tensor([[1.0, 1.0, 1.0, 1.0]])",
        "detail": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def device():\n    \"\"\"Get the device for testing.\"\"\"\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n@pytest.fixture\ndef temp_dir():\n    \"\"\"Create a temporary directory for testing.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        yield tmpdir\n@pytest.fixture\ndef mock_model_config():",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "temp_dir",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def temp_dir():\n    \"\"\"Create a temporary directory for testing.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        yield tmpdir\n@pytest.fixture\ndef mock_model_config():\n    \"\"\"Create a mock model configuration.\"\"\"\n    config = Mock()\n    config.d_model = 512\n    config.n_heads = 8",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "mock_model_config",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def mock_model_config():\n    \"\"\"Create a mock model configuration.\"\"\"\n    config = Mock()\n    config.d_model = 512\n    config.n_heads = 8\n    config.n_kv_heads = 8\n    config.n_kv_groups = 1\n    config.d_k = 64\n    config.d_ff = 2048\n    config.rms_norm_eps = 1e-6",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "mock_tokenizer",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def mock_tokenizer():\n    \"\"\"Create a mock tokenizer.\"\"\"\n    tokenizer = Mock()\n    tokenizer.encode = Mock(return_value=torch.tensor([1, 2, 3, 4, 5]))\n    tokenizer.decode = Mock(return_value=\"Generated text\")\n    tokenizer.eos_token_id = 2\n    tokenizer.pad_token = None\n    return tokenizer\n@pytest.fixture\ndef sample_logits():",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "sample_logits",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def sample_logits():\n    \"\"\"Create sample logits for testing.\"\"\"\n    return torch.tensor([[1.0, 2.0, 0.5, 3.0, 1.5]])\n@pytest.fixture\ndef sample_kv_tensors():\n    \"\"\"Create sample K and V tensors for testing.\"\"\"\n    k = torch.randn(8, 10, 64, dtype=torch.float16)\n    v = torch.randn(8, 10, 64, dtype=torch.float16)\n    return k, v\n@pytest.fixture",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "sample_kv_tensors",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def sample_kv_tensors():\n    \"\"\"Create sample K and V tensors for testing.\"\"\"\n    k = torch.randn(8, 10, 64, dtype=torch.float16)\n    v = torch.randn(8, 10, 64, dtype=torch.float16)\n    return k, v\n@pytest.fixture\ndef sample_prompts():\n    \"\"\"Create sample prompts for testing.\"\"\"\n    return [\n        \"Hello, how are you?\",",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "sample_prompts",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def sample_prompts():\n    \"\"\"Create sample prompts for testing.\"\"\"\n    return [\n        \"Hello, how are you?\",\n        \"Tell me a joke about programming\",\n        \"Write a short story about\",\n        \"Explain the concept of machine learning\",\n        \"What is the meaning of life?\"\n    ]\n@pytest.fixture",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "sample_sampling_params",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def sample_sampling_params():\n    \"\"\"Create sample sampling parameters.\"\"\"\n    from fast_inference.utils.sampling import SamplingParams\n    return SamplingParams(\n        max_new_tokens=50,\n        temperature=0.8,\n        top_k=50,\n        top_p=0.9,\n        repetition_penalty=1.0\n    )",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "pytest_configure",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def pytest_configure(config):\n    \"\"\"Configure pytest markers.\"\"\"\n    config.addinivalue_line(\"markers\", \"slow: marks tests as slow\")\n    config.addinivalue_line(\"markers\", \"integration: marks tests as integration tests\")\n    config.addinivalue_line(\"markers\", \"unit: marks tests as unit tests\")\n    config.addinivalue_line(\"markers\", \"gpu: marks tests that require GPU\")\n    config.addinivalue_line(\"markers\", \"cpu: marks tests that run on CPU only\")\ndef pytest_collection_modifyitems(config, items):\n    \"\"\"Modify test collection to add markers based on test location.\"\"\"\n    for item in items:",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "pytest_collection_modifyitems",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def pytest_collection_modifyitems(config, items):\n    \"\"\"Modify test collection to add markers based on test location.\"\"\"\n    for item in items:\n        # Add markers based on test file location\n        if \"unit\" in str(item.fspath):\n            item.add_marker(pytest.mark.unit)\n        elif \"integration\" in str(item.fspath):\n            item.add_marker(pytest.mark.integration)\n        # Add slow marker to tests that might take time\n        if \"slow\" in item.name or \"benchmark\" in item.name:",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "BenchmarkResult",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "class BenchmarkResult:\n    \"\"\"\n    Results from a benchmark run.\n    Attributes:\n        method_name: Name of the inference method\n        total_time: Total time in seconds\n        total_tokens: Total number of tokens generated\n        total_requests: Total number of requests processed\n        throughput_tokens_per_sec: Tokens per second\n        throughput_requests_per_sec: Requests per second",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "BenchmarkRunner",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "class BenchmarkRunner:\n    \"\"\"\n    Benchmark runner for inference methods.\n    This class provides utilities for running benchmarks and collecting\n    performance metrics across different inference methods.\n    \"\"\"\n    def __init__(self, device: str = \"auto\"):\n        \"\"\"\n        Initialize benchmark runner.\n        Args:",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "benchmark_inference",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "def benchmark_inference(engine, test_prompts: List[str], max_new_tokens: int = 50,\n                       method_name: str = \"Fast Inference\") -> BenchmarkResult:\n    \"\"\"\n    Quick benchmark function for inference engines.\n    Args:\n        engine: Inference engine with generate_batch method\n        test_prompts: List of test prompts\n        max_new_tokens: Maximum tokens to generate\n        method_name: Name for the benchmark\n    Returns:",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "compare_methods",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "def compare_methods(methods: Dict[str, Callable], test_prompts: List[str], \n                   max_new_tokens: int = 50) -> Dict[str, Any]:\n    \"\"\"\n    Compare multiple inference methods.\n    Args:\n        methods: Dictionary mapping method names to inference functions\n        test_prompts: List of test prompts\n        max_new_tokens: Maximum tokens to generate\n    Returns:\n        Comparison results",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "generate_test_prompts",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "def generate_test_prompts(num_prompts: int = 10, \n                         min_length: int = 20, \n                         max_length: int = 100) -> List[str]:\n    \"\"\"\n    Generate test prompts for benchmarking.\n    Args:\n        num_prompts: Number of prompts to generate\n        min_length: Minimum prompt length\n        max_length: Maximum prompt length\n    Returns:",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "create_performance_report",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "def create_performance_report(results: List[BenchmarkResult], \n                            output_file: Optional[str] = None) -> str:\n    \"\"\"\n    Create a detailed performance report.\n    Args:\n        results: List of benchmark results\n        output_file: Optional file to save report\n    Returns:\n        Report as string\n    \"\"\"",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "class SamplingParams:\n    \"\"\"\n    Sampling parameters for text generation.\n    Attributes:\n        max_new_tokens: Maximum number of new tokens to generate\n        temperature: Sampling temperature (0.0 = greedy, >1.0 = more random)\n        top_k: Number of top tokens to consider (0 = no limit)\n        top_p: Cumulative probability threshold for nucleus sampling (1.0 = no limit)\n        repetition_penalty: Penalty for repeated tokens (1.0 = no penalty)\n        stop_token_ids: List of token IDs to stop generation at",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "sample_tokens",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def sample_tokens(logits: torch.Tensor, sampling_params: SamplingParams, \n                 previous_tokens: Optional[torch.Tensor] = None) -> torch.Tensor:\n    \"\"\"\n    Sample tokens from logits using the specified sampling parameters.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        sampling_params: Sampling parameters\n        previous_tokens: Previously generated tokens for repetition penalty\n    Returns:\n        Sampled token IDs (batch_size,)",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "apply_repetition_penalty",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def apply_repetition_penalty(logits: torch.Tensor, previous_tokens: torch.Tensor, \n                           penalty: float) -> torch.Tensor:\n    \"\"\"\n    Apply repetition penalty to logits.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        previous_tokens: Previously generated tokens\n        penalty: Repetition penalty factor\n    Returns:\n        Logits with repetition penalty applied",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "apply_top_k_filtering",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def apply_top_k_filtering(logits: torch.Tensor, top_k: int) -> torch.Tensor:\n    \"\"\"\n    Apply top-k filtering to logits.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        top_k: Number of top tokens to keep\n    Returns:\n        Logits with top-k filtering applied\n    \"\"\"\n    if top_k <= 0:",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "apply_top_p_filtering",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def apply_top_p_filtering(logits: torch.Tensor, top_p: float) -> torch.Tensor:\n    \"\"\"\n    Apply top-p (nucleus) filtering to logits.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        top_p: Cumulative probability threshold\n    Returns:\n        Logits with top-p filtering applied\n    \"\"\"\n    if top_p >= 1.0:",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "sample_greedy",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def sample_greedy(logits: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Greedy sampling (always pick the most likely token).\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n    Returns:\n        Greedily sampled token IDs (batch_size,)\n    \"\"\"\n    return logits.argmax(dim=-1)\ndef sample_random(logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "sample_random",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def sample_random(logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n    \"\"\"\n    Random sampling with temperature.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        temperature: Sampling temperature\n    Returns:\n        Randomly sampled token IDs (batch_size,)\n    \"\"\"\n    if temperature > 0:",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "sample_beam_search",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def sample_beam_search(logits: torch.Tensor, beam_size: int = 4, \n                      length_penalty: float = 1.0) -> List[torch.Tensor]:\n    \"\"\"\n    Beam search sampling (simplified version).\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        beam_size: Number of beams to maintain\n        length_penalty: Length penalty factor\n    Returns:\n        List of beam sequences",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def create_engine(args):\n    \"\"\"Create inference engine based on arguments.\"\"\"\n    if args.advanced:\n        return create_fast_inference_engine(\n            model_path=args.model_path,\n            tokenizer_path=args.tokenizer_path,\n            max_batch_size=args.batch_size,\n            max_seq_len=args.max_seq_len,\n            n_pages=args.n_pages,\n            page_size=args.page_size",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "cmd_generate",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def cmd_generate(args):\n    \"\"\"Generate text from prompts.\"\"\"\n    print(\" Fast Inference - Text Generation\")\n    print(\"=\" * 40)\n    try:\n        # Create engine\n        print(f\"Loading model from {args.model_path}...\")\n        engine = create_engine(args)\n        print(\" Model loaded successfully!\")\n        # Create sampling parameters",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "cmd_benchmark",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def cmd_benchmark(args):\n    \"\"\"Run performance benchmark.\"\"\"\n    print(\" Fast Inference - Performance Benchmark\")\n    print(\"=\" * 45)\n    try:\n        # Create engine\n        print(f\"Loading model from {args.model_path}...\")\n        engine = create_engine(args)\n        print(\" Model loaded successfully!\")\n        # Generate test prompts",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "cmd_compare",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def cmd_compare(args):\n    \"\"\"Compare different inference methods.\"\"\"\n    print(\" Fast Inference - Method Comparison\")\n    print(\"=\" * 40)\n    try:\n        # Create engines\n        print(f\"Loading models from {args.model_path}...\")\n        # Simple engine\n        simple_engine = create_simple_fast_inference(\n            model_path=args.model_path,",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def main():\n    \"\"\"Main CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Fast Inference Engine CLI\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  # Generate text from a prompt\n  fast-inference generate --model-path model.pt --tokenizer-path tokenizer --prompts \"Hello, world!\"\n  # Generate from file",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "naive_generate_text",
        "kind": 2,
        "importPath": "qwen-llm.original_files.compare_inference",
        "description": "qwen-llm.original_files.compare_inference",
        "peekOfCode": "def naive_generate_text(model: MinimalLLM, tokenizer, prompt: str, max_length: int = 100,\n                       temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9) -> str:\n    \"\"\"\n     NAIVE TEXT GENERATION (NO KV CACHE)\n    This is the original text generation method that processes the entire sequence\n    for each new token. Very slow but simple.\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    # Tokenize prompt",
        "detail": "qwen-llm.original_files.compare_inference",
        "documentation": {}
    },
    {
        "label": "compare_inference_methods",
        "kind": 2,
        "importPath": "qwen-llm.original_files.compare_inference",
        "description": "qwen-llm.original_files.compare_inference",
        "peekOfCode": "def compare_inference_methods(model_path: str, tokenizer_path: str, \n                            test_prompts: List[str], max_new_tokens: int = 50):\n    \"\"\"\n    Compare different inference methods\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        test_prompts: List of test prompts\n        max_new_tokens: Maximum tokens to generate\n    \"\"\"",
        "detail": "qwen-llm.original_files.compare_inference",
        "documentation": {}
    },
    {
        "label": "run_quick_test",
        "kind": 2,
        "importPath": "qwen-llm.original_files.compare_inference",
        "description": "qwen-llm.original_files.compare_inference",
        "peekOfCode": "def run_quick_test():\n    \"\"\"Run a quick test with simple prompts\"\"\"\n    test_prompts = [\n        \"Hello, how are you?\",\n        \"Tell me a joke about\",\n        \"Write a short story about\",\n        \"Explain the concept of\",\n        \"What is the meaning of\"\n    ]\n    # You'll need to provide actual model paths",
        "detail": "qwen-llm.original_files.compare_inference",
        "documentation": {}
    },
    {
        "label": "run_comprehensive_benchmark",
        "kind": 2,
        "importPath": "qwen-llm.original_files.compare_inference",
        "description": "qwen-llm.original_files.compare_inference",
        "peekOfCode": "def run_comprehensive_benchmark():\n    \"\"\"Run a comprehensive benchmark with more prompts\"\"\"\n    test_prompts = [\n        \"The quick brown fox jumps over the lazy dog. This is a test of\",\n        \"In a world where artificial intelligence has become\",\n        \"The ancient library contained thousands of books about\",\n        \"As the sun set over the mountains, the travelers\",\n        \"The scientist discovered a new element that could\",\n        \"Once upon a time, in a distant galaxy\",\n        \"The recipe for the perfect chocolate cake includes\",",
        "detail": "qwen-llm.original_files.compare_inference",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class SamplingParams:\n    \"\"\"Sampling parameters for text generation\"\"\"\n    max_new_tokens: int = 100\n    temperature: float = 0.8\n    top_k: int = 50\n    top_p: float = 0.9\n    repetition_penalty: float = 1.0\n    stop_token_ids: List[int] = None\n    def __post_init__(self):\n        if self.stop_token_ids is None:",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class Sequence:\n    \"\"\"Represents a single generation sequence\"\"\"\n    seq_id: int\n    prompt: str\n    input_ids: torch.Tensor\n    output_ids: List[int]\n    sampling_params: SamplingParams\n    batch_idx: int = -1\n    finished: bool = False\n    prefill_done: bool = False",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "PagedKVCache",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class PagedKVCache(nn.Module):\n    \"\"\"\n     PAGED KV CACHE\n    Efficient memory management for KV cache using page-based allocation.\n    Inspired by vLLM's PagedAttention but simplified for our use case.\n    \"\"\"\n    def __init__(self, n_pages: int, page_size: int, n_heads: int, head_dim: int, dtype: torch.dtype, device: str):\n        super().__init__()\n        self.n_pages = n_pages\n        self.page_size = page_size",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "OptimizedAttention",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class OptimizedAttention(nn.Module):\n    \"\"\"\n     OPTIMIZED ATTENTION WITH KV CACHE\n    Combines GQA with efficient KV caching for fast inference.\n    \"\"\"\n    def __init__(self, config: SmallModelConfig, kv_cache: PagedKVCache):\n        super().__init__()\n        self.d_model = config.d_model\n        self.n_heads = config.n_heads\n        self.n_kv_heads = config.n_kv_heads",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "OptimizedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class OptimizedTransformerBlock(nn.Module):\n    \"\"\"\n     OPTIMIZED TRANSFORMER BLOCK\n    Transformer block with optimized attention and KV caching.\n    \"\"\"\n    def __init__(self, config: SmallModelConfig, kv_cache: PagedKVCache):\n        super().__init__()\n        self.attention = OptimizedAttention(config, kv_cache)\n        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n        # Pre-norm architecture",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "FastInferenceEngine",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class FastInferenceEngine:\n    \"\"\"\n     FAST INFERENCE ENGINE\n    High-performance inference engine with:\n    - Paged KV cache for memory efficiency\n    - Continuous batching for throughput\n    - CUDA graphs for optimization\n    - Dynamic scheduling\n    \"\"\"\n    def __init__(self, model: MinimalLLM, tokenizer, config: SmallModelConfig, ",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "create_fast_inference_engine",
        "kind": 2,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "def create_fast_inference_engine(model_path: str, tokenizer_path: str, \n                                max_batch_size: int = 32, max_seq_len: int = 2048,\n                                n_pages: int = 1000, page_size: int = 128) -> FastInferenceEngine:\n    \"\"\"\n    Create a fast inference engine from saved model\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        max_batch_size: Maximum batch size\n        max_seq_len: Maximum sequence length",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "benchmark_inference_engine",
        "kind": 2,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "def benchmark_inference_engine(engine: FastInferenceEngine, num_requests: int = 100, \n                              max_input_len: int = 512, max_output_len: int = 256):\n    \"\"\"\n    Benchmark the inference engine\n    Args:\n        engine: FastInferenceEngine instance\n        num_requests: Number of requests to process\n        max_input_len: Maximum input length\n        max_output_len: Maximum output length\n    \"\"\"",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "SimpleKVCache",
        "kind": 6,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "class SimpleKVCache:\n    \"\"\"\n     SIMPLE KV CACHE\n    A straightforward KV cache implementation that stores key-value pairs\n    for each sequence position. Much simpler than paged attention but still effective.\n    \"\"\"\n    def __init__(self, max_seq_len: int, n_heads: int, head_dim: int, dtype: torch.dtype, device: str):\n        self.max_seq_len = max_seq_len\n        self.n_heads = n_heads\n        self.head_dim = head_dim",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "CachedAttention",
        "kind": 6,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "class CachedAttention(nn.Module):\n    \"\"\"\n     CACHED ATTENTION\n    Attention layer with simple KV caching for fast inference.\n    \"\"\"\n    def __init__(self, config: SmallModelConfig, kv_cache: SimpleKVCache):\n        super().__init__()\n        self.d_model = config.d_model\n        self.n_heads = config.n_heads\n        self.n_kv_heads = config.n_kv_heads",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "CachedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "class CachedTransformerBlock(nn.Module):\n    \"\"\"\n     CACHED TRANSFORMER BLOCK\n    Transformer block with cached attention.\n    \"\"\"\n    def __init__(self, config: SmallModelConfig, kv_cache: SimpleKVCache):\n        super().__init__()\n        self.attention = CachedAttention(config, kv_cache)\n        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n        # Pre-norm architecture",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "kind": 6,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "class SimpleFastInference:\n    \"\"\"\n     SIMPLE FAST INFERENCE ENGINE\n    A simplified but fast inference engine that adds KV caching to your existing model.\n    Much easier to understand and integrate than full vLLM-style implementations.\n    \"\"\"\n    def __init__(self, model: MinimalLLM, tokenizer, config: SmallModelConfig, \n                 max_seq_len: int = 2048):\n        self.model = model\n        self.tokenizer = tokenizer",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "kind": 2,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "def create_simple_fast_inference(model_path: str, tokenizer_path: str, \n                                max_seq_len: int = 2048) -> SimpleFastInference:\n    \"\"\"\n    Create a simple fast inference engine from saved model\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        max_seq_len: Maximum sequence length\n    Returns:\n        SimpleFastInference instance",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "benchmark_simple_inference",
        "kind": 2,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "def benchmark_simple_inference(engine: SimpleFastInference, num_requests: int = 10, \n                              max_input_len: int = 100, max_output_len: int = 100):\n    \"\"\"\n    Benchmark the simple inference engine\n    Args:\n        engine: SimpleFastInference instance\n        num_requests: Number of requests to process\n        max_input_len: Maximum input length\n        max_output_len: Maximum output length\n    \"\"\"",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "BenchmarkConfig",
        "kind": 6,
        "importPath": "qwen-llm.benchmark_quantization",
        "description": "qwen-llm.benchmark_quantization",
        "peekOfCode": "class BenchmarkConfig:\n    \"\"\"\n     BENCHMARK CONFIGURATION\n    \"\"\"\n    # Model parameters\n    d_model: int = 128\n    n_layers: int = 3\n    vocab_size: int = 1000\n    # Benchmark parameters\n    num_samples: int = 100",
        "detail": "qwen-llm.benchmark_quantization",
        "documentation": {}
    },
    {
        "label": "QuantizationBenchmarker",
        "kind": 6,
        "importPath": "qwen-llm.benchmark_quantization",
        "description": "qwen-llm.benchmark_quantization",
        "peekOfCode": "class QuantizationBenchmarker:\n    \"\"\"\n     QUANTIZATION BENCHMARKER\n    Comprehensive benchmarking of different quantization techniques.\n    \"\"\"\n    def __init__(self, config: BenchmarkConfig):\n        self.config = config\n        self.results = {}\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        # Create test data",
        "detail": "qwen-llm.benchmark_quantization",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.benchmark_quantization",
        "description": "qwen-llm.benchmark_quantization",
        "peekOfCode": "def main():\n    \"\"\"Main benchmark function\"\"\"\n    print(\" QUANTIZATION BENCHMARK\")\n    print(\"=\" * 40)\n    # Create benchmark config\n    config = BenchmarkConfig()\n    config.d_model = 128\n    config.n_layers = 3\n    config.vocab_size = 1000\n    config.num_samples = 100",
        "detail": "qwen-llm.benchmark_quantization",
        "documentation": {}
    },
    {
        "label": "convert_checkpoint",
        "kind": 2,
        "importPath": "qwen-llm.convert_checkpoint",
        "description": "qwen-llm.convert_checkpoint",
        "peekOfCode": "def convert_checkpoint(old_path: str, new_path: str):\n    \"\"\"\n    Convert old checkpoint to new format\n    \"\"\"\n    print(f\" Converting checkpoint from {old_path} to {new_path}\")\n    try:\n        # Try to load the old checkpoint\n        print(\" Loading old checkpoint...\")\n        checkpoint = torch.load(old_path, map_location='cpu', weights_only=True)\n        # Extract only the essential data",
        "detail": "qwen-llm.convert_checkpoint",
        "documentation": {}
    },
    {
        "label": "demo_finetune",
        "kind": 2,
        "importPath": "qwen-llm.demo_finetune",
        "description": "qwen-llm.demo_finetune",
        "peekOfCode": "def demo_finetune():\n    \"\"\"\n     DEMO FINE-TUNING PROCESS\n    Demonstrates the fine-tuning process with a small subset of data.\n    \"\"\"\n    print(\" IMDB Sentiment Analysis Fine-tuning Demo\")\n    print(\"=\" * 50)\n    # Create config for demo (smaller dataset, fewer epochs)\n    config = FineTuneConfig()\n    config.batch_size = 8",
        "detail": "qwen-llm.demo_finetune",
        "documentation": {}
    },
    {
        "label": "FineTuneConfig",
        "kind": 6,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "class FineTuneConfig:\n    \"\"\"\n     FINE-TUNING CONFIGURATION\n    Configuration for fine-tuning the pre-trained model on IMDB sentiment analysis.\n    \"\"\"\n    # Model architecture\n    d_model: int = 128\n    n_heads: int = 4\n    n_layers: int = 3\n    d_ff: int = 512",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "IMDBDataset",
        "kind": 6,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "class IMDBDataset(Dataset):\n    \"\"\"\n     IMDB DATASET CLASS\n    Custom dataset class for IMDB sentiment analysis.\n    Handles tokenization and padding of movie reviews.\n    \"\"\"\n    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "SentimentClassifier",
        "kind": 6,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "class SentimentClassifier(nn.Module):\n    \"\"\"\n     SENTIMENT CLASSIFICATION MODEL\n    This model adds a classification head on top of our pre-trained Qwen3 model.\n    It freezes the pre-trained weights and only trains the classification layer.\n    \"\"\"\n    def __init__(self, pretrained_model: MinimalLLM, num_classes: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Freeze pre-trained model parameters",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "load_imdb_data",
        "kind": 2,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "def load_imdb_data(config: FineTuneConfig):\n    \"\"\"\n     LOAD IMDB DATASET\n    Loads and prepares the IMDB sentiment analysis dataset.\n    \"\"\"\n    print(\" Loading IMDB dataset...\")\n    # Load dataset\n    dataset = load_dataset(\"imdb\")\n    # Load tokenizer (same as used in pre-training)\n    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "def evaluate_model(model: SentimentClassifier, test_loader: DataLoader, config: FineTuneConfig):\n    \"\"\"\n     EVALUATE MODEL PERFORMANCE\n    Evaluates the model on the test set and returns accuracy and loss.\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "fine_tune_model",
        "kind": 2,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "def fine_tune_model(config: FineTuneConfig, pretrained_model_path: str):\n    \"\"\"\n     FINE-TUNE MODEL ON IMDB SENTIMENT ANALYSIS\n    Fine-tunes the pre-trained model on IMDB sentiment analysis task.\n    \"\"\"\n    print(\" Starting IMDB Sentiment Analysis Fine-tuning\")\n    print(\"=\" * 60)\n    # Set seed for reproducibility\n    set_seed(42)\n    # Load data",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "test_sentiment_classifier",
        "kind": 2,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "def test_sentiment_classifier(model_path: str = \"models/imdb_sentiment_classifier.pt\"):\n    \"\"\"\n     TEST SENTIMENT CLASSIFIER\n    Tests the fine-tuned sentiment classifier on sample texts.\n    \"\"\"\n    print(\" Testing Sentiment Classifier\")\n    print(\"=\" * 40)\n    # Load model\n    checkpoint = torch.load(model_path, map_location='cpu')\n    config = checkpoint['config']",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "LoRATrainingConfig",
        "kind": 6,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "class LoRATrainingConfig:\n    \"\"\"\n     LORA TRAINING CONFIGURATION\n    \"\"\"\n    # Model parameters\n    pretrained_model_path: str = \"models/final_model1.pt\"\n    # LoRA parameters\n    lora_rank: int = 16\n    lora_alpha: float = 32.0\n    lora_dropout: float = 0.1",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "LoRADataset",
        "kind": 6,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "class LoRADataset(Dataset):\n    \"\"\"\n     LORA DATASET CLASS\n    Custom dataset for LoRA fine-tuning on IMDB sentiment analysis.\n    \"\"\"\n    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 256):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "LoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "class LoRAClassifier(nn.Module):\n    \"\"\"\n     LORA CLASSIFIER\n    Combines pre-trained model with LoRA adaptation and classification head.\n    \"\"\"\n    def __init__(self, pretrained_model: MinimalLLM, num_classes: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Add classification head\n        self.classifier = nn.Sequential(",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "load_data_for_lora",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def load_data_for_lora(config: LoRATrainingConfig):\n    \"\"\"\n     LOAD DATA FOR LORA FINE-TUNING\n    \"\"\"\n    print(\" Loading data for LoRA fine-tuning...\")\n    # Load dataset\n    if config.dataset_name == \"imdb\":\n        dataset = load_dataset(\"imdb\")\n        train_texts = dataset['train']['text'][:config.num_samples]\n        train_labels = dataset['train']['label'][:config.num_samples]",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "setup_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def setup_lora_model(config: LoRATrainingConfig, tokenizer):\n    \"\"\"\n     SETUP LORA MODEL\n    Loads pre-trained model and applies LoRA adaptation.\n    \"\"\"\n    print(f\" Setting up LoRA model...\")\n    # Load pre-trained model\n    print(f\" Loading pre-trained model from {config.pretrained_model_path}\")\n    checkpoint = torch.load(config.pretrained_model_path, map_location='cpu')\n    # Create model config",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "train_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def train_lora_model(classifier: LoRAClassifier, lora_manager: LoRAManager, \n                    train_loader: DataLoader, test_loader: DataLoader, config: LoRATrainingConfig):\n    \"\"\"\n     TRAIN LORA MODEL\n    Fine-tunes the model using LoRA adaptation.\n    \"\"\"\n    print(f\" Starting LoRA fine-tuning...\")\n    # Setup optimizer (only for trainable parameters)\n    trainable_params = lora_manager.get_trainable_parameters()\n    trainable_params.extend([p for p in classifier.classifier.parameters()])",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "evaluate_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def evaluate_lora_model(classifier: LoRAClassifier, test_loader: DataLoader, config: LoRATrainingConfig):\n    \"\"\"\n     EVALUATE LORA MODEL\n    \"\"\"\n    classifier.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "save_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def save_lora_model(classifier: LoRAClassifier, lora_manager: LoRAManager, \n                   config: LoRATrainingConfig, final_accuracy: float):\n    \"\"\"\n     SAVE LORA MODEL\n    Saves the LoRA-adapted model and LoRA weights separately.\n    \"\"\"\n    print(f\" Saving LoRA model...\")\n    # Save LoRA weights\n    lora_weights = {}\n    for name, lora_layer in lora_manager.lora_layers.items():",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "test_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def test_lora_model(model_path: str = \"models/lora_sentiment_classifier.pt\"):\n    \"\"\"\n     TEST LORA MODEL\n    Tests the LoRA fine-tuned model on sample texts.\n    \"\"\"\n    print(\" Testing LoRA Model\")\n    print(\"=\" * 40)\n    # Load model\n    checkpoint = torch.load(model_path, map_location='cpu')\n    config = checkpoint['config']",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def main():\n    \"\"\"\n     MAIN LORA FINE-TUNING FUNCTION\n    \"\"\"\n    print(\" LORA FINE-TUNING FOR SENTIMENT ANALYSIS\")\n    print(\"=\" * 60)\n    # Set seed\n    set_seed(42)\n    # Create config\n    config = LoRATrainingConfig()",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "QLoRATrainingConfig",
        "kind": 6,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "class QLoRATrainingConfig:\n    \"\"\"\n     QLORA TRAINING CONFIGURATION\n    \"\"\"\n    # Model parameters\n    pretrained_model_path: str = \"models/final_model1.pt\"\n    # QLoRA parameters\n    lora_rank: int = 16\n    lora_alpha: float = 32.0\n    lora_dropout: float = 0.1",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "QLoRADataset",
        "kind": 6,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "class QLoRADataset(Dataset):\n    \"\"\"\n     QLORA DATASET CLASS\n    Custom dataset for QLoRA fine-tuning on IMDB sentiment analysis.\n    \"\"\"\n    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 256):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "QLoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "class QLoRAClassifier(nn.Module):\n    \"\"\"\n     QLORA CLASSIFIER\n    Combines pre-trained model with QLoRA adaptation and classification head.\n    \"\"\"\n    def __init__(self, pretrained_model: MinimalLLM, num_classes: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Add classification head\n        self.classifier = nn.Sequential(",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "load_data_for_qlora",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def load_data_for_qlora(config: QLoRATrainingConfig):\n    \"\"\"\n     LOAD DATA FOR QLORA FINE-TUNING\n    \"\"\"\n    print(\" Loading data for QLoRA fine-tuning...\")\n    # Load dataset\n    if config.dataset_name == \"imdb\":\n        dataset = load_dataset(\"imdb\")\n        train_texts = dataset['train']['text'][:config.num_samples]\n        train_labels = dataset['train']['label'][:config.num_samples]",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "setup_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def setup_qlora_model(config: QLoRATrainingConfig, tokenizer):\n    \"\"\"\n     SETUP QLORA MODEL\n    Loads pre-trained model and applies QLoRA adaptation.\n    \"\"\"\n    print(f\" Setting up QLoRA model...\")\n    # Load pre-trained model\n    print(f\" Loading pre-trained model from {config.pretrained_model_path}\")\n    checkpoint = torch.load(config.pretrained_model_path, map_location='cpu')\n    # Create model config",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "train_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def train_qlora_model(classifier: QLoRAClassifier, qlora_manager: QLoRAManager, \n                      train_loader: DataLoader, test_loader: DataLoader, config: QLoRATrainingConfig):\n    \"\"\"\n     TRAIN QLORA MODEL\n    Fine-tunes the model using QLoRA adaptation.\n    \"\"\"\n    print(f\" Starting QLoRA fine-tuning...\")\n    # Setup optimizer (only for trainable parameters)\n    trainable_params = qlora_manager.get_trainable_parameters()\n    trainable_params.extend([p for p in classifier.classifier.parameters()])",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "evaluate_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def evaluate_qlora_model(classifier: QLoRAClassifier, test_loader: DataLoader, config: QLoRATrainingConfig):\n    \"\"\"\n     EVALUATE QLORA MODEL\n    \"\"\"\n    classifier.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "save_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def save_qlora_model(classifier: QLoRAClassifier, qlora_manager: QLoRAManager, \n                     config: QLoRATrainingConfig, final_accuracy: float):\n    \"\"\"\n     SAVE QLORA MODEL\n    Saves the QLoRA-adapted model and QLoRA weights separately.\n    \"\"\"\n    print(f\" Saving QLoRA model...\")\n    # Save QLoRA weights\n    qlora_weights = {}\n    for name, qlora_layer in qlora_manager.qlora_layers.items():",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "test_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def test_qlora_model(model_path: str = \"models/qlora_sentiment_classifier.pt\"):\n    \"\"\"\n     TEST QLORA MODEL\n    Tests the QLoRA fine-tuned model on sample texts.\n    \"\"\"\n    print(\" Testing QLoRA Model\")\n    print(\"=\" * 40)\n    # Load model\n    checkpoint = torch.load(model_path, map_location='cpu')\n    config = checkpoint['config']",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def main():\n    \"\"\"\n     MAIN QLORA FINE-TUNING FUNCTION\n    \"\"\"\n    print(\" QLORA FINE-TUNING FOR SENTIMENT ANALYSIS\")\n    print(\"=\" * 60)\n    # Set seed\n    set_seed(42)\n    # Create config\n    config = QLoRATrainingConfig()",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QuantizationConfig:\n    \"\"\"\n     QUANTIZATION CONFIGURATION\n    Configuration for different quantization techniques.\n    \"\"\"\n    # Basic quantization\n    bits: int = 8  # Number of bits for quantization (4, 8, 16)\n    symmetric: bool = True  # Symmetric vs asymmetric quantization\n    # LoRA parameters\n    lora_rank: int = 16  # Rank of LoRA adaptation",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationExpert",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QuantizationExpert:\n    \"\"\"\n     QUANTIZATION EXPERT CLASS\n    This class demonstrates different quantization techniques and their trade-offs.\n    \"\"\"\n    def __init__(self):\n        self.quantization_methods = {\n            'fp32': self._fp32_quantization,\n            'fp16': self._fp16_quantization,\n            'int8': self._int8_quantization,",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class LoRALayer(nn.Module):\n    \"\"\"\n     LORA LAYER IMPLEMENTATION\n    LoRA (Low-Rank Adaptation) decomposes weight updates into low-rank matrices.\n    Mathematical Foundation:\n    W = W + W = W + BA\n    Where:\n    - W: Original frozen weights\n    - B: Low-rank matrix (d  r)\n    - A: Low-rank matrix (r  k)",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class LoRALinear(nn.Module):\n    \"\"\"\n     LORA LINEAR LAYER\n    Combines original linear layer with LoRA adaptation.\n    \"\"\"\n    def __init__(self, original_layer: nn.Linear, rank: int = 16, alpha: float = 32.0, dropout: float = 0.1):\n        super().__init__()\n        self.original_layer = original_layer\n        self.lora = LoRALayer(\n            original_layer.in_features,",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class LoRAManager:\n    \"\"\"\n     LORA MANAGER\n    Manages LoRA adaptation for entire models.\n    \"\"\"\n    def __init__(self, model: nn.Module, config: QuantizationConfig):\n        self.model = model\n        self.config = config\n        self.lora_layers = {}\n        self.original_layers = {}",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QLoRALayer(nn.Module):\n    \"\"\"\n     QLORA LAYER IMPLEMENTATION\n    QLoRA combines LoRA with 4-bit quantization for maximum efficiency.\n    Key Innovation:\n    - Quantizes base model to 4-bit\n    - Uses LoRA for adaptation\n    - Enables fine-tuning on consumer hardware\n    Memory Savings:\n    - 4-bit quantization: 8x reduction",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QLoRALinear(nn.Module):\n    \"\"\"\n     QLORA LINEAR LAYER\n    Combines quantized base weights with LoRA adaptation.\n    \"\"\"\n    def __init__(self, original_layer: nn.Linear, rank: int = 16, alpha: float = 32.0, \n                 dropout: float = 0.1, bits: int = 4):\n        super().__init__()\n        self.original_layer = original_layer\n        self.qlora = QLoRALayer(",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QLoRAManager:\n    \"\"\"\n     QLORA MANAGER\n    Manages QLoRA adaptation for entire models.\n    \"\"\"\n    def __init__(self, model: nn.Module, config: QuantizationConfig):\n        self.model = model\n        self.config = config\n        self.qlora_layers = {}\n        self.original_layers = {}",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationBenchmark",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QuantizationBenchmark:\n    \"\"\"\n     QUANTIZATION BENCHMARK\n    Comprehensive benchmarking of different quantization techniques.\n    \"\"\"\n    def __init__(self):\n        self.results = {}\n    def benchmark_model(self, model: nn.Module, test_data: torch.Tensor, \n                       configs: List[QuantizationConfig]) -> Dict:\n        \"\"\"",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "demo_quantization",
        "kind": 2,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "def demo_quantization():\n    \"\"\"\n     QUANTIZATION DEMO\n    Demonstrates different quantization techniques on our Qwen3 model.\n    \"\"\"\n    print(\" QUANTIZATION EXPERT TUTORIAL\")\n    print(\"=\" * 50)\n    # Create a small model for demo\n    config = SmallModelConfig()\n    config.d_model = 64  # Smaller for demo",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "demo_lora",
        "kind": 2,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "def demo_lora():\n    \"\"\"\n     LORA DEMO\n    Demonstrates LoRA adaptation on our Qwen3 model.\n    \"\"\"\n    print(\"\\n LORA (LOW-RANK ADAPTATION) DEMO\")\n    print(\"=\" * 50)\n    # Create model\n    config = SmallModelConfig()\n    config.d_model = 128",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "demo_qlora",
        "kind": 2,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "def demo_qlora():\n    \"\"\"\n     QLORA DEMO\n    Demonstrates QLoRA adaptation on our Qwen3 model.\n    \"\"\"\n    print(\"\\n QLORA (QUANTIZED LORA) DEMO\")\n    print(\"=\" * 50)\n    # Create model\n    config = SmallModelConfig()\n    config.d_model = 128",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "class MinimalLLM(nn.Module):\n    \"\"\"\n     COMPLETE QWEN3-STYLE LANGUAGE MODEL\n    This is the full language model that combines all components:\n     Architecture:\n    1. Token Embedding: Convert token IDs to vectors\n    2. Positional Dropout: Prevent overfitting on positions\n    3. Transformer Blocks: Stack of attention + feed-forward layers\n    4. Final Normalization: RMSNorm before output\n    5. Language Modeling Head: Convert vectors back to token probabilities",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def evaluate_model(model: nn.Module, val_loader: DataLoader, config: SmallModelConfig):\n    \"\"\"\n     MODEL EVALUATION FUNCTION\n    This function evaluates the model's performance on validation data:\n     Metrics Computed:\n    1. Loss: Cross-entropy loss (lower is better)\n    2. Accuracy: Percentage of correct next-token predictions\n    3. Perplexity: exp(loss) - measures model's \"surprise\" (lower is better)\n     Why these metrics matter:\n    - Loss: Direct measure of how well the model predicts",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "setup_muon_optimizer",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def setup_muon_optimizer(model: nn.Module, config: SmallModelConfig):\n    \"\"\"\n     HYBRID OPTIMIZER SETUP\n    This function sets up a hybrid optimization strategy:\n    - Muon optimizer for 2D parameters (attention and feed-forward weights)\n    - AdamW optimizer for other parameters (embeddings, norms, biases)\n     Why hybrid approach:\n    - Muon works best on 2D matrices (attention, feed-forward)\n    - AdamW is better for 1D parameters (embeddings, biases)\n    - This gives us the best of both worlds",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def load_checkpoint(model_path: str, config: SmallModelConfig):\n    \"\"\"\n     LOAD CHECKPOINT FOR RESUMING TRAINING\n    This function loads a previously trained model checkpoint and returns\n    the model, optimizers, schedulers, and training state.\n    \"\"\"\n    print(f\" Loading checkpoint from {model_path}\")\n    # Load checkpoint with safe loading to handle import path changes\n    try:\n        # Try loading with weights_only=False to handle custom classes",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "resume_training",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def resume_training(config: SmallModelConfig, train_loader: DataLoader, val_loader: DataLoader, checkpoint_path: str):\n    \"\"\"\n     RESUME TRAINING FROM CHECKPOINT\n    This function resumes training from a previously saved checkpoint.\n    It loads the model state and continues training from where it left off.\n    Args:\n        config: Model configuration\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        checkpoint_path: Path to the checkpoint file",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def train_model(config: SmallModelConfig, train_loader: DataLoader, val_loader: DataLoader):\n    \"\"\"\n     COMPLETE TRAINING LOOP\n    This is the heart of the training process, implementing:\n     Key Features:\n    1. Gradient Accumulation: Simulate larger batch sizes\n    2. Mixed Precision: Faster training with minimal accuracy loss\n    3. Learning Rate Scheduling: Warmup + cosine decay\n    4. Gradient Clipping: Prevent gradient explosions\n    5. Model Checkpointing: Save best model",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def generate_text(model: nn.Module, tokenizer, prompt: str, max_length: int = 100,\n                 temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9):\n    \"\"\"\n     TEXT GENERATION FUNCTION\n    This function generates text using the trained model with advanced sampling:\n     Sampling Strategies:\n    1. Temperature Scaling: Controls randomness (0.1 = focused, 2.0 = random)\n    2. Top-k Sampling: Only consider top k most likely tokens\n    3. Top-p (Nucleus) Sampling: Consider tokens until cumulative probability reaches p\n     How it works:",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class Rotary(nn.Module):\n    \"\"\"\n     ROTARY POSITIONAL EMBEDDINGS (RoPE)\n    This is one of the most important innovations in modern transformers!\n     What RoPE does:\n    - Encodes position information by ROTATING vectors\n    - Unlike traditional positional embeddings that just ADD position info\n    - Allows the model to understand relative positions naturally\n     The Math:\n    - For each position i, we compute rotation angles based on i",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class Qwen3Attention(nn.Module):\n    \"\"\"\n     GROUPED-QUERY ATTENTION (GQA) IMPLEMENTATION\n    This is the heart of modern transformer attention mechanisms!\n     Key Innovations:\n    1. Grouped-Query Attention: Fewer KV heads than Query heads\n    2. QK-Normalization: Normalizes queries and keys for stability\n    3. RoPE: Rotary positional embeddings\n    4. Scaled dot-product attention: The core attention mechanism\n     How it works:",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class SwiGLUFeedForward(nn.Module):\n    \"\"\"\n     SWIGLU FEED-FORWARD NETWORK\n    SwiGLU is a modern activation function that combines:\n    - Swish activation: x * sigmoid(x) (smooth, non-monotonic)\n    - GLU (Gated Linear Unit): element-wise multiplication with a gate\n     The Math:\n    SwiGLU(x) = Swish(W1(x))  W2(x)\n    Where:\n    - W1(x) is the \"gate\" (controls information flow)",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class TransformerBlock(nn.Module):\n    \"\"\"\n     TRANSFORMER BLOCK - THE BUILDING BLOCK OF MODERN LLMs\n    This combines all the components into a complete transformer layer:\n     Architecture:\n    1. Pre-norm attention (RMSNorm before attention)\n    2. Residual connection (x + attention(x))\n    3. Pre-norm feed-forward (RMSNorm before SwiGLU)\n    4. Residual connection (x + feedforward(x))\n     Pre-norm vs Post-norm:",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class RMSNorm(nn.Module):\n    \"\"\"\n     RMSNorm - ROOT MEAN SQUARE NORMALIZATION\n    This is a modern alternative to LayerNorm that's more efficient:\n     The Math:\n    RMSNorm(x) = x / sqrt(mean(x) + ) * g\n    Where:\n    - x is the input\n    - mean(x) is the mean of squared values\n    -  is a small constant (1e-6)",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n     GROUPED-QUERY ATTENTION HELPER\n    This implements the key innovation in GQA:\n    - Fewer Key-Value heads than Query heads\n    - Each KV head is \"shared\" across multiple Query heads\n    - Massive memory savings with minimal performance loss\n     Example:\n    - 8 Query heads, 2 KV heads\n    - KV head 1 is used by Query heads 1,2,3,4",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "QuantizedModelServer",
        "kind": 6,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "class QuantizedModelServer:\n    \"\"\"\n     QUANTIZED MODEL SERVER\n    Serves different types of quantized models for inference.\n    \"\"\"\n    def __init__(self, model_path: str, model_type: str = \"lora\"):\n        self.model_path = model_path\n        self.model_type = model_type\n        self.model = None\n        self.tokenizer = None",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "health_check",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return jsonify({'status': 'healthy', 'model_type': server.model_type})\n@app.route('/info', methods=['GET'])\ndef model_info():\n    \"\"\"Get model information\"\"\"\n    return jsonify(server.get_model_info())\n@app.route('/generate', methods=['POST'])\ndef generate():\n    \"\"\"Generate text endpoint\"\"\"",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "model_info",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def model_info():\n    \"\"\"Get model information\"\"\"\n    return jsonify(server.get_model_info())\n@app.route('/generate', methods=['POST'])\ndef generate():\n    \"\"\"Generate text endpoint\"\"\"\n    try:\n        data = request.get_json()\n        prompt = data.get('prompt', '')\n        max_length = data.get('max_length', 100)",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "generate",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def generate():\n    \"\"\"Generate text endpoint\"\"\"\n    try:\n        data = request.get_json()\n        prompt = data.get('prompt', '')\n        max_length = data.get('max_length', 100)\n        temperature = data.get('temperature', 0.8)\n        top_k = data.get('top_k', 50)\n        top_p = data.get('top_p', 0.9)\n        if not prompt:",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "classify",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def classify():\n    \"\"\"Classify sentiment endpoint\"\"\"\n    try:\n        data = request.get_json()\n        text = data.get('text', '')\n        if not text:\n            return jsonify({'error': 'Text is required'}), 400\n        start_time = time.time()\n        result = server.classify_sentiment(text)\n        classification_time = time.time() - start_time",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "benchmark",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def benchmark():\n    \"\"\"Benchmark model performance\"\"\"\n    try:\n        data = request.get_json()\n        num_samples = data.get('num_samples', 10)\n        max_length = data.get('max_length', 50)\n        # Benchmark text generation\n        prompts = [\n            \"The weather today is\",\n            \"I think that\",",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def main():\n    \"\"\"Main function to start the server\"\"\"\n    global server\n    parser = argparse.ArgumentParser(description='Serve quantized models')\n    parser.add_argument('--model_path', required=True, help='Path to the quantized model')\n    parser.add_argument('--model_type', choices=['lora', 'qlora', 'quantized'], \n                       default='lora', help='Type of quantized model')\n    parser.add_argument('--host', default='0.0.0.0', help='Host to bind to')\n    parser.add_argument('--port', type=int, default=5000, help='Port to bind to')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "app = Flask(__name__)\nserver = None\n@app.route('/health', methods=['GET'])\ndef health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return jsonify({'status': 'healthy', 'model_type': server.model_type})\n@app.route('/info', methods=['GET'])\ndef model_info():\n    \"\"\"Get model information\"\"\"\n    return jsonify(server.get_model_info())",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "server = None\n@app.route('/health', methods=['GET'])\ndef health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return jsonify({'status': 'healthy', 'model_type': server.model_type})\n@app.route('/info', methods=['GET'])\ndef model_info():\n    \"\"\"Get model information\"\"\"\n    return jsonify(server.get_model_info())\n@app.route('/generate', methods=['POST'])",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"\n    global model, tokenizer, config\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file {model_path} not found!\")\n    print(f\" Loading model from {model_path}\")\n    # Load checkpoint",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "health_check",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def health_check():\n    \"\"\"\n     HEALTH CHECK ENDPOINT\n    Returns the health status of the model server.\n    \"\"\"\n    return jsonify({\n        'status': 'healthy',\n        'model_loaded': model is not None,\n        'device': str(next(model.parameters()).device) if model else None\n    })",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "generate",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def generate():\n    \"\"\"\n     TEXT GENERATION ENDPOINT\n    Generates text based on the provided prompt.\n    Expected JSON payload:\n    {\n        \"prompt\": \"Your text prompt here\",\n        \"max_length\": 100,\n        \"temperature\": 0.8,\n        \"top_k\": 50,",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "model_info",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def model_info():\n    \"\"\"\n     MODEL INFORMATION ENDPOINT\n    Returns information about the loaded model.\n    \"\"\"\n    if model is None:\n        return jsonify({'error': 'Model not loaded'}), 500\n    return jsonify({\n        'model_type': 'Qwen3-style Language Model',\n        'parameters': sum(p.numel() for p in model.parameters()),",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "home",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def home():\n    \"\"\"\n     HOME ENDPOINT\n    Returns a simple HTML interface for testing the model.\n    \"\"\"\n    return '''\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <title>Qwen3 Model Server</title>",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def main():\n    \"\"\"\n     MAIN SERVER FUNCTION\n    Starts the Flask server to serve the Qwen3 model.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser(description='Serve Qwen3 model')\n    parser.add_argument('--model', default='models/final_model1.pt', help='Path to model file')\n    parser.add_argument('--host', default='0.0.0.0', help='Host to bind to')\n    parser.add_argument('--port', type=int, default=5003, help='Port to bind to')",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "app = Flask(__name__)\n# Global variables for model and tokenizer\nmodel = None\ntokenizer = None\nconfig = None\ndef load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "model = None\ntokenizer = None\nconfig = None\ndef load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"\n    global model, tokenizer, config\n    if not os.path.exists(model_path):",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "tokenizer = None\nconfig = None\ndef load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"\n    global model, tokenizer, config\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file {model_path} not found!\")",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "config = None\ndef load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"\n    global model, tokenizer, config\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file {model_path} not found!\")\n    print(f\" Loading model from {model_path}\")",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.train_qwen3",
        "description": "qwen-llm.train_qwen3",
        "peekOfCode": "def main(resume_from: str = None):\n    \"\"\"\n     MAIN TRAINING FUNCTION\n    This function orchestrates the entire training process:\n    1. System check and configuration\n    2. Data loading and preparation\n    3. Model training (from scratch or resume)\n    4. Results reporting\n    Args:\n        resume_from: Path to checkpoint to resume training from",
        "detail": "qwen-llm.train_qwen3",
        "documentation": {}
    },
    {
        "label": "demo_inference",
        "kind": 2,
        "importPath": "qwen-llm.train_qwen3",
        "description": "qwen-llm.train_qwen3",
        "peekOfCode": "def demo_inference(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     DEMO INFERENCE FUNCTION\n    This function demonstrates the trained model's capabilities\n    \"\"\"\n    print(\" Running inference demo\")\n    # Load model\n    if not os.path.exists(model_path):\n        print(f\" Model file {model_path} not found!\")\n        print(\" Please run training first with: python train_qwen3.py\")",
        "detail": "qwen-llm.train_qwen3",
        "documentation": {}
    },
    {
        "label": "interactive_inference",
        "kind": 2,
        "importPath": "qwen-llm.train_qwen3",
        "description": "qwen-llm.train_qwen3",
        "peekOfCode": "def interactive_inference(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     INTERACTIVE INFERENCE SESSION\n    This function allows you to interact with the trained model\n    \"\"\"\n    print(\" Starting interactive inference session\")\n    print(\"Type 'quit' to exit\")\n    # Load model\n    if not os.path.exists(model_path):\n        print(f\" Model file {model_path} not found!\")",
        "detail": "qwen-llm.train_qwen3",
        "documentation": {}
    },
    {
        "label": "convert_checkpoint",
        "kind": 2,
        "importPath": "convert_checkpoint",
        "description": "convert_checkpoint",
        "peekOfCode": "def convert_checkpoint(old_path: str, new_path: str):\n    \"\"\"\n    Convert old checkpoint to new format\n    \"\"\"\n    print(f\" Converting checkpoint from {old_path} to {new_path}\")\n    try:\n        # Try to load the old checkpoint\n        print(\" Loading old checkpoint...\")\n        checkpoint = torch.load(old_path, map_location='cpu', weights_only=False)\n        # Extract only the essential data",
        "detail": "convert_checkpoint",
        "documentation": {}
    }
]