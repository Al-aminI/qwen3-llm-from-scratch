[
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "torch.utils.data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "BlockMask",
        "importPath": "torch.nn.attention.flex_attention",
        "description": "torch.nn.attention.flex_attention",
        "isExtraImport": true,
        "detail": "torch.nn.attention.flex_attention",
        "documentation": {}
    },
    {
        "label": "create_block_mask",
        "importPath": "torch.nn.attention.flex_attention",
        "description": "torch.nn.attention.flex_attention",
        "isExtraImport": true,
        "detail": "torch.nn.attention.flex_attention",
        "documentation": {}
    },
    {
        "label": "BlockMask",
        "importPath": "torch.nn.attention.flex_attention",
        "description": "torch.nn.attention.flex_attention",
        "isExtraImport": true,
        "detail": "torch.nn.attention.flex_attention",
        "documentation": {}
    },
    {
        "label": "create_block_mask",
        "importPath": "torch.nn.attention.flex_attention",
        "description": "torch.nn.attention.flex_attention",
        "isExtraImport": true,
        "detail": "torch.nn.attention.flex_attention",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "asynccontextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "BenchmarkRunner",
        "importPath": "fast_inference.utils.benchmarking",
        "description": "fast_inference.utils.benchmarking",
        "isExtraImport": true,
        "detail": "fast_inference.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "BenchmarkRunner",
        "importPath": "fast_inference.utils.benchmarking",
        "description": "fast_inference.utils.benchmarking",
        "isExtraImport": true,
        "detail": "fast_inference.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "generate_test_prompts",
        "importPath": "fast_inference.utils.benchmarking",
        "description": "fast_inference.utils.benchmarking",
        "isExtraImport": true,
        "detail": "fast_inference.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "create_universal_fast_inference",
        "importPath": "fast_inference.core.engine.universal_engine",
        "description": "fast_inference.core.engine.universal_engine",
        "isExtraImport": true,
        "detail": "fast_inference.core.engine.universal_engine",
        "documentation": {}
    },
    {
        "label": "create_universal_vllm_style_engine",
        "importPath": "fast_inference.core.engine.universal_vllm_engine",
        "description": "fast_inference.core.engine.universal_vllm_engine",
        "isExtraImport": true,
        "detail": "fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "SchedulerPolicy",
        "importPath": "fast_inference.core.engine.universal_vllm_engine",
        "description": "fast_inference.core.engine.universal_vllm_engine",
        "isExtraImport": true,
        "detail": "fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "create_vllm_style_engine",
        "importPath": "fast_inference.core.engine.vllm_style_engine",
        "description": "fast_inference.core.engine.vllm_style_engine",
        "isExtraImport": true,
        "detail": "fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "SchedulerPolicy",
        "importPath": "fast_inference.core.engine.vllm_style_engine",
        "description": "fast_inference.core.engine.vllm_style_engine",
        "isExtraImport": true,
        "detail": "fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "Mock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "Mock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "fast_inference.core.engine",
        "description": "fast_inference.core.engine",
        "isExtraImport": true,
        "detail": "fast_inference.core.engine",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "sample_tokens",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "apply_repetition_penalty",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "apply_top_k_filtering",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "apply_top_p_filtering",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "sample_greedy",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "sample_random",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "SimpleKVCache",
        "importPath": "fast_inference.core.cache",
        "description": "fast_inference.core.cache",
        "isExtraImport": true,
        "detail": "fast_inference.core.cache",
        "documentation": {}
    },
    {
        "label": "PagedKVCache",
        "importPath": "fast_inference.core.cache",
        "description": "fast_inference.core.cache",
        "isExtraImport": true,
        "detail": "fast_inference.core.cache",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "Muon",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "train_model",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "resume_training",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "LoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "QLoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "LoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "QLoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "LoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "QLoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "LoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "QLoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "LoRATrainer",
        "importPath": "lora_qlora.core.training.trainer",
        "description": "lora_qlora.core.training.trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "QLoRATrainer",
        "importPath": "lora_qlora.core.training.trainer",
        "description": "lora_qlora.core.training.trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "LoRATrainer",
        "importPath": "lora_qlora.core.training.trainer",
        "description": "lora_qlora.core.training.trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "QLoRATrainer",
        "importPath": "lora_qlora.core.training.trainer",
        "description": "lora_qlora.core.training.trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "LoRATrainer",
        "importPath": "lora_qlora.core.training.trainer",
        "description": "lora_qlora.core.training.trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "QLoRATrainer",
        "importPath": "lora_qlora.core.training.trainer",
        "description": "lora_qlora.core.training.trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "load_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "split_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "balance_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "load_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "split_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "lora_qlora.utils.config",
        "description": "lora_qlora.utils.config",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.config",
        "documentation": {}
    },
    {
        "label": "save_config",
        "importPath": "lora_qlora.utils.config",
        "description": "lora_qlora.utils.config",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.config",
        "documentation": {}
    },
    {
        "label": "UniversalLoRATrainer",
        "importPath": "lora_qlora.core.training.universal_trainer",
        "description": "lora_qlora.core.training.universal_trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "fine_tune_huggingface_model",
        "importPath": "lora_qlora.core.training.universal_trainer",
        "description": "lora_qlora.core.training.universal_trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "fine_tune_custom_model",
        "importPath": "lora_qlora.core.training.universal_trainer",
        "description": "lora_qlora.core.training.universal_trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "unittest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unittest",
        "description": "unittest",
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "LoRADataset",
        "importPath": "lora_qlora.core.training.dataset",
        "description": "lora_qlora.core.training.dataset",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.dataset",
        "documentation": {}
    },
    {
        "label": "QLoRADataset",
        "importPath": "lora_qlora.core.training.dataset",
        "description": "lora_qlora.core.training.dataset",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.dataset",
        "documentation": {}
    },
    {
        "label": "LoRALayer",
        "importPath": "lora_qlora.core.lora.lora_layer",
        "description": "lora_qlora.core.lora.lora_layer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.lora.lora_layer",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "importPath": "lora_qlora.core.lora.lora_linear",
        "description": "lora_qlora.core.lora.lora_linear",
        "isExtraImport": true,
        "detail": "lora_qlora.core.lora.lora_linear",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "importPath": "lora_qlora.core.lora.lora_manager",
        "description": "lora_qlora.core.lora.lora_manager",
        "isExtraImport": true,
        "detail": "lora_qlora.core.lora.lora_manager",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "lora_qlora.core.quantization.quantization_expert",
        "description": "lora_qlora.core.quantization.quantization_expert",
        "isExtraImport": true,
        "detail": "lora_qlora.core.quantization.quantization_expert",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "lora_qlora.core.quantization.quantization_expert",
        "description": "lora_qlora.core.quantization.quantization_expert",
        "isExtraImport": true,
        "detail": "lora_qlora.core.quantization.quantization_expert",
        "documentation": {}
    },
    {
        "label": "QLoRALayer",
        "importPath": "lora_qlora.core.qlora.qlora_layer",
        "description": "lora_qlora.core.qlora.qlora_layer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.qlora.qlora_layer",
        "documentation": {}
    },
    {
        "label": "QLoRALinear",
        "importPath": "lora_qlora.core.qlora.qlora_linear",
        "description": "lora_qlora.core.qlora.qlora_linear",
        "isExtraImport": true,
        "detail": "lora_qlora.core.qlora.qlora_linear",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "importPath": "lora_qlora.core.qlora.qlora_manager",
        "description": "lora_qlora.core.qlora.qlora_manager",
        "isExtraImport": true,
        "detail": "lora_qlora.core.qlora.qlora_manager",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationExpert",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationBenchmark",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALayer",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALayer",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALinear",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALayer",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALayer",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALinear",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "simple_fast_inference",
        "description": "simple_fast_inference",
        "isExtraImport": true,
        "detail": "simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "importPath": "simple_fast_inference",
        "description": "simple_fast_inference",
        "isExtraImport": true,
        "detail": "simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "PretrainingConfig",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "PretrainingTrainer",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "PretrainingConfig",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "PretrainingTrainer",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "psutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil",
        "description": "psutil",
        "detail": "psutil",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "FineTuneConfig",
        "importPath": "finetune_imdb",
        "description": "finetune_imdb",
        "isExtraImport": true,
        "detail": "finetune_imdb",
        "documentation": {}
    },
    {
        "label": "load_imdb_data",
        "importPath": "finetune_imdb",
        "description": "finetune_imdb",
        "isExtraImport": true,
        "detail": "finetune_imdb",
        "documentation": {}
    },
    {
        "label": "SentimentClassifier",
        "importPath": "finetune_imdb",
        "description": "finetune_imdb",
        "isExtraImport": true,
        "detail": "finetune_imdb",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "kind": 6,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "class SmallModelConfig:\n    \"\"\"\n    🎯 SMALL CONFIGURATION FOR FAST TRAINING\n    This configuration is optimized for:\n    - Fast training on CPU/limited GPU\n    - Learning the concepts without waiting hours\n    - Still demonstrating all key components\n    \"\"\"\n    # Model architecture - MUCH SMALLER\n    d_model: int = 128          # Reduced from 384 (3x smaller)",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "Muon",
        "kind": 6,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "class Muon(torch.optim.Optimizer):\n    \"\"\"\n    🚀 MUON OPTIMIZER: MomentUm Orthogonalized by Newton-schulz\n    This is a revolutionary optimizer that combines:\n    1. Momentum (like Adam) - remembers past gradients\n    2. Orthogonalization (Newton-Schulz) - makes gradients \"well-behaved\"\n    3. Adaptive learning rates - adjusts based on matrix dimensions\n    🎯 Why Muon is special:\n    - 30-50% faster convergence than Adam\n    - More stable training (fewer gradient explosions)",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "kind": 6,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "class TextTokenDataset(Dataset):\n    \"\"\"\n    📚 CUSTOM DATASET FOR LANGUAGE MODELING\n    This creates training examples for our language model:\n    🎯 What it does:\n    - Takes a long sequence of tokens\n    - Creates sliding windows of fixed length\n    - Each example: input sequence + target sequence (shifted by 1)\n    📖 Example:\n    Original text: \"The cat sat on the mat\"",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def set_seed(seed: int = 42):\n    \"\"\"Set all random seeds for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f\"🌱 Set all seeds to {seed}\")\n@dataclass",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    🔑 KEY COMPONENT: Grouped-Query Attention (GQA)\n    GQA is a memory-efficient attention mechanism where:\n    - We have fewer Key-Value heads than Query heads\n    - Each KV head is \"repeated\" to match the number of Query heads\n    - This reduces memory usage while maintaining performance\n    Example:\n    - 4 Query heads, 2 KV heads → each KV head repeated 2 times\n    - Memory savings: 50% reduction in KV cache memory",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "zeropower_via_newtonschulz5",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n    \"\"\"\n    🔬 NEWTON-SCHULZ ORTHOGONALIZATION\n    This is the mathematical heart of the Muon optimizer:\n    🎯 What it does:\n    - Takes a matrix G (gradients)\n    - Makes it \"orthogonal\" (like rotating it to be perfectly aligned)\n    - Uses Newton-Schulz iteration (a fast numerical method)\n    🧮 Why orthogonalization helps:\n    - Orthogonal matrices preserve vector lengths and angles",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def load_and_cache_data(config: SmallModelConfig, cache_dir: str = \"data_cache\"):\n    \"\"\"\n    📦 SMART DATA LOADING WITH CACHING\n    This function demonstrates modern ML data handling:\n    🎯 Key features:\n    1. Caching: Avoids reprocessing the same data\n    2. Streaming: Loads large datasets without memory issues\n    3. Tokenization: Converts text to numbers the model can understand\n    4. Efficient storage: Uses pickle for fast loading\n    🔄 The process:",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "CachedAttention",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.attention.cached_attention",
        "description": "qwen-llm.fast_inference.core.attention.cached_attention",
        "peekOfCode": "class CachedAttention(nn.Module):\n    \"\"\"\n    🎯 CACHED ATTENTION\n    Attention layer with simple KV caching for fast inference.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'SimpleKVCache'):\n        \"\"\"\n        Initialize cached attention layer.\n        Args:\n            config: Model configuration",
        "detail": "qwen-llm.fast_inference.core.attention.cached_attention",
        "documentation": {}
    },
    {
        "label": "OptimizedAttention",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.attention.optimized_attention",
        "description": "qwen-llm.fast_inference.core.attention.optimized_attention",
        "peekOfCode": "class OptimizedAttention(nn.Module):\n    \"\"\"\n    🎯 OPTIMIZED ATTENTION WITH PAGED KV CACHE\n    Advanced attention layer with paged KV caching for maximum performance.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'PagedKVCache'):\n        \"\"\"\n        Initialize optimized attention layer.\n        Args:\n            config: Model configuration",
        "detail": "qwen-llm.fast_inference.core.attention.optimized_attention",
        "documentation": {}
    },
    {
        "label": "PagedKVCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.cache.paged_cache",
        "description": "qwen-llm.fast_inference.core.cache.paged_cache",
        "peekOfCode": "class PagedKVCache(nn.Module):\n    \"\"\"\n    🧠 PAGED KV CACHE\n    Efficient memory management for KV cache using page-based allocation.\n    Inspired by vLLM's PagedAttention but simplified for our use case.\n    \"\"\"\n    def __init__(self, n_pages: int, page_size: int, n_heads: int, head_dim: int, dtype: torch.dtype, device: str):\n        \"\"\"\n        Initialize paged KV cache.\n        Args:",
        "detail": "qwen-llm.fast_inference.core.cache.paged_cache",
        "documentation": {}
    },
    {
        "label": "SimpleKVCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.cache.simple_cache",
        "description": "qwen-llm.fast_inference.core.cache.simple_cache",
        "peekOfCode": "class SimpleKVCache:\n    \"\"\"\n    🎯 SIMPLE KV CACHE\n    A straightforward KV cache implementation that stores key-value pairs\n    for each sequence position. Much simpler than paged attention but still effective.\n    \"\"\"\n    def __init__(self, max_seq_len: int, n_heads: int, head_dim: int, dtype: torch.dtype, device: str):\n        \"\"\"\n        Initialize simple KV cache.\n        Args:",
        "detail": "qwen-llm.fast_inference.core.cache.simple_cache",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "description": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "peekOfCode": "class Sequence:\n    \"\"\"Represents a single generation sequence.\"\"\"\n    seq_id: int\n    prompt: str\n    input_ids: torch.Tensor\n    output_ids: List[int]\n    sampling_params: 'SamplingParams'\n    batch_idx: int = -1\n    finished: bool = False\n    prefill_done: bool = False",
        "detail": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "documentation": {}
    },
    {
        "label": "FastInferenceEngine",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "description": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "peekOfCode": "class FastInferenceEngine:\n    \"\"\"\n    🚀 FAST INFERENCE ENGINE\n    High-performance inference engine with:\n    - Paged KV cache for memory efficiency\n    - Continuous batching for throughput\n    - CUDA graphs for optimization\n    - Dynamic scheduling\n    \"\"\"\n    def __init__(self, model: nn.Module, tokenizer, config, ",
        "detail": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "documentation": {}
    },
    {
        "label": "OptimizedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "description": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "peekOfCode": "class OptimizedTransformerBlock(nn.Module):\n    \"\"\"\n    🏗️ OPTIMIZED TRANSFORMER BLOCK\n    Transformer block with optimized attention and KV caching.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'PagedKVCache'):\n        super().__init__()\n        self.attention = OptimizedAttention(config, kv_cache)\n        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n        # Pre-norm architecture",
        "detail": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "documentation": {}
    },
    {
        "label": "create_fast_inference_engine",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "description": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "peekOfCode": "def create_fast_inference_engine(model_path: str, tokenizer_path: str, \n                                max_batch_size: int = 32, max_seq_len: int = 2048,\n                                n_pages: int = 1000, page_size: int = 128) -> FastInferenceEngine:\n    \"\"\"\n    Create a fast inference engine from saved model.\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        max_batch_size: Maximum batch size\n        max_seq_len: Maximum sequence length",
        "detail": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.simple_engine",
        "description": "qwen-llm.fast_inference.core.engine.simple_engine",
        "peekOfCode": "class SimpleFastInference:\n    \"\"\"\n    🚀 SIMPLE FAST INFERENCE ENGINE\n    A simplified but fast inference engine that adds KV caching to your existing model.\n    Much easier to understand and integrate than full vLLM-style implementations.\n    \"\"\"\n    def __init__(self, model: nn.Module, tokenizer, config, max_seq_len: int = 2048):\n        \"\"\"\n        Initialize simple fast inference engine.\n        Args:",
        "detail": "qwen-llm.fast_inference.core.engine.simple_engine",
        "documentation": {}
    },
    {
        "label": "CachedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.simple_engine",
        "description": "qwen-llm.fast_inference.core.engine.simple_engine",
        "peekOfCode": "class CachedTransformerBlock(nn.Module):\n    \"\"\"\n    🏗️ CACHED TRANSFORMER BLOCK\n    Transformer block with cached attention.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'SimpleKVCache'):\n        super().__init__()\n        self.attention = CachedAttention(config, kv_cache)\n        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n        # Pre-norm architecture",
        "detail": "qwen-llm.fast_inference.core.engine.simple_engine",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.core.engine.simple_engine",
        "description": "qwen-llm.fast_inference.core.engine.simple_engine",
        "peekOfCode": "def create_simple_fast_inference(model_path: str, tokenizer_path: str, \n                                max_seq_len: int = 2048) -> SimpleFastInference:\n    \"\"\"\n    Create a simple fast inference engine from saved model.\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        max_seq_len: Maximum sequence length\n    Returns:\n        SimpleFastInference instance",
        "detail": "qwen-llm.fast_inference.core.engine.simple_engine",
        "documentation": {}
    },
    {
        "label": "UniversalFastInference",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_engine",
        "peekOfCode": "class UniversalFastInference:\n    \"\"\"\n    🌐 UNIVERSAL FAST INFERENCE ENGINE\n    A universal inference engine that can work with ANY model:\n    - HuggingFace models (BERT, RoBERTa, GPT, LLaMA, etc.)\n    - Custom models (like your MinimalLLM)\n    - Any PyTorch model with transformer architecture\n    Key Features:\n    - Automatic model detection\n    - Universal KV caching",
        "detail": "qwen-llm.fast_inference.core.engine.universal_engine",
        "documentation": {}
    },
    {
        "label": "CachedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_engine",
        "peekOfCode": "class CachedTransformerBlock(nn.Module):\n    \"\"\"\n    🏗️ CACHED TRANSFORMER BLOCK\n    Universal transformer block with cached attention.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'SimpleKVCache'):\n        super().__init__()\n        from ..attention.cached_attention import CachedAttention\n        from pretraining.core.model.components import SwiGLUFeedForward, RMSNorm\n        self.attention = CachedAttention(config, kv_cache)",
        "detail": "qwen-llm.fast_inference.core.engine.universal_engine",
        "documentation": {}
    },
    {
        "label": "create_universal_fast_inference",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_engine",
        "peekOfCode": "def create_universal_fast_inference(model: Union[nn.Module, str], tokenizer: Union[Any, str], \n                                   max_seq_len: int = 2048, model_type: str = \"auto\") -> UniversalFastInference:\n    \"\"\"\n    Create a universal fast inference engine.\n    Args:\n        model: Model instance, model path, or HuggingFace model name\n        tokenizer: Tokenizer instance, tokenizer path, or HuggingFace tokenizer name\n        max_seq_len: Maximum sequence length\n        model_type: Model type (\"auto\", \"huggingface\", \"custom\", \"minimal_llm\")\n    Returns:",
        "detail": "qwen-llm.fast_inference.core.engine.universal_engine",
        "documentation": {}
    },
    {
        "label": "SchedulerPolicy",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "class SchedulerPolicy(Enum):\n    \"\"\"Scheduling policies for request handling.\"\"\"\n    FCFS = \"first_come_first_served\"  # First Come First Served\n    PRIORITY = \"priority\"             # Priority-based\n    MEMORY_AWARE = \"memory_aware\"     # Memory-aware scheduling\n@dataclass\nclass Block:\n    \"\"\"Represents a memory block in the cache.\"\"\"\n    block_id: int\n    ref_count: int = 0",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "class Block:\n    \"\"\"Represents a memory block in the cache.\"\"\"\n    block_id: int\n    ref_count: int = 0\n    is_allocated: bool = False\n    sequence_ids: List[int] = field(default_factory=list)\n@dataclass\nclass SequenceMetadata:\n    \"\"\"Metadata for a generation sequence.\"\"\"\n    seq_id: int",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "SequenceMetadata",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "class SequenceMetadata:\n    \"\"\"Metadata for a generation sequence.\"\"\"\n    seq_id: int\n    prompt: str\n    input_ids: torch.Tensor\n    output_ids: List[int] = field(default_factory=list)\n    sampling_params: Dict[str, Any] = field(default_factory=dict)\n    blocks: List[int] = field(default_factory=list)  # Allocated block IDs\n    finished: bool = False\n    prefill_done: bool = False",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "UniversalPagedAttentionCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "class UniversalPagedAttentionCache:\n    \"\"\"\n    🌐 UNIVERSAL PAGED ATTENTION CACHE\n    Universal PagedAttention implementation that works with any model:\n    - Automatic model detection\n    - Universal block-wise memory management\n    - Memory fragmentation handling\n    - Efficient block allocation/deallocation\n    \"\"\"\n    def __init__(self, num_blocks: int, block_size: int, model_info: Dict[str, Any], ",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "UniversalVLLMStyleEngine",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "class UniversalVLLMStyleEngine:\n    \"\"\"\n    🌐 UNIVERSAL VLLM-STYLE INFERENCE ENGINE\n    Combines universal model support with vLLM features:\n    - Works with ANY model (MinimalLLM, HuggingFace, custom)\n    - True PagedAttention with block-wise memory management\n    - Advanced scheduling policies\n    - Async API support\n    - Production-ready features\n    \"\"\"",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "create_universal_vllm_style_engine",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "def create_universal_vllm_style_engine(model: Union[nn.Module, str], tokenizer: Union[Any, str], \n                                      **kwargs) -> UniversalVLLMStyleEngine:\n    \"\"\"\n    Create a universal vLLM-style inference engine.\n    Args:\n        model: Model instance, model path, or HuggingFace model name\n        tokenizer: Tokenizer instance, tokenizer path, or HuggingFace tokenizer name\n        **kwargs: Additional arguments\n    Returns:\n        UniversalVLLMStyleEngine instance",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SchedulerPolicy(Enum):\n    \"\"\"Scheduling policies for request handling.\"\"\"\n    FCFS = \"first_come_first_served\"  # First Come First Served\n    PRIORITY = \"priority\"             # Priority-based\n    MEMORY_AWARE = \"memory_aware\"     # Memory-aware scheduling\n@dataclass\nclass Block:\n    \"\"\"Represents a memory block in the cache.\"\"\"\n    block_id: int",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "SchedulerPolicy",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "class SchedulerPolicy(Enum):\n    \"\"\"Scheduling policies for request handling.\"\"\"\n    FCFS = \"first_come_first_served\"  # First Come First Served\n    PRIORITY = \"priority\"             # Priority-based\n    MEMORY_AWARE = \"memory_aware\"     # Memory-aware scheduling\n@dataclass\nclass Block:\n    \"\"\"Represents a memory block in the cache.\"\"\"\n    block_id: int\n    ref_count: int = 0",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "class Block:\n    \"\"\"Represents a memory block in the cache.\"\"\"\n    block_id: int\n    ref_count: int = 0\n    is_allocated: bool = False\n    sequence_ids: List[int] = field(default_factory=list)\n@dataclass\nclass SequenceMetadata:\n    \"\"\"Metadata for a generation sequence.\"\"\"\n    seq_id: int",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "SequenceMetadata",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "class SequenceMetadata:\n    \"\"\"Metadata for a generation sequence.\"\"\"\n    seq_id: int\n    prompt: str\n    input_ids: torch.Tensor\n    output_ids: List[int] = field(default_factory=list)\n    sampling_params: Dict[str, Any] = field(default_factory=dict)\n    blocks: List[int] = field(default_factory=list)  # Allocated block IDs\n    finished: bool = False\n    prefill_done: bool = False",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "PagedAttentionCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "class PagedAttentionCache:\n    \"\"\"\n    🧠 PAGED ATTENTION CACHE\n    True vLLM-style PagedAttention implementation with:\n    - Block-wise memory management\n    - Memory fragmentation handling\n    - Efficient block allocation/deallocation\n    \"\"\"\n    def __init__(self, num_blocks: int, block_size: int, num_heads: int, head_dim: int, \n                 dtype: torch.dtype, device: str):",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "VLLMStyleEngine",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "class VLLMStyleEngine:\n    \"\"\"\n    🚀 VLLM-STYLE INFERENCE ENGINE\n    Production-ready inference engine with:\n    - True PagedAttention\n    - Advanced scheduling policies\n    - Async API support\n    - Memory management\n    - Performance monitoring\n    \"\"\"",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "create_vllm_style_engine",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "def create_vllm_style_engine(model: nn.Module, tokenizer, config, **kwargs) -> VLLMStyleEngine:\n    \"\"\"\n    Create a vLLM-style inference engine.\n    Args:\n        model: Pre-trained model\n        tokenizer: Tokenizer for the model\n        config: Model configuration\n        **kwargs: Additional arguments\n    Returns:\n        VLLMStyleEngine instance",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SchedulerPolicy(Enum):\n    \"\"\"Scheduling policies for request handling.\"\"\"\n    FCFS = \"first_come_first_served\"  # First Come First Served\n    PRIORITY = \"priority\"             # Priority-based\n    MEMORY_AWARE = \"memory_aware\"     # Memory-aware scheduling\n@dataclass\nclass Block:\n    \"\"\"Represents a memory block in the cache.\"\"\"\n    block_id: int",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "InferenceRequest",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.examples.advanced.production_server",
        "description": "qwen-llm.fast_inference.examples.advanced.production_server",
        "peekOfCode": "class InferenceRequest:\n    \"\"\"Request structure for inference.\"\"\"\n    prompt: str\n    max_new_tokens: int = 100\n    temperature: float = 0.8\n    top_k: int = 50\n    top_p: float = 0.9\n    request_id: Optional[str] = None\n@dataclass\nclass InferenceResponse:",
        "detail": "qwen-llm.fast_inference.examples.advanced.production_server",
        "documentation": {}
    },
    {
        "label": "InferenceResponse",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.examples.advanced.production_server",
        "description": "qwen-llm.fast_inference.examples.advanced.production_server",
        "peekOfCode": "class InferenceResponse:\n    \"\"\"Response structure for inference.\"\"\"\n    request_id: str\n    generated_text: str\n    generation_time: float\n    tokens_generated: int\n    success: bool\n    error_message: Optional[str] = None\nclass InferenceServer:\n    \"\"\"",
        "detail": "qwen-llm.fast_inference.examples.advanced.production_server",
        "documentation": {}
    },
    {
        "label": "InferenceServer",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.examples.advanced.production_server",
        "description": "qwen-llm.fast_inference.examples.advanced.production_server",
        "peekOfCode": "class InferenceServer:\n    \"\"\"\n    Production-ready inference server.\n    This class provides a robust inference server with:\n    - Request queuing and batching\n    - Error handling and recovery\n    - Performance monitoring\n    - Health checks\n    - Graceful shutdown\n    \"\"\"",
        "detail": "qwen-llm.fast_inference.examples.advanced.production_server",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "qwen-llm.fast_inference.examples.advanced.production_server",
        "description": "qwen-llm.fast_inference.examples.advanced.production_server",
        "peekOfCode": "logger = logging.getLogger(__name__)\n@dataclass\nclass InferenceRequest:\n    \"\"\"Request structure for inference.\"\"\"\n    prompt: str\n    max_new_tokens: int = 100\n    temperature: float = 0.8\n    top_k: int = 50\n    top_p: float = 0.9\n    request_id: Optional[str] = None",
        "detail": "qwen-llm.fast_inference.examples.advanced.production_server",
        "documentation": {}
    },
    {
        "label": "naive_generate_text",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "description": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "peekOfCode": "def naive_generate_text(model, tokenizer, prompt: str, max_length: int = 100,\n                       temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9) -> str:\n    \"\"\"\n    Naive text generation without KV caching (for comparison).\n    This is the original text generation method that processes the entire sequence\n    for each new token. Very slow but simple.\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    # Tokenize prompt",
        "detail": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "description": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "peekOfCode": "def main():\n    \"\"\"Main function demonstrating performance comparison.\"\"\"\n    print(\"📊 Performance Comparison Example\")\n    print(\"=\" * 50)\n    # Test prompts\n    test_prompts = [\n        \"Hello, how are you today?\",\n        \"Tell me a joke about programming\",\n        \"Write a short story about\",\n        \"Explain the concept of machine learning\",",
        "detail": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.basic.quick_start",
        "description": "qwen-llm.fast_inference.examples.basic.quick_start",
        "peekOfCode": "def main():\n    \"\"\"Main function demonstrating basic usage.\"\"\"\n    print(\"🚀 Fast Inference Quick Start Example\")\n    print(\"=\" * 50)\n    # Example 1: Single text generation\n    print(\"\\n📝 Example 1: Single Text Generation\")\n    print(\"-\" * 40)\n    try:\n        # Create engine (you'll need to provide actual model paths)\n        engine = create_simple_fast_inference(",
        "detail": "qwen-llm.fast_inference.examples.basic.quick_start",
        "documentation": {}
    },
    {
        "label": "example_minimal_llm",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.universal_example",
        "description": "qwen-llm.fast_inference.examples.universal_example",
        "peekOfCode": "def example_minimal_llm():\n    \"\"\"\n    🎯 EXAMPLE: Your Custom MinimalLLM Model\n    \"\"\"\n    print(\"🎯 Example 1: Your Custom MinimalLLM Model\")\n    print(\"=\" * 50)\n    # Load your trained model\n    model_path = \"models/final_model1.pt\"\n    tokenizer_path = \"HuggingFaceTB/SmolLM-135M\"\n    if not os.path.exists(model_path):",
        "detail": "qwen-llm.fast_inference.examples.universal_example",
        "documentation": {}
    },
    {
        "label": "example_huggingface_models",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.universal_example",
        "description": "qwen-llm.fast_inference.examples.universal_example",
        "peekOfCode": "def example_huggingface_models():\n    \"\"\"\n    🎯 EXAMPLE: HuggingFace Models\n    \"\"\"\n    print(\"\\n🎯 Example 2: HuggingFace Models\")\n    print(\"=\" * 50)\n    # List of HuggingFace models to try\n    models_to_try = [\n        \"microsoft/DialoGPT-small\",\n        \"gpt2\",",
        "detail": "qwen-llm.fast_inference.examples.universal_example",
        "documentation": {}
    },
    {
        "label": "example_batch_generation",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.universal_example",
        "description": "qwen-llm.fast_inference.examples.universal_example",
        "peekOfCode": "def example_batch_generation():\n    \"\"\"\n    🎯 EXAMPLE: Batch Generation\n    \"\"\"\n    print(\"\\n🎯 Example 3: Batch Generation\")\n    print(\"=\" * 50)\n    # Use a small model for batch generation\n    model_name = \"distilgpt2\"\n    try:\n        # Create universal engine",
        "detail": "qwen-llm.fast_inference.examples.universal_example",
        "documentation": {}
    },
    {
        "label": "example_performance_comparison",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.universal_example",
        "description": "qwen-llm.fast_inference.examples.universal_example",
        "peekOfCode": "def example_performance_comparison():\n    \"\"\"\n    🎯 EXAMPLE: Performance Comparison\n    \"\"\"\n    print(\"\\n🎯 Example 4: Performance Comparison\")\n    print(\"=\" * 50)\n    import time\n    # Test with a small model\n    model_name = \"distilgpt2\"\n    try:",
        "detail": "qwen-llm.fast_inference.examples.universal_example",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.universal_example",
        "description": "qwen-llm.fast_inference.examples.universal_example",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Run all examples to demonstrate universal fast inference.\n    \"\"\"\n    print(\"🌐 UNIVERSAL FAST INFERENCE EXAMPLES\")\n    print(\"=\" * 60)\n    print(\"This example demonstrates how to use the universal fast inference\")\n    print(\"engine with different types of models.\")\n    print()",
        "detail": "qwen-llm.fast_inference.examples.universal_example",
        "documentation": {}
    },
    {
        "label": "TestSimpleFastInference",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.integration.test_engine",
        "description": "qwen-llm.fast_inference.tests.integration.test_engine",
        "peekOfCode": "class TestSimpleFastInference:\n    \"\"\"Integration tests for SimpleFastInference.\"\"\"\n    @pytest.fixture\n    def mock_model(self):\n        \"\"\"Create a mock model for testing.\"\"\"\n        model = Mock()\n        model.parameters.return_value = [torch.tensor([1.0])]\n        model.dtype = torch.float16\n        model.transformer_blocks = [Mock() for _ in range(2)]\n        # Mock transformer block components",
        "detail": "qwen-llm.fast_inference.tests.integration.test_engine",
        "documentation": {}
    },
    {
        "label": "TestSimpleKVCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.unit.test_cache",
        "description": "qwen-llm.fast_inference.tests.unit.test_cache",
        "peekOfCode": "class TestSimpleKVCache:\n    \"\"\"Test cases for SimpleKVCache.\"\"\"\n    def test_initialization(self):\n        \"\"\"Test cache initialization.\"\"\"\n        cache = SimpleKVCache(\n            max_seq_len=100,\n            n_heads=8,\n            head_dim=64,\n            dtype=torch.float16,\n            device=\"cpu\"",
        "detail": "qwen-llm.fast_inference.tests.unit.test_cache",
        "documentation": {}
    },
    {
        "label": "TestPagedKVCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.unit.test_cache",
        "description": "qwen-llm.fast_inference.tests.unit.test_cache",
        "peekOfCode": "class TestPagedKVCache:\n    \"\"\"Test cases for PagedKVCache.\"\"\"\n    def test_initialization(self):\n        \"\"\"Test cache initialization.\"\"\"\n        cache = PagedKVCache(\n            n_pages=10,\n            page_size=128,\n            n_heads=8,\n            head_dim=64,\n            dtype=torch.float16,",
        "detail": "qwen-llm.fast_inference.tests.unit.test_cache",
        "documentation": {}
    },
    {
        "label": "TestSamplingParams",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "description": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "peekOfCode": "class TestSamplingParams:\n    \"\"\"Test cases for SamplingParams.\"\"\"\n    def test_default_initialization(self):\n        \"\"\"Test default parameter initialization.\"\"\"\n        params = SamplingParams()\n        assert params.max_new_tokens == 100\n        assert params.temperature == 0.8\n        assert params.top_k == 50\n        assert params.top_p == 0.9\n        assert params.repetition_penalty == 1.0",
        "detail": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "documentation": {}
    },
    {
        "label": "TestSamplingFunctions",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "description": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "peekOfCode": "class TestSamplingFunctions:\n    \"\"\"Test cases for sampling functions.\"\"\"\n    def test_sample_greedy(self):\n        \"\"\"Test greedy sampling.\"\"\"\n        logits = torch.tensor([[1.0, 2.0, 0.5, 3.0]])\n        tokens = sample_greedy(logits)\n        assert tokens.item() == 3  # Highest logit at index 3\n    def test_sample_random(self):\n        \"\"\"Test random sampling.\"\"\"\n        logits = torch.tensor([[1.0, 1.0, 1.0, 1.0]])",
        "detail": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def device():\n    \"\"\"Get the device for testing.\"\"\"\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n@pytest.fixture\ndef temp_dir():\n    \"\"\"Create a temporary directory for testing.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        yield tmpdir\n@pytest.fixture\ndef mock_model_config():",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "temp_dir",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def temp_dir():\n    \"\"\"Create a temporary directory for testing.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        yield tmpdir\n@pytest.fixture\ndef mock_model_config():\n    \"\"\"Create a mock model configuration.\"\"\"\n    config = Mock()\n    config.d_model = 512\n    config.n_heads = 8",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "mock_model_config",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def mock_model_config():\n    \"\"\"Create a mock model configuration.\"\"\"\n    config = Mock()\n    config.d_model = 512\n    config.n_heads = 8\n    config.n_kv_heads = 8\n    config.n_kv_groups = 1\n    config.d_k = 64\n    config.d_ff = 2048\n    config.rms_norm_eps = 1e-6",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "mock_tokenizer",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def mock_tokenizer():\n    \"\"\"Create a mock tokenizer.\"\"\"\n    tokenizer = Mock()\n    tokenizer.encode = Mock(return_value=torch.tensor([1, 2, 3, 4, 5]))\n    tokenizer.decode = Mock(return_value=\"Generated text\")\n    tokenizer.eos_token_id = 2\n    tokenizer.pad_token = None\n    return tokenizer\n@pytest.fixture\ndef sample_logits():",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "sample_logits",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def sample_logits():\n    \"\"\"Create sample logits for testing.\"\"\"\n    return torch.tensor([[1.0, 2.0, 0.5, 3.0, 1.5]])\n@pytest.fixture\ndef sample_kv_tensors():\n    \"\"\"Create sample K and V tensors for testing.\"\"\"\n    k = torch.randn(8, 10, 64, dtype=torch.float16)\n    v = torch.randn(8, 10, 64, dtype=torch.float16)\n    return k, v\n@pytest.fixture",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "sample_kv_tensors",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def sample_kv_tensors():\n    \"\"\"Create sample K and V tensors for testing.\"\"\"\n    k = torch.randn(8, 10, 64, dtype=torch.float16)\n    v = torch.randn(8, 10, 64, dtype=torch.float16)\n    return k, v\n@pytest.fixture\ndef sample_prompts():\n    \"\"\"Create sample prompts for testing.\"\"\"\n    return [\n        \"Hello, how are you?\",",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "sample_prompts",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def sample_prompts():\n    \"\"\"Create sample prompts for testing.\"\"\"\n    return [\n        \"Hello, how are you?\",\n        \"Tell me a joke about programming\",\n        \"Write a short story about\",\n        \"Explain the concept of machine learning\",\n        \"What is the meaning of life?\"\n    ]\n@pytest.fixture",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "sample_sampling_params",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def sample_sampling_params():\n    \"\"\"Create sample sampling parameters.\"\"\"\n    from fast_inference.utils.sampling import SamplingParams\n    return SamplingParams(\n        max_new_tokens=50,\n        temperature=0.8,\n        top_k=50,\n        top_p=0.9,\n        repetition_penalty=1.0\n    )",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "pytest_configure",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def pytest_configure(config):\n    \"\"\"Configure pytest markers.\"\"\"\n    config.addinivalue_line(\"markers\", \"slow: marks tests as slow\")\n    config.addinivalue_line(\"markers\", \"integration: marks tests as integration tests\")\n    config.addinivalue_line(\"markers\", \"unit: marks tests as unit tests\")\n    config.addinivalue_line(\"markers\", \"gpu: marks tests that require GPU\")\n    config.addinivalue_line(\"markers\", \"cpu: marks tests that run on CPU only\")\ndef pytest_collection_modifyitems(config, items):\n    \"\"\"Modify test collection to add markers based on test location.\"\"\"\n    for item in items:",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "pytest_collection_modifyitems",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def pytest_collection_modifyitems(config, items):\n    \"\"\"Modify test collection to add markers based on test location.\"\"\"\n    for item in items:\n        # Add markers based on test file location\n        if \"unit\" in str(item.fspath):\n            item.add_marker(pytest.mark.unit)\n        elif \"integration\" in str(item.fspath):\n            item.add_marker(pytest.mark.integration)\n        # Add slow marker to tests that might take time\n        if \"slow\" in item.name or \"benchmark\" in item.name:",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "BenchmarkResult",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "class BenchmarkResult:\n    \"\"\"\n    Results from a benchmark run.\n    Attributes:\n        method_name: Name of the inference method\n        total_time: Total time in seconds\n        total_tokens: Total number of tokens generated\n        total_requests: Total number of requests processed\n        throughput_tokens_per_sec: Tokens per second\n        throughput_requests_per_sec: Requests per second",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "BenchmarkRunner",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "class BenchmarkRunner:\n    \"\"\"\n    Benchmark runner for inference methods.\n    This class provides utilities for running benchmarks and collecting\n    performance metrics across different inference methods.\n    \"\"\"\n    def __init__(self, device: str = \"auto\"):\n        \"\"\"\n        Initialize benchmark runner.\n        Args:",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "benchmark_inference",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "def benchmark_inference(engine, test_prompts: List[str], max_new_tokens: int = 50,\n                       method_name: str = \"Fast Inference\") -> BenchmarkResult:\n    \"\"\"\n    Quick benchmark function for inference engines.\n    Args:\n        engine: Inference engine with generate_batch method\n        test_prompts: List of test prompts\n        max_new_tokens: Maximum tokens to generate\n        method_name: Name for the benchmark\n    Returns:",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "compare_methods",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "def compare_methods(methods: Dict[str, Callable], test_prompts: List[str], \n                   max_new_tokens: int = 50) -> Dict[str, Any]:\n    \"\"\"\n    Compare multiple inference methods.\n    Args:\n        methods: Dictionary mapping method names to inference functions\n        test_prompts: List of test prompts\n        max_new_tokens: Maximum tokens to generate\n    Returns:\n        Comparison results",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "generate_test_prompts",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "def generate_test_prompts(num_prompts: int = 10, \n                         min_length: int = 20, \n                         max_length: int = 100) -> List[str]:\n    \"\"\"\n    Generate test prompts for benchmarking.\n    Args:\n        num_prompts: Number of prompts to generate\n        min_length: Minimum prompt length\n        max_length: Maximum prompt length\n    Returns:",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "create_performance_report",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "def create_performance_report(results: List[BenchmarkResult], \n                            output_file: Optional[str] = None) -> str:\n    \"\"\"\n    Create a detailed performance report.\n    Args:\n        results: List of benchmark results\n        output_file: Optional file to save report\n    Returns:\n        Report as string\n    \"\"\"",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "class SamplingParams:\n    \"\"\"\n    Sampling parameters for text generation.\n    Attributes:\n        max_new_tokens: Maximum number of new tokens to generate\n        temperature: Sampling temperature (0.0 = greedy, >1.0 = more random)\n        top_k: Number of top tokens to consider (0 = no limit)\n        top_p: Cumulative probability threshold for nucleus sampling (1.0 = no limit)\n        repetition_penalty: Penalty for repeated tokens (1.0 = no penalty)\n        stop_token_ids: List of token IDs to stop generation at",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "sample_tokens",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def sample_tokens(logits: torch.Tensor, sampling_params: SamplingParams, \n                 previous_tokens: Optional[torch.Tensor] = None) -> torch.Tensor:\n    \"\"\"\n    Sample tokens from logits using the specified sampling parameters.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        sampling_params: Sampling parameters\n        previous_tokens: Previously generated tokens for repetition penalty\n    Returns:\n        Sampled token IDs (batch_size,)",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "apply_repetition_penalty",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def apply_repetition_penalty(logits: torch.Tensor, previous_tokens: torch.Tensor, \n                           penalty: float) -> torch.Tensor:\n    \"\"\"\n    Apply repetition penalty to logits.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        previous_tokens: Previously generated tokens\n        penalty: Repetition penalty factor\n    Returns:\n        Logits with repetition penalty applied",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "apply_top_k_filtering",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def apply_top_k_filtering(logits: torch.Tensor, top_k: int) -> torch.Tensor:\n    \"\"\"\n    Apply top-k filtering to logits.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        top_k: Number of top tokens to keep\n    Returns:\n        Logits with top-k filtering applied\n    \"\"\"\n    if top_k <= 0:",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "apply_top_p_filtering",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def apply_top_p_filtering(logits: torch.Tensor, top_p: float) -> torch.Tensor:\n    \"\"\"\n    Apply top-p (nucleus) filtering to logits.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        top_p: Cumulative probability threshold\n    Returns:\n        Logits with top-p filtering applied\n    \"\"\"\n    if top_p >= 1.0:",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "sample_greedy",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def sample_greedy(logits: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Greedy sampling (always pick the most likely token).\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n    Returns:\n        Greedily sampled token IDs (batch_size,)\n    \"\"\"\n    return logits.argmax(dim=-1)\ndef sample_random(logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "sample_random",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def sample_random(logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n    \"\"\"\n    Random sampling with temperature.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        temperature: Sampling temperature\n    Returns:\n        Randomly sampled token IDs (batch_size,)\n    \"\"\"\n    if temperature > 0:",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "sample_beam_search",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def sample_beam_search(logits: torch.Tensor, beam_size: int = 4, \n                      length_penalty: float = 1.0) -> List[torch.Tensor]:\n    \"\"\"\n    Beam search sampling (simplified version).\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        beam_size: Number of beams to maintain\n        length_penalty: Length penalty factor\n    Returns:\n        List of beam sequences",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def create_engine(args):\n    \"\"\"Create inference engine based on arguments.\"\"\"\n    if args.advanced:\n        return create_fast_inference_engine(\n            model_path=args.model_path,\n            tokenizer_path=args.tokenizer_path,\n            max_batch_size=args.batch_size,\n            max_seq_len=args.max_seq_len,\n            n_pages=args.n_pages,\n            page_size=args.page_size",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "cmd_generate",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def cmd_generate(args):\n    \"\"\"Generate text from prompts.\"\"\"\n    print(\"🚀 Fast Inference - Text Generation\")\n    print(\"=\" * 40)\n    try:\n        # Create engine\n        print(f\"Loading model from {args.model_path}...\")\n        engine = create_engine(args)\n        print(\"✅ Model loaded successfully!\")\n        # Create sampling parameters",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "cmd_benchmark",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def cmd_benchmark(args):\n    \"\"\"Run performance benchmark.\"\"\"\n    print(\"📊 Fast Inference - Performance Benchmark\")\n    print(\"=\" * 45)\n    try:\n        # Create engine\n        print(f\"Loading model from {args.model_path}...\")\n        engine = create_engine(args)\n        print(\"✅ Model loaded successfully!\")\n        # Generate test prompts",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "cmd_compare",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def cmd_compare(args):\n    \"\"\"Compare different inference methods.\"\"\"\n    print(\"⚖️ Fast Inference - Method Comparison\")\n    print(\"=\" * 40)\n    try:\n        # Create engines\n        print(f\"Loading models from {args.model_path}...\")\n        # Simple engine\n        simple_engine = create_simple_fast_inference(\n            model_path=args.model_path,",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def main():\n    \"\"\"Main CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Fast Inference Engine CLI\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  # Generate text from a prompt\n  fast-inference generate --model-path model.pt --tokenizer-path tokenizer --prompts \"Hello, world!\"\n  # Generate from file",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "LoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.lora.lora_layer",
        "description": "qwen-llm.lora_qlora.core.lora.lora_layer",
        "peekOfCode": "class LoRALayer(nn.Module):\n    \"\"\"\n    🎯 LORA LAYER IMPLEMENTATION\n    LoRA (Low-Rank Adaptation) decomposes weight updates into low-rank matrices.\n    Mathematical Foundation:\n    W = W₀ + ΔW = W₀ + BA\n    Where:\n    - W₀: Original frozen weights\n    - B: Low-rank matrix (d × r)\n    - A: Low-rank matrix (r × k)",
        "detail": "qwen-llm.lora_qlora.core.lora.lora_layer",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.lora.lora_linear",
        "description": "qwen-llm.lora_qlora.core.lora.lora_linear",
        "peekOfCode": "class LoRALinear(nn.Module):\n    \"\"\"\n    🎯 LORA LINEAR LAYER\n    Combines original linear layer with LoRA adaptation.\n    This layer maintains the original linear layer's functionality while adding\n    LoRA adaptation for efficient fine-tuning.\n    \"\"\"\n    def __init__(self, original_layer: nn.Linear, rank: int = 16, alpha: float = 32.0, dropout: float = 0.1):\n        \"\"\"\n        Initialize LoRA linear layer.",
        "detail": "qwen-llm.lora_qlora.core.lora.lora_linear",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.lora.lora_manager",
        "description": "qwen-llm.lora_qlora.core.lora.lora_manager",
        "peekOfCode": "class LoRAManager:\n    \"\"\"\n    🎯 LORA MANAGER\n    Manages LoRA adaptation for entire models.\n    This class provides functionality to:\n    - Apply LoRA to specified modules in a model\n    - Track and manage LoRA layers\n    - Analyze parameter counts and memory usage\n    - Save and load LoRA weights\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.core.lora.lora_manager",
        "documentation": {}
    },
    {
        "label": "QLoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.qlora.qlora_layer",
        "description": "qwen-llm.lora_qlora.core.qlora.qlora_layer",
        "peekOfCode": "class QLoRALayer(nn.Module):\n    \"\"\"\n    🎯 QLORA LAYER\n    QLoRA layer combining 4-bit quantization with LoRA adaptation.\n    This layer provides:\n    - 4-bit quantized weights for memory efficiency\n    - LoRA adaptation for fine-tuning\n    - Efficient forward pass with quantized operations\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, rank: int = 16, alpha: float = 1.0, dropout: float = 0.0):",
        "detail": "qwen-llm.lora_qlora.core.qlora.qlora_layer",
        "documentation": {}
    },
    {
        "label": "QLoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.qlora.qlora_linear",
        "description": "qwen-llm.lora_qlora.core.qlora.qlora_linear",
        "peekOfCode": "class QLoRALinear(nn.Module):\n    \"\"\"\n    🎯 QLORA LINEAR LAYER\n    QLoRA linear layer combining 4-bit quantization with LoRA adaptation.\n    This layer provides:\n    - 4-bit quantized weights for memory efficiency\n    - LoRA adaptation for fine-tuning\n    - Efficient forward pass with quantized operations\n    \"\"\"\n    def __init__(self, original_layer: nn.Linear, rank: int = 16, alpha: float = 1.0, dropout: float = 0.0):",
        "detail": "qwen-llm.lora_qlora.core.qlora.qlora_linear",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.qlora.qlora_manager",
        "description": "qwen-llm.lora_qlora.core.qlora.qlora_manager",
        "peekOfCode": "class QLoRAManager:\n    \"\"\"\n    🎯 QLORA MANAGER\n    Manages QLoRA adaptation for entire models.\n    This class provides functionality to:\n    - Apply QLoRA to specified modules in a model\n    - Track and manage QLoRA layers\n    - Analyze parameter counts and memory usage\n    - Save and load QLoRA weights\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.core.qlora.qlora_manager",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.quantization.quantization_expert",
        "description": "qwen-llm.lora_qlora.core.quantization.quantization_expert",
        "peekOfCode": "class QuantizationConfig:\n    \"\"\"\n    🎯 QUANTIZATION CONFIGURATION\n    Configuration for different quantization techniques.\n    \"\"\"\n    # Basic quantization\n    bits: int = 8  # Number of bits for quantization (4, 8, 16)\n    symmetric: bool = True  # Symmetric vs asymmetric quantization\n    # LoRA parameters\n    lora_rank: int = 16  # Rank of LoRA adaptation",
        "detail": "qwen-llm.lora_qlora.core.quantization.quantization_expert",
        "documentation": {}
    },
    {
        "label": "QuantizationExpert",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.quantization.quantization_expert",
        "description": "qwen-llm.lora_qlora.core.quantization.quantization_expert",
        "peekOfCode": "class QuantizationExpert:\n    \"\"\"\n    🎓 QUANTIZATION EXPERT CLASS\n    This class demonstrates different quantization techniques and their trade-offs.\n    \"\"\"\n    def __init__(self):\n        self.quantization_methods = {\n            'fp32': self._fp32_quantization,\n            'fp16': self._fp16_quantization,\n            'int8': self._int8_quantization,",
        "detail": "qwen-llm.lora_qlora.core.quantization.quantization_expert",
        "documentation": {}
    },
    {
        "label": "LoRATrainingConfig",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.config",
        "description": "qwen-llm.lora_qlora.core.training.config",
        "peekOfCode": "class LoRATrainingConfig:\n    \"\"\"\n    🎯 LORA TRAINING CONFIGURATION\n    Configuration for LoRA fine-tuning following the original approach.\n    \"\"\"\n    # Model parameters\n    pretrained_model_path: str = \"models/final_model1.pt\"\n    tokenizer_path: str = \"HuggingFaceTB/SmolLM-135M\"\n    # LoRA parameters\n    lora_rank: int = 16",
        "detail": "qwen-llm.lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "QLoRATrainingConfig",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.config",
        "description": "qwen-llm.lora_qlora.core.training.config",
        "peekOfCode": "class QLoRATrainingConfig:\n    \"\"\"\n    🎯 QLORA TRAINING CONFIGURATION\n    Configuration for QLoRA fine-tuning following the original approach.\n    \"\"\"\n    # Model parameters\n    pretrained_model_path: str = \"models/final_model1.pt\"\n    tokenizer_path: str = \"HuggingFaceTB/SmolLM-135M\"\n    # QLoRA parameters\n    lora_rank: int = 16",
        "detail": "qwen-llm.lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "create_lora_config",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.core.training.config",
        "description": "qwen-llm.lora_qlora.core.training.config",
        "peekOfCode": "def create_lora_config(\n    pretrained_model_path: str = \"models/final_model1.pt\",\n    lora_rank: int = 16,\n    lora_alpha: float = 32.0,\n    learning_rate: float = 1e-4,\n    num_epochs: int = 3,\n    batch_size: int = 8,\n    num_samples: int = 1000\n) -> LoRATrainingConfig:\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "create_qlora_config",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.core.training.config",
        "description": "qwen-llm.lora_qlora.core.training.config",
        "peekOfCode": "def create_qlora_config(\n    pretrained_model_path: str = \"models/final_model1.pt\",\n    lora_rank: int = 16,\n    lora_alpha: float = 32.0,\n    qlora_bits: int = 4,\n    learning_rate: float = 2e-4,\n    num_epochs: int = 3,\n    batch_size: int = 8,\n    num_samples: int = 1000\n) -> QLoRATrainingConfig:",
        "detail": "qwen-llm.lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "LoRADataset",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.dataset",
        "description": "qwen-llm.lora_qlora.core.training.dataset",
        "peekOfCode": "class LoRADataset(Dataset):\n    \"\"\"\n    📚 LORA DATASET CLASS\n    Custom dataset for LoRA fine-tuning on IMDB sentiment analysis.\n    \"\"\"\n    def __init__(self, texts: List[str], labels: List[int], tokenizer: AutoTokenizer, max_length: int = 256):\n        \"\"\"\n        Initialize LoRA dataset.\n        Args:\n            texts: List of text samples",
        "detail": "qwen-llm.lora_qlora.core.training.dataset",
        "documentation": {}
    },
    {
        "label": "QLoRADataset",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.dataset",
        "description": "qwen-llm.lora_qlora.core.training.dataset",
        "peekOfCode": "class QLoRADataset(Dataset):\n    \"\"\"\n    📚 QLORA DATASET CLASS\n    Custom dataset for QLoRA fine-tuning on IMDB sentiment analysis.\n    \"\"\"\n    def __init__(self, texts: List[str], labels: List[int], tokenizer: AutoTokenizer, max_length: int = 256):\n        \"\"\"\n        Initialize QLoRA dataset.\n        Args:\n            texts: List of text samples",
        "detail": "qwen-llm.lora_qlora.core.training.dataset",
        "documentation": {}
    },
    {
        "label": "load_data_for_lora",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.core.training.dataset",
        "description": "qwen-llm.lora_qlora.core.training.dataset",
        "peekOfCode": "def load_data_for_lora(tokenizer: AutoTokenizer, max_length: int = 256, num_samples: int = 1000):\n    \"\"\"\n    📊 LOAD DATA FOR LORA FINE-TUNING\n    Loads IMDB dataset for LoRA fine-tuning.\n    Args:\n        tokenizer: Tokenizer for encoding text\n        max_length: Maximum sequence length\n        num_samples: Number of samples to use\n    Returns:\n        Tuple of (train_dataset, test_dataset)",
        "detail": "qwen-llm.lora_qlora.core.training.dataset",
        "documentation": {}
    },
    {
        "label": "load_data_for_qlora",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.core.training.dataset",
        "description": "qwen-llm.lora_qlora.core.training.dataset",
        "peekOfCode": "def load_data_for_qlora(tokenizer: AutoTokenizer, max_length: int = 256, num_samples: int = 1000):\n    \"\"\"\n    📊 LOAD DATA FOR QLORA FINE-TUNING\n    Loads IMDB dataset for QLoRA fine-tuning.\n    Args:\n        tokenizer: Tokenizer for encoding text\n        max_length: Maximum sequence length\n        num_samples: Number of samples to use\n    Returns:\n        Tuple of (train_dataset, test_dataset)",
        "detail": "qwen-llm.lora_qlora.core.training.dataset",
        "documentation": {}
    },
    {
        "label": "LoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.trainer",
        "description": "qwen-llm.lora_qlora.core.training.trainer",
        "peekOfCode": "class LoRAClassifier(nn.Module):\n    \"\"\"\n    🎯 LORA CLASSIFIER\n    Combines pre-trained model with LoRA adaptation and classification head.\n    \"\"\"\n    def __init__(self, pretrained_model: MinimalLLM, num_classes: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Add classification head\n        self.classifier = nn.Sequential(",
        "detail": "qwen-llm.lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "QLoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.trainer",
        "description": "qwen-llm.lora_qlora.core.training.trainer",
        "peekOfCode": "class QLoRAClassifier(nn.Module):\n    \"\"\"\n    🎯 QLORA CLASSIFIER\n    Combines pre-trained model with QLoRA adaptation and classification head.\n    \"\"\"\n    def __init__(self, pretrained_model: MinimalLLM, num_classes: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Add classification head\n        self.classifier = nn.Sequential(",
        "detail": "qwen-llm.lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "LoRATrainer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.trainer",
        "description": "qwen-llm.lora_qlora.core.training.trainer",
        "peekOfCode": "class LoRATrainer:\n    \"\"\"\n    🎯 LORA TRAINER\n    Trainer class for LoRA fine-tuning following the original approach.\n    \"\"\"\n    def __init__(self, config: LoRATrainingConfig):\n        \"\"\"\n        Initialize LoRA trainer.\n        Args:\n            config: LoRA training configuration",
        "detail": "qwen-llm.lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "QLoRATrainer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.trainer",
        "description": "qwen-llm.lora_qlora.core.training.trainer",
        "peekOfCode": "class QLoRATrainer:\n    \"\"\"\n    🎯 QLORA TRAINER\n    Trainer class for QLoRA fine-tuning following the original approach.\n    \"\"\"\n    def __init__(self, config: QLoRATrainingConfig):\n        \"\"\"\n        Initialize QLoRA trainer.\n        Args:\n            config: QLoRA training configuration",
        "detail": "qwen-llm.lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "UniversalLoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "description": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "peekOfCode": "class UniversalLoRAClassifier(nn.Module):\n    \"\"\"\n    🎯 UNIVERSAL LORA CLASSIFIER\n    Works with any base model (HuggingFace or custom).\n    \"\"\"\n    def __init__(self, base_model: nn.Module, num_classes: int = 2, dropout: float = 0.1, \n                 model_type: str = \"custom\", hidden_size: Optional[int] = None):\n        super().__init__()\n        self.base_model = base_model\n        self.model_type = model_type",
        "detail": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "UniversalQLoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "description": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "peekOfCode": "class UniversalQLoRAClassifier(nn.Module):\n    \"\"\"\n    🎯 UNIVERSAL QLORA CLASSIFIER\n    Works with any base model (HuggingFace or custom).\n    \"\"\"\n    def __init__(self, base_model: nn.Module, num_classes: int = 2, dropout: float = 0.1, \n                 model_type: str = \"custom\", hidden_size: Optional[int] = None):\n        super().__init__()\n        self.base_model = base_model\n        self.model_type = model_type",
        "detail": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "UniversalLoRATrainer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "description": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "peekOfCode": "class UniversalLoRATrainer:\n    \"\"\"\n    🎯 UNIVERSAL LORA TRAINER\n    Can work with any base model (HuggingFace or custom).\n    \"\"\"\n    def __init__(self, config: LoRATrainingConfig):\n        \"\"\"\n        Initialize Universal LoRA trainer.\n        Args:\n            config: LoRA training configuration",
        "detail": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "fine_tune_huggingface_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "description": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "peekOfCode": "def fine_tune_huggingface_model(model_name: str, config: LoRATrainingConfig):\n    \"\"\"\n    Fine-tune a HuggingFace model with LoRA.\n    Args:\n        model_name: HuggingFace model name (e.g., \"bert-base-uncased\")\n        config: LoRA training configuration\n    \"\"\"\n    trainer = UniversalLoRATrainer(config)\n    trainer.setup_model(model_name_or_path=model_name, model_type=\"huggingface\")\n    trainer.load_data()",
        "detail": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "fine_tune_custom_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "description": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "peekOfCode": "def fine_tune_custom_model(model_path: str, config: LoRATrainingConfig):\n    \"\"\"\n    Fine-tune your custom model with LoRA.\n    Args:\n        model_path: Path to your custom model\n        config: LoRA training configuration\n    \"\"\"\n    config.pretrained_model_path = model_path\n    trainer = UniversalLoRATrainer(config)\n    trainer.setup_model(model_type=\"custom\")",
        "detail": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "compare_lora_qlora",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "description": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "peekOfCode": "def compare_lora_qlora():\n    \"\"\"Compare LoRA and QLoRA performance.\"\"\"\n    print(\"🎯 LoRA vs QLoRA Comparison\")\n    print(\"=\" * 50)\n    # Common configuration\n    common_config = {\n        'model_name': \"Qwen/Qwen2.5-0.5B\",\n        'data_path': \"data/classification_data.json\",\n        'num_epochs': 2,\n        'batch_size': 8,",
        "detail": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "documentation": {}
    },
    {
        "label": "hyperparameter_search",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "description": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "peekOfCode": "def hyperparameter_search():\n    \"\"\"Perform hyperparameter search.\"\"\"\n    print(\"🎯 Hyperparameter Search\")\n    print(\"=\" * 50)\n    # Define search space\n    lora_ranks = [8, 16, 32, 64]\n    lora_alphas = [16.0, 32.0, 64.0, 128.0]\n    learning_rates = [1e-4, 2e-4, 5e-4, 1e-3]\n    best_config = None\n    best_score = 0.0",
        "detail": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "documentation": {}
    },
    {
        "label": "custom_data_processing",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "description": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "peekOfCode": "def custom_data_processing():\n    \"\"\"Demonstrate custom data processing.\"\"\"\n    print(\"🎯 Custom Data Processing\")\n    print(\"=\" * 50)\n    # Load and preprocess data\n    data = load_data(\"data/classification_data.json\")\n    print(f\"Original data size: {len(data)}\")\n    # Preprocess\n    processed_data = preprocess_data(data)\n    print(f\"Processed data size: {len(processed_data)}\")",
        "detail": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "description": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "peekOfCode": "def main():\n    \"\"\"Main function for advanced examples.\"\"\"\n    print(\"🎯 Advanced LoRA/QLoRA Examples\")\n    print(\"=\" * 50)\n    # Run examples\n    compare_lora_qlora()\n    print(\"\\n\" + \"=\" * 50)\n    hyperparameter_search()\n    print(\"\\n\" + \"=\" * 50)\n    custom_data_processing()",
        "detail": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "documentation": {}
    },
    {
        "label": "example_huggingface_models",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "description": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "peekOfCode": "def example_huggingface_models():\n    \"\"\"\n    🎯 EXAMPLE: Fine-tune HuggingFace models with LoRA\n    \"\"\"\n    print(\"🎯 HuggingFace Models with LoRA\")\n    print(\"=\" * 50)\n    # Example 1: BERT\n    print(\"\\n1. Fine-tuning BERT with LoRA:\")\n    config = LoRATrainingConfig(\n        num_epochs=1,",
        "detail": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "documentation": {}
    },
    {
        "label": "example_custom_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "description": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "peekOfCode": "def example_custom_model():\n    \"\"\"\n    🎯 EXAMPLE: Fine-tune your custom model with LoRA\n    \"\"\"\n    print(\"\\n🎯 Custom Model with LoRA\")\n    print(\"=\" * 50)\n    config = LoRATrainingConfig(\n        pretrained_model_path=\"models/final_model1.pt\",\n        num_epochs=1,\n        batch_size=4,",
        "detail": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "documentation": {}
    },
    {
        "label": "example_manual_setup",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "description": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "peekOfCode": "def example_manual_setup():\n    \"\"\"\n    🎯 EXAMPLE: Manual setup for more control\n    \"\"\"\n    print(\"\\n🎯 Manual Setup Example\")\n    print(\"=\" * 50)\n    # Create config\n    config = LoRATrainingConfig(\n        num_epochs=1,\n        batch_size=4,",
        "detail": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "description": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Demonstrates different ways to use the universal trainer.\n    \"\"\"\n    print(\"🎯 Universal LoRA Fine-tuning Examples\")\n    print(\"=\" * 60)\n    # Example 1: HuggingFace models\n    example_huggingface_models()\n    # Example 2: Custom model",
        "detail": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.basic.lora_example",
        "description": "qwen-llm.lora_qlora.examples.basic.lora_example",
        "peekOfCode": "def main():\n    \"\"\"Main function for LoRA example.\"\"\"\n    print(\"🎯 LoRA Fine-tuning Example\")\n    print(\"=\" * 50)\n    # Configuration following original approach\n    config = LoRATrainingConfig(\n        pretrained_model_path=\"models/final_model1.pt\",\n        num_epochs=1,\n        batch_size=4,\n        learning_rate=1e-4,",
        "detail": "qwen-llm.lora_qlora.examples.basic.lora_example",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.basic.qlora_example",
        "description": "qwen-llm.lora_qlora.examples.basic.qlora_example",
        "peekOfCode": "def main():\n    \"\"\"Main function for QLoRA example.\"\"\"\n    print(\"🎯 QLoRA Fine-tuning Example\")\n    print(\"=\" * 50)\n    # Configuration following original approach\n    config = QLoRATrainingConfig(\n        pretrained_model_path=\"models/final_model1.pt\",\n        num_epochs=1,\n        batch_size=4,\n        learning_rate=2e-4,",
        "detail": "qwen-llm.lora_qlora.examples.basic.qlora_example",
        "documentation": {}
    },
    {
        "label": "TestTrainingPipeline",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.integration.test_training_pipeline",
        "description": "qwen-llm.lora_qlora.tests.integration.test_training_pipeline",
        "peekOfCode": "class TestTrainingPipeline(unittest.TestCase):\n    \"\"\"Test cases for training pipeline.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        # Create temporary directory\n        self.temp_dir = tempfile.mkdtemp()\n        # Create sample data\n        self.sample_data = [\n            {\"text\": \"This is a positive review\", \"label\": 1},\n            {\"text\": \"This is a negative review\", \"label\": 0},",
        "detail": "qwen-llm.lora_qlora.tests.integration.test_training_pipeline",
        "documentation": {}
    },
    {
        "label": "TestLoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "description": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "peekOfCode": "class TestLoRALayer(unittest.TestCase):\n    \"\"\"Test cases for LoRALayer.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.in_features = 128\n        self.out_features = 64\n        self.rank = 16\n        self.alpha = 32.0\n        self.dropout = 0.1\n        self.lora_layer = LoRALayer(",
        "detail": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "documentation": {}
    },
    {
        "label": "TestLoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "description": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "peekOfCode": "class TestLoRALinear(unittest.TestCase):\n    \"\"\"Test cases for LoRALinear.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.in_features = 128\n        self.out_features = 64\n        self.rank = 16\n        self.alpha = 32.0\n        self.dropout = 0.1\n        # Create original linear layer",
        "detail": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "documentation": {}
    },
    {
        "label": "TestLoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "description": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "peekOfCode": "class TestLoRAManager(unittest.TestCase):\n    \"\"\"Test cases for LoRAManager.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        # Create a simple model\n        self.model = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),",
        "detail": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "documentation": {}
    },
    {
        "label": "TestQLoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "description": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "peekOfCode": "class TestQLoRALayer(unittest.TestCase):\n    \"\"\"Test cases for QLoRALayer.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.in_features = 128\n        self.out_features = 64\n        self.rank = 16\n        self.alpha = 32.0\n        self.dropout = 0.1\n        self.qlora_layer = QLoRALayer(",
        "detail": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "documentation": {}
    },
    {
        "label": "TestQLoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "description": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "peekOfCode": "class TestQLoRALinear(unittest.TestCase):\n    \"\"\"Test cases for QLoRALinear.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.in_features = 128\n        self.out_features = 64\n        self.rank = 16\n        self.alpha = 32.0\n        self.dropout = 0.1\n        # Create original linear layer",
        "detail": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "documentation": {}
    },
    {
        "label": "TestQLoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "description": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "peekOfCode": "class TestQLoRAManager(unittest.TestCase):\n    \"\"\"Test cases for QLoRAManager.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        # Create a simple model\n        self.model = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),",
        "detail": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.config.config",
        "description": "qwen-llm.lora_qlora.utils.config.config",
        "peekOfCode": "def load_config(config_path: Union[str, Path]) -> Dict[str, Any]:\n    \"\"\"\n    🎯 LOAD CONFIGURATION\n    Load configuration from file.\n    Args:\n        config_path: Path to configuration file\n    Returns:\n        Configuration dictionary\n    \"\"\"\n    config_path = Path(config_path)",
        "detail": "qwen-llm.lora_qlora.utils.config.config",
        "documentation": {}
    },
    {
        "label": "save_config",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.config.config",
        "description": "qwen-llm.lora_qlora.utils.config.config",
        "peekOfCode": "def save_config(config: Dict[str, Any], config_path: Union[str, Path], format: str = 'json'):\n    \"\"\"\n    🎯 SAVE CONFIGURATION\n    Save configuration to file.\n    Args:\n        config: Configuration dictionary\n        config_path: Path to save configuration\n        format: File format ('json' or 'yaml')\n    \"\"\"\n    config_path = Path(config_path)",
        "detail": "qwen-llm.lora_qlora.utils.config.config",
        "documentation": {}
    },
    {
        "label": "merge_configs",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.config.config",
        "description": "qwen-llm.lora_qlora.utils.config.config",
        "peekOfCode": "def merge_configs(base_config: Dict[str, Any], override_config: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    🎯 MERGE CONFIGURATIONS\n    Merge two configurations with override taking precedence.\n    Args:\n        base_config: Base configuration\n        override_config: Override configuration\n    Returns:\n        Merged configuration\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.config.config",
        "documentation": {}
    },
    {
        "label": "validate_config",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.config.config",
        "description": "qwen-llm.lora_qlora.utils.config.config",
        "peekOfCode": "def validate_config(config: Dict[str, Any], required_keys: list) -> bool:\n    \"\"\"\n    🎯 VALIDATE CONFIGURATION\n    Validate that configuration contains required keys.\n    Args:\n        config: Configuration to validate\n        required_keys: List of required keys\n    Returns:\n        True if valid, False otherwise\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.config.config",
        "documentation": {}
    },
    {
        "label": "get_config_value",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.config.config",
        "description": "qwen-llm.lora_qlora.utils.config.config",
        "peekOfCode": "def get_config_value(config: Dict[str, Any], key: str, default: Any = None) -> Any:\n    \"\"\"\n    🎯 GET CONFIG VALUE\n    Get configuration value with default fallback.\n    Args:\n        config: Configuration dictionary\n        key: Key to retrieve\n        default: Default value if key not found\n    Returns:\n        Configuration value or default",
        "detail": "qwen-llm.lora_qlora.utils.config.config",
        "documentation": {}
    },
    {
        "label": "update_config",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.config.config",
        "description": "qwen-llm.lora_qlora.utils.config.config",
        "peekOfCode": "def update_config(config: Dict[str, Any], updates: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    🎯 UPDATE CONFIGURATION\n    Update configuration with new values.\n    Args:\n        config: Configuration to update\n        updates: Updates to apply\n    Returns:\n        Updated configuration\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.config.config",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.data.data",
        "description": "qwen-llm.lora_qlora.utils.data.data",
        "peekOfCode": "def load_data(data_path: str, format: str = 'auto') -> List[Dict[str, Any]]:\n    \"\"\"\n    🎯 LOAD DATA\n    Load data from file.\n    Args:\n        data_path: Path to data file\n        format: Data format ('json', 'csv', 'auto')\n    Returns:\n        List of data samples\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.data.data",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.data.data",
        "description": "qwen-llm.lora_qlora.utils.data.data",
        "peekOfCode": "def preprocess_data(data: List[Dict[str, Any]], text_field: str = 'text', label_field: str = 'label') -> List[Dict[str, Any]]:\n    \"\"\"\n    🎯 PREPROCESS DATA\n    Preprocess data for training.\n    Args:\n        data: Raw data\n        text_field: Name of text field\n        label_field: Name of label field\n    Returns:\n        Preprocessed data",
        "detail": "qwen-llm.lora_qlora.utils.data.data",
        "documentation": {}
    },
    {
        "label": "split_data",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.data.data",
        "description": "qwen-llm.lora_qlora.utils.data.data",
        "peekOfCode": "def split_data(data: List[Dict[str, Any]], train_split: float = 0.8, val_split: float = 0.1, test_split: float = 0.1, random_seed: int = 42) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n    \"\"\"\n    🎯 SPLIT DATA\n    Split data into train, validation, and test sets.\n    Args:\n        data: Data to split\n        train_split: Training set proportion\n        val_split: Validation set proportion\n        test_split: Test set proportion\n        random_seed: Random seed for reproducibility",
        "detail": "qwen-llm.lora_qlora.utils.data.data",
        "documentation": {}
    },
    {
        "label": "balance_data",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.data.data",
        "description": "qwen-llm.lora_qlora.utils.data.data",
        "peekOfCode": "def balance_data(data: List[Dict[str, Any]], label_field: str = 'label', method: str = 'undersample') -> List[Dict[str, Any]]:\n    \"\"\"\n    🎯 BALANCE DATA\n    Balance data by class distribution.\n    Args:\n        data: Data to balance\n        label_field: Name of label field\n        method: Balancing method ('undersample', 'oversample')\n    Returns:\n        Balanced data",
        "detail": "qwen-llm.lora_qlora.utils.data.data",
        "documentation": {}
    },
    {
        "label": "filter_data",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.data.data",
        "description": "qwen-llm.lora_qlora.utils.data.data",
        "peekOfCode": "def filter_data(data: List[Dict[str, Any]], min_length: int = 10, max_length: int = 1000) -> List[Dict[str, Any]]:\n    \"\"\"\n    🎯 FILTER DATA\n    Filter data by text length.\n    Args:\n        data: Data to filter\n        min_length: Minimum text length\n        max_length: Maximum text length\n    Returns:\n        Filtered data",
        "detail": "qwen-llm.lora_qlora.utils.data.data",
        "documentation": {}
    },
    {
        "label": "get_data_stats",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.data.data",
        "description": "qwen-llm.lora_qlora.utils.data.data",
        "peekOfCode": "def get_data_stats(data: List[Dict[str, Any]], label_field: str = 'label') -> Dict[str, Any]:\n    \"\"\"\n    🎯 GET DATA STATISTICS\n    Get statistics about the data.\n    Args:\n        data: Data to analyze\n        label_field: Name of label field\n    Returns:\n        Data statistics\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.data.data",
        "documentation": {}
    },
    {
        "label": "ModelServer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.utils.serving.serving",
        "description": "qwen-llm.lora_qlora.utils.serving.serving",
        "peekOfCode": "class ModelServer:\n    \"\"\"\n    🎯 MODEL SERVER\n    Server for serving trained LoRA and QLoRA models using your custom MinimalLLM.\n    This class provides:\n    - Model loading and initialization\n    - Inference capabilities\n    - Batch processing\n    - Performance monitoring\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.serving.serving",
        "documentation": {}
    },
    {
        "label": "InferenceEngine",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.utils.serving.serving",
        "description": "qwen-llm.lora_qlora.utils.serving.serving",
        "peekOfCode": "class InferenceEngine:\n    \"\"\"\n    🎯 INFERENCE ENGINE\n    High-performance inference engine for LoRA and QLoRA models.\n    This class provides:\n    - Optimized inference\n    - Caching mechanisms\n    - Performance monitoring\n    - Batch processing\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.serving.serving",
        "documentation": {}
    },
    {
        "label": "naive_generate_text",
        "kind": 2,
        "importPath": "qwen-llm.original_files.compare_inference",
        "description": "qwen-llm.original_files.compare_inference",
        "peekOfCode": "def naive_generate_text(model: MinimalLLM, tokenizer, prompt: str, max_length: int = 100,\n                       temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9) -> str:\n    \"\"\"\n    🐌 NAIVE TEXT GENERATION (NO KV CACHE)\n    This is the original text generation method that processes the entire sequence\n    for each new token. Very slow but simple.\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    # Tokenize prompt",
        "detail": "qwen-llm.original_files.compare_inference",
        "documentation": {}
    },
    {
        "label": "compare_inference_methods",
        "kind": 2,
        "importPath": "qwen-llm.original_files.compare_inference",
        "description": "qwen-llm.original_files.compare_inference",
        "peekOfCode": "def compare_inference_methods(model_path: str, tokenizer_path: str, \n                            test_prompts: List[str], max_new_tokens: int = 50):\n    \"\"\"\n    Compare different inference methods\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        test_prompts: List of test prompts\n        max_new_tokens: Maximum tokens to generate\n    \"\"\"",
        "detail": "qwen-llm.original_files.compare_inference",
        "documentation": {}
    },
    {
        "label": "run_quick_test",
        "kind": 2,
        "importPath": "qwen-llm.original_files.compare_inference",
        "description": "qwen-llm.original_files.compare_inference",
        "peekOfCode": "def run_quick_test():\n    \"\"\"Run a quick test with simple prompts\"\"\"\n    test_prompts = [\n        \"Hello, how are you?\",\n        \"Tell me a joke about\",\n        \"Write a short story about\",\n        \"Explain the concept of\",\n        \"What is the meaning of\"\n    ]\n    # You'll need to provide actual model paths",
        "detail": "qwen-llm.original_files.compare_inference",
        "documentation": {}
    },
    {
        "label": "run_comprehensive_benchmark",
        "kind": 2,
        "importPath": "qwen-llm.original_files.compare_inference",
        "description": "qwen-llm.original_files.compare_inference",
        "peekOfCode": "def run_comprehensive_benchmark():\n    \"\"\"Run a comprehensive benchmark with more prompts\"\"\"\n    test_prompts = [\n        \"The quick brown fox jumps over the lazy dog. This is a test of\",\n        \"In a world where artificial intelligence has become\",\n        \"The ancient library contained thousands of books about\",\n        \"As the sun set over the mountains, the travelers\",\n        \"The scientist discovered a new element that could\",\n        \"Once upon a time, in a distant galaxy\",\n        \"The recipe for the perfect chocolate cake includes\",",
        "detail": "qwen-llm.original_files.compare_inference",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class SamplingParams:\n    \"\"\"Sampling parameters for text generation\"\"\"\n    max_new_tokens: int = 100\n    temperature: float = 0.8\n    top_k: int = 50\n    top_p: float = 0.9\n    repetition_penalty: float = 1.0\n    stop_token_ids: List[int] = None\n    def __post_init__(self):\n        if self.stop_token_ids is None:",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class Sequence:\n    \"\"\"Represents a single generation sequence\"\"\"\n    seq_id: int\n    prompt: str\n    input_ids: torch.Tensor\n    output_ids: List[int]\n    sampling_params: SamplingParams\n    batch_idx: int = -1\n    finished: bool = False\n    prefill_done: bool = False",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "PagedKVCache",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class PagedKVCache(nn.Module):\n    \"\"\"\n    🧠 PAGED KV CACHE\n    Efficient memory management for KV cache using page-based allocation.\n    Inspired by vLLM's PagedAttention but simplified for our use case.\n    \"\"\"\n    def __init__(self, n_pages: int, page_size: int, n_heads: int, head_dim: int, dtype: torch.dtype, device: str):\n        super().__init__()\n        self.n_pages = n_pages\n        self.page_size = page_size",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "OptimizedAttention",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class OptimizedAttention(nn.Module):\n    \"\"\"\n    🎯 OPTIMIZED ATTENTION WITH KV CACHE\n    Combines GQA with efficient KV caching for fast inference.\n    \"\"\"\n    def __init__(self, config: SmallModelConfig, kv_cache: PagedKVCache):\n        super().__init__()\n        self.d_model = config.d_model\n        self.n_heads = config.n_heads\n        self.n_kv_heads = config.n_kv_heads",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "OptimizedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class OptimizedTransformerBlock(nn.Module):\n    \"\"\"\n    🏗️ OPTIMIZED TRANSFORMER BLOCK\n    Transformer block with optimized attention and KV caching.\n    \"\"\"\n    def __init__(self, config: SmallModelConfig, kv_cache: PagedKVCache):\n        super().__init__()\n        self.attention = OptimizedAttention(config, kv_cache)\n        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n        # Pre-norm architecture",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "FastInferenceEngine",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class FastInferenceEngine:\n    \"\"\"\n    🚀 FAST INFERENCE ENGINE\n    High-performance inference engine with:\n    - Paged KV cache for memory efficiency\n    - Continuous batching for throughput\n    - CUDA graphs for optimization\n    - Dynamic scheduling\n    \"\"\"\n    def __init__(self, model: MinimalLLM, tokenizer, config: SmallModelConfig, ",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "create_fast_inference_engine",
        "kind": 2,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "def create_fast_inference_engine(model_path: str, tokenizer_path: str, \n                                max_batch_size: int = 32, max_seq_len: int = 2048,\n                                n_pages: int = 1000, page_size: int = 128) -> FastInferenceEngine:\n    \"\"\"\n    Create a fast inference engine from saved model\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        max_batch_size: Maximum batch size\n        max_seq_len: Maximum sequence length",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "benchmark_inference_engine",
        "kind": 2,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "def benchmark_inference_engine(engine: FastInferenceEngine, num_requests: int = 100, \n                              max_input_len: int = 512, max_output_len: int = 256):\n    \"\"\"\n    Benchmark the inference engine\n    Args:\n        engine: FastInferenceEngine instance\n        num_requests: Number of requests to process\n        max_input_len: Maximum input length\n        max_output_len: Maximum output length\n    \"\"\"",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "SimpleKVCache",
        "kind": 6,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "class SimpleKVCache:\n    \"\"\"\n    🎯 SIMPLE KV CACHE\n    A straightforward KV cache implementation that stores key-value pairs\n    for each sequence position. Much simpler than paged attention but still effective.\n    \"\"\"\n    def __init__(self, max_seq_len: int, n_heads: int, head_dim: int, dtype: torch.dtype, device: str):\n        self.max_seq_len = max_seq_len\n        self.n_heads = n_heads\n        self.head_dim = head_dim",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "CachedAttention",
        "kind": 6,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "class CachedAttention(nn.Module):\n    \"\"\"\n    🎯 CACHED ATTENTION\n    Attention layer with simple KV caching for fast inference.\n    \"\"\"\n    def __init__(self, config: SmallModelConfig, kv_cache: SimpleKVCache):\n        super().__init__()\n        self.d_model = config.d_model\n        self.n_heads = config.n_heads\n        self.n_kv_heads = config.n_kv_heads",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "CachedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "class CachedTransformerBlock(nn.Module):\n    \"\"\"\n    🏗️ CACHED TRANSFORMER BLOCK\n    Transformer block with cached attention.\n    \"\"\"\n    def __init__(self, config: SmallModelConfig, kv_cache: SimpleKVCache):\n        super().__init__()\n        self.attention = CachedAttention(config, kv_cache)\n        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n        # Pre-norm architecture",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "kind": 6,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "class SimpleFastInference:\n    \"\"\"\n    🚀 SIMPLE FAST INFERENCE ENGINE\n    A simplified but fast inference engine that adds KV caching to your existing model.\n    Much easier to understand and integrate than full vLLM-style implementations.\n    \"\"\"\n    def __init__(self, model: MinimalLLM, tokenizer, config: SmallModelConfig, \n                 max_seq_len: int = 2048):\n        self.model = model\n        self.tokenizer = tokenizer",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "kind": 2,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "def create_simple_fast_inference(model_path: str, tokenizer_path: str, \n                                max_seq_len: int = 2048) -> SimpleFastInference:\n    \"\"\"\n    Create a simple fast inference engine from saved model\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        max_seq_len: Maximum sequence length\n    Returns:\n        SimpleFastInference instance",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "benchmark_simple_inference",
        "kind": 2,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "def benchmark_simple_inference(engine: SimpleFastInference, num_requests: int = 10, \n                              max_input_len: int = 100, max_output_len: int = 100):\n    \"\"\"\n    Benchmark the simple inference engine\n    Args:\n        engine: SimpleFastInference instance\n        num_requests: Number of requests to process\n        max_input_len: Maximum input length\n        max_output_len: Maximum output length\n    \"\"\"",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "PretrainingConfig",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.config.config",
        "description": "qwen-llm.pretraining.core.config.config",
        "peekOfCode": "class PretrainingConfig:\n    \"\"\"\n    🎯 PRETRAINING CONFIGURATION\n    This configuration is optimized for:\n    - Fast training on CPU/limited GPU\n    - Learning the concepts without waiting hours\n    - Still demonstrating all key components\n    \"\"\"\n    # Model architecture - MUCH SMALLER\n    d_model: int = 128          # Reduced from 384 (3x smaller)",
        "detail": "qwen-llm.pretraining.core.config.config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.core.config.config",
        "description": "qwen-llm.pretraining.core.config.config",
        "peekOfCode": "def set_seed(seed: int = 42):\n    \"\"\"Set all random seeds for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f\"🌱 Set all seeds to {seed}\")\n@dataclass",
        "detail": "qwen-llm.pretraining.core.config.config",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.model.components",
        "description": "qwen-llm.pretraining.core.model.components",
        "peekOfCode": "class Rotary(nn.Module):\n    \"\"\"\n    🔄 ROTARY POSITIONAL EMBEDDINGS (RoPE)\n    This is one of the most important innovations in modern transformers!\n    🎯 What RoPE does:\n    - Encodes position information by ROTATING vectors\n    - Unlike traditional positional embeddings that just ADD position info\n    - Allows the model to understand relative positions naturally\n    🧮 The Math:\n    - For each position i, we compute rotation angles based on i",
        "detail": "qwen-llm.pretraining.core.model.components",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.model.components",
        "description": "qwen-llm.pretraining.core.model.components",
        "peekOfCode": "class Qwen3Attention(nn.Module):\n    \"\"\"\n    🎯 GROUPED-QUERY ATTENTION (GQA) IMPLEMENTATION\n    This is the heart of modern transformer attention mechanisms!\n    🧠 Key Innovations:\n    1. Grouped-Query Attention: Fewer KV heads than Query heads\n    2. QK-Normalization: Normalizes queries and keys for stability\n    3. RoPE: Rotary positional embeddings\n    4. Scaled dot-product attention: The core attention mechanism\n    🔍 How it works:",
        "detail": "qwen-llm.pretraining.core.model.components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.model.components",
        "description": "qwen-llm.pretraining.core.model.components",
        "peekOfCode": "class SwiGLUFeedForward(nn.Module):\n    \"\"\"\n    🔥 SWIGLU FEED-FORWARD NETWORK\n    SwiGLU is a modern activation function that combines:\n    - Swish activation: x * sigmoid(x) (smooth, non-monotonic)\n    - GLU (Gated Linear Unit): element-wise multiplication with a gate\n    🧮 The Math:\n    SwiGLU(x) = Swish(W1(x)) ⊙ W2(x)\n    Where:\n    - W1(x) is the \"gate\" (controls information flow)",
        "detail": "qwen-llm.pretraining.core.model.components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.model.components",
        "description": "qwen-llm.pretraining.core.model.components",
        "peekOfCode": "class TransformerBlock(nn.Module):\n    \"\"\"\n    🏗️ TRANSFORMER BLOCK - THE BUILDING BLOCK OF MODERN LLMs\n    This combines all the components into a complete transformer layer:\n    🧠 Architecture:\n    1. Pre-norm attention (RMSNorm before attention)\n    2. Residual connection (x + attention(x))\n    3. Pre-norm feed-forward (RMSNorm before SwiGLU)\n    4. Residual connection (x + feedforward(x))\n    🔍 Pre-norm vs Post-norm:",
        "detail": "qwen-llm.pretraining.core.model.components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.model.components",
        "description": "qwen-llm.pretraining.core.model.components",
        "peekOfCode": "class RMSNorm(nn.Module):\n    \"\"\"\n    📐 RMSNorm - ROOT MEAN SQUARE NORMALIZATION\n    This is a modern alternative to LayerNorm that's more efficient:\n    🧮 The Math:\n    RMSNorm(x) = x / sqrt(mean(x²) + ε) * g\n    Where:\n    - x is the input\n    - mean(x²) is the mean of squared values\n    - ε is a small constant (1e-6)",
        "detail": "qwen-llm.pretraining.core.model.components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.core.model.components",
        "description": "qwen-llm.pretraining.core.model.components",
        "peekOfCode": "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    🔑 GROUPED-QUERY ATTENTION HELPER\n    This implements the key innovation in GQA:\n    - Fewer Key-Value heads than Query heads\n    - Each KV head is \"shared\" across multiple Query heads\n    - Massive memory savings with minimal performance loss\n    🧮 Example:\n    - 8 Query heads, 2 KV heads\n    - KV head 1 is used by Query heads 1,2,3,4",
        "detail": "qwen-llm.pretraining.core.model.components",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.model.minimal_llm",
        "description": "qwen-llm.pretraining.core.model.minimal_llm",
        "peekOfCode": "class MinimalLLM(nn.Module):\n    \"\"\"\n    🏗️ COMPLETE QWEN3-STYLE LANGUAGE MODEL\n    This is the full language model that combines all components:\n    🧠 Architecture:\n    1. Token Embedding: Convert token IDs to vectors\n    2. Positional Dropout: Prevent overfitting on positions\n    3. Transformer Blocks: Stack of attention + feed-forward layers\n    4. Final Normalization: RMSNorm before output\n    5. Language Modeling Head: Convert vectors back to token probabilities",
        "detail": "qwen-llm.pretraining.core.model.minimal_llm",
        "documentation": {}
    },
    {
        "label": "Muon",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.training.optimizer",
        "description": "qwen-llm.pretraining.core.training.optimizer",
        "peekOfCode": "class Muon(torch.optim.Optimizer):\n    \"\"\"\n    🚀 MUON OPTIMIZER: MomentUm Orthogonalized by Newton-schulz\n    This is a revolutionary optimizer that combines:\n    1. Momentum (like Adam) - remembers past gradients\n    2. Orthogonalization (Newton-Schulz) - makes gradients \"well-behaved\"\n    3. Adaptive learning rates - adjusts based on matrix dimensions\n    🎯 Why Muon is special:\n    - 30-50% faster convergence than Adam\n    - More stable training (fewer gradient explosions)",
        "detail": "qwen-llm.pretraining.core.training.optimizer",
        "documentation": {}
    },
    {
        "label": "zeropower_via_newtonschulz5",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.core.training.optimizer",
        "description": "qwen-llm.pretraining.core.training.optimizer",
        "peekOfCode": "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n    \"\"\"\n    🔬 NEWTON-SCHULZ ORTHOGONALIZATION\n    This is the mathematical heart of the Muon optimizer:\n    🎯 What it does:\n    - Takes a matrix G (gradients)\n    - Makes it \"orthogonal\" (like rotating it to be perfectly aligned)\n    - Uses Newton-Schulz iteration (a fast numerical method)\n    🧮 Why orthogonalization helps:\n    - Orthogonal matrices preserve vector lengths and angles",
        "detail": "qwen-llm.pretraining.core.training.optimizer",
        "documentation": {}
    },
    {
        "label": "setup_muon_optimizer",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.core.training.optimizer",
        "description": "qwen-llm.pretraining.core.training.optimizer",
        "peekOfCode": "def setup_muon_optimizer(model: nn.Module, config):\n    \"\"\"\n    🚀 HYBRID OPTIMIZER SETUP\n    This function sets up a hybrid optimization strategy:\n    - Muon optimizer for 2D parameters (attention and feed-forward weights)\n    - AdamW optimizer for other parameters (embeddings, norms, biases)\n    🎯 Why hybrid approach:\n    - Muon works best on 2D matrices (attention, feed-forward)\n    - AdamW is better for 1D parameters (embeddings, biases)\n    - This gives us the best of both worlds",
        "detail": "qwen-llm.pretraining.core.training.optimizer",
        "documentation": {}
    },
    {
        "label": "PretrainingTrainer",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.training.trainer",
        "description": "qwen-llm.pretraining.core.training.trainer",
        "peekOfCode": "class PretrainingTrainer:\n    \"\"\"\n    🎯 PRETRAINING TRAINER\n    Complete training pipeline for pretraining language models.\n    \"\"\"\n    def __init__(self, config):\n        self.config = config\n        self.model = None\n        self.optimizers = None\n        self.schedulers = None",
        "detail": "qwen-llm.pretraining.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.core.training.trainer",
        "description": "qwen-llm.pretraining.core.training.trainer",
        "peekOfCode": "def evaluate_model(model: nn.Module, val_loader: DataLoader, config):\n    \"\"\"\n    📊 MODEL EVALUATION FUNCTION\n    This function evaluates the model's performance on validation data:\n    🎯 Metrics Computed:\n    1. Loss: Cross-entropy loss (lower is better)\n    2. Accuracy: Percentage of correct next-token predictions\n    3. Perplexity: exp(loss) - measures model's \"surprise\" (lower is better)\n    🔍 Why these metrics matter:\n    - Loss: Direct measure of how well the model predicts",
        "detail": "qwen-llm.pretraining.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.core.training.trainer",
        "description": "qwen-llm.pretraining.core.training.trainer",
        "peekOfCode": "def load_checkpoint(model_path: str, config):\n    \"\"\"\n    📦 LOAD CHECKPOINT FOR RESUMING TRAINING\n    This function loads a previously trained model checkpoint and returns\n    the model, optimizers, schedulers, and training state.\n    \"\"\"\n    print(f\"📦 Loading checkpoint from {model_path}\")\n    # Load checkpoint with safe loading to handle import path changes\n    try:\n        # Try loading with weights_only=False to handle custom classes",
        "detail": "qwen-llm.pretraining.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.examples.advanced.resume_training",
        "description": "qwen-llm.pretraining.examples.advanced.resume_training",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 RESUME TRAINING EXAMPLE\n    This function demonstrates how to resume training from a checkpoint.\n    \"\"\"\n    print(\"🔄 RESUME TRAINING EXAMPLE\")\n    print(\"=\" * 50)\n    # Check if checkpoint exists\n    checkpoint_path = \"models/best_model1.pt\"\n    if not os.path.exists(checkpoint_path):",
        "detail": "qwen-llm.pretraining.examples.advanced.resume_training",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.examples.basic.inference_example",
        "description": "qwen-llm.pretraining.examples.basic.inference_example",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 INFERENCE EXAMPLE\n    This function demonstrates how to load and use a trained model.\n    \"\"\"\n    print(\"🎭 INFERENCE EXAMPLE\")\n    print(\"=\" * 50)\n    # Check if model exists\n    model_path = \"models/final_model1.pt\"\n    if not os.path.exists(model_path):",
        "detail": "qwen-llm.pretraining.examples.basic.inference_example",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.examples.basic.train_example",
        "description": "qwen-llm.pretraining.examples.basic.train_example",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN TRAINING FUNCTION\n    This function demonstrates the complete pretraining pipeline.\n    \"\"\"\n    print(\"🚀 PRETRAINING EXAMPLE\")\n    print(\"=\" * 50)\n    # Set seed for reproducibility\n    set_seed(42)\n    # Create configuration",
        "detail": "qwen-llm.pretraining.examples.basic.train_example",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.utils.data",
        "description": "qwen-llm.pretraining.utils.data",
        "peekOfCode": "class TextTokenDataset(Dataset):\n    \"\"\"\n    📚 CUSTOM DATASET FOR LANGUAGE MODELING\n    This creates training examples for our language model:\n    🎯 What it does:\n    - Takes a long sequence of tokens\n    - Creates sliding windows of fixed length\n    - Each example: input sequence + target sequence (shifted by 1)\n    📖 Example:\n    Original text: \"The cat sat on the mat\"",
        "detail": "qwen-llm.pretraining.utils.data",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.utils.data",
        "description": "qwen-llm.pretraining.utils.data",
        "peekOfCode": "def load_and_cache_data(config, cache_dir: str = \"data_cache\"):\n    \"\"\"\n    📦 SMART DATA LOADING WITH CACHING\n    This function demonstrates modern ML data handling:\n    🎯 Key features:\n    1. Caching: Avoids reprocessing the same data\n    2. Streaming: Loads large datasets without memory issues\n    3. Tokenization: Converts text to numbers the model can understand\n    4. Efficient storage: Uses pickle for fast loading\n    🔄 The process:",
        "detail": "qwen-llm.pretraining.utils.data",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.utils.generation",
        "description": "qwen-llm.pretraining.utils.generation",
        "peekOfCode": "def generate_text(model, tokenizer, prompt: str, max_length: int = 100,\n                 temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9):\n    \"\"\"\n    🔮 TEXT GENERATION FUNCTION\n    This function generates text using the trained model with advanced sampling:\n    🎯 Sampling Strategies:\n    1. Temperature Scaling: Controls randomness (0.1 = focused, 2.0 = random)\n    2. Top-k Sampling: Only consider top k most likely tokens\n    3. Top-p (Nucleus) Sampling: Consider tokens until cumulative probability reaches p\n    🔍 How it works:",
        "detail": "qwen-llm.pretraining.utils.generation",
        "documentation": {}
    },
    {
        "label": "BenchmarkConfig",
        "kind": 6,
        "importPath": "qwen-llm.benchmark_quantization",
        "description": "qwen-llm.benchmark_quantization",
        "peekOfCode": "class BenchmarkConfig:\n    \"\"\"\n    🎯 BENCHMARK CONFIGURATION\n    \"\"\"\n    # Model parameters\n    d_model: int = 128\n    n_layers: int = 3\n    vocab_size: int = 1000\n    # Benchmark parameters\n    num_samples: int = 100",
        "detail": "qwen-llm.benchmark_quantization",
        "documentation": {}
    },
    {
        "label": "QuantizationBenchmarker",
        "kind": 6,
        "importPath": "qwen-llm.benchmark_quantization",
        "description": "qwen-llm.benchmark_quantization",
        "peekOfCode": "class QuantizationBenchmarker:\n    \"\"\"\n    📊 QUANTIZATION BENCHMARKER\n    Comprehensive benchmarking of different quantization techniques.\n    \"\"\"\n    def __init__(self, config: BenchmarkConfig):\n        self.config = config\n        self.results = {}\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        # Create test data",
        "detail": "qwen-llm.benchmark_quantization",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.benchmark_quantization",
        "description": "qwen-llm.benchmark_quantization",
        "peekOfCode": "def main():\n    \"\"\"Main benchmark function\"\"\"\n    print(\"🎯 QUANTIZATION BENCHMARK\")\n    print(\"=\" * 40)\n    # Create benchmark config\n    config = BenchmarkConfig()\n    config.d_model = 128\n    config.n_layers = 3\n    config.vocab_size = 1000\n    config.num_samples = 100",
        "detail": "qwen-llm.benchmark_quantization",
        "documentation": {}
    },
    {
        "label": "convert_checkpoint",
        "kind": 2,
        "importPath": "qwen-llm.convert_checkpoint",
        "description": "qwen-llm.convert_checkpoint",
        "peekOfCode": "def convert_checkpoint(old_path: str, new_path: str):\n    \"\"\"\n    Convert old checkpoint to new format\n    \"\"\"\n    print(f\"🔄 Converting checkpoint from {old_path} to {new_path}\")\n    try:\n        # Try to load the old checkpoint\n        print(\"📦 Loading old checkpoint...\")\n        checkpoint = torch.load(old_path, map_location='cpu', weights_only=True)\n        # Extract only the essential data",
        "detail": "qwen-llm.convert_checkpoint",
        "documentation": {}
    },
    {
        "label": "demo_finetune",
        "kind": 2,
        "importPath": "qwen-llm.demo_finetune",
        "description": "qwen-llm.demo_finetune",
        "peekOfCode": "def demo_finetune():\n    \"\"\"\n    🎯 DEMO FINE-TUNING PROCESS\n    Demonstrates the fine-tuning process with a small subset of data.\n    \"\"\"\n    print(\"🎯 IMDB Sentiment Analysis Fine-tuning Demo\")\n    print(\"=\" * 50)\n    # Create config for demo (smaller dataset, fewer epochs)\n    config = FineTuneConfig()\n    config.batch_size = 8",
        "detail": "qwen-llm.demo_finetune",
        "documentation": {}
    },
    {
        "label": "FineTuneConfig",
        "kind": 6,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "class FineTuneConfig:\n    \"\"\"\n    🎯 FINE-TUNING CONFIGURATION\n    Configuration for fine-tuning the pre-trained model on IMDB sentiment analysis.\n    \"\"\"\n    # Model architecture\n    d_model: int = 128\n    n_heads: int = 4\n    n_layers: int = 3\n    d_ff: int = 512",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "IMDBDataset",
        "kind": 6,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "class IMDBDataset(Dataset):\n    \"\"\"\n    📚 IMDB DATASET CLASS\n    Custom dataset class for IMDB sentiment analysis.\n    Handles tokenization and padding of movie reviews.\n    \"\"\"\n    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "SentimentClassifier",
        "kind": 6,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "class SentimentClassifier(nn.Module):\n    \"\"\"\n    🎯 SENTIMENT CLASSIFICATION MODEL\n    This model adds a classification head on top of our pre-trained Qwen3 model.\n    It freezes the pre-trained weights and only trains the classification layer.\n    \"\"\"\n    def __init__(self, pretrained_model: MinimalLLM, num_classes: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Freeze pre-trained model parameters",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "load_imdb_data",
        "kind": 2,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "def load_imdb_data(config: FineTuneConfig):\n    \"\"\"\n    📊 LOAD IMDB DATASET\n    Loads and prepares the IMDB sentiment analysis dataset.\n    \"\"\"\n    print(\"📊 Loading IMDB dataset...\")\n    # Load dataset\n    dataset = load_dataset(\"imdb\")\n    # Load tokenizer (same as used in pre-training)\n    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "def evaluate_model(model: SentimentClassifier, test_loader: DataLoader, config: FineTuneConfig):\n    \"\"\"\n    📊 EVALUATE MODEL PERFORMANCE\n    Evaluates the model on the test set and returns accuracy and loss.\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "fine_tune_model",
        "kind": 2,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "def fine_tune_model(config: FineTuneConfig, pretrained_model_path: str):\n    \"\"\"\n    🎯 FINE-TUNE MODEL ON IMDB SENTIMENT ANALYSIS\n    Fine-tunes the pre-trained model on IMDB sentiment analysis task.\n    \"\"\"\n    print(\"🎯 Starting IMDB Sentiment Analysis Fine-tuning\")\n    print(\"=\" * 60)\n    # Set seed for reproducibility\n    set_seed(42)\n    # Load data",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "test_sentiment_classifier",
        "kind": 2,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "def test_sentiment_classifier(model_path: str = \"models/imdb_sentiment_classifier.pt\"):\n    \"\"\"\n    🧪 TEST SENTIMENT CLASSIFIER\n    Tests the fine-tuned sentiment classifier on sample texts.\n    \"\"\"\n    print(\"🧪 Testing Sentiment Classifier\")\n    print(\"=\" * 40)\n    # Load model\n    checkpoint = torch.load(model_path, map_location='cpu')\n    config = checkpoint['config']",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "LoRATrainingConfig",
        "kind": 6,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "class LoRATrainingConfig:\n    \"\"\"\n    🎯 LORA TRAINING CONFIGURATION\n    \"\"\"\n    # Model parameters\n    pretrained_model_path: str = \"models/final_model1.pt\"\n    # LoRA parameters\n    lora_rank: int = 16\n    lora_alpha: float = 32.0\n    lora_dropout: float = 0.1",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "LoRADataset",
        "kind": 6,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "class LoRADataset(Dataset):\n    \"\"\"\n    📚 LORA DATASET CLASS\n    Custom dataset for LoRA fine-tuning on IMDB sentiment analysis.\n    \"\"\"\n    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 256):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "LoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "class LoRAClassifier(nn.Module):\n    \"\"\"\n    🎯 LORA CLASSIFIER\n    Combines pre-trained model with LoRA adaptation and classification head.\n    \"\"\"\n    def __init__(self, pretrained_model: MinimalLLM, num_classes: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Add classification head\n        self.classifier = nn.Sequential(",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "load_data_for_lora",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def load_data_for_lora(config: LoRATrainingConfig):\n    \"\"\"\n    📊 LOAD DATA FOR LORA FINE-TUNING\n    \"\"\"\n    print(\"📊 Loading data for LoRA fine-tuning...\")\n    # Load dataset\n    if config.dataset_name == \"imdb\":\n        dataset = load_dataset(\"imdb\")\n        train_texts = dataset['train']['text'][:config.num_samples]\n        train_labels = dataset['train']['label'][:config.num_samples]",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "setup_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def setup_lora_model(config: LoRATrainingConfig, tokenizer):\n    \"\"\"\n    🏗️ SETUP LORA MODEL\n    Loads pre-trained model and applies LoRA adaptation.\n    \"\"\"\n    print(f\"🏗️ Setting up LoRA model...\")\n    # Load pre-trained model\n    print(f\"📦 Loading pre-trained model from {config.pretrained_model_path}\")\n    checkpoint = torch.load(config.pretrained_model_path, map_location='cpu')\n    # Create model config",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "train_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def train_lora_model(classifier: LoRAClassifier, lora_manager: LoRAManager, \n                    train_loader: DataLoader, test_loader: DataLoader, config: LoRATrainingConfig):\n    \"\"\"\n    🎯 TRAIN LORA MODEL\n    Fine-tunes the model using LoRA adaptation.\n    \"\"\"\n    print(f\"🎯 Starting LoRA fine-tuning...\")\n    # Setup optimizer (only for trainable parameters)\n    trainable_params = lora_manager.get_trainable_parameters()\n    trainable_params.extend([p for p in classifier.classifier.parameters()])",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "evaluate_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def evaluate_lora_model(classifier: LoRAClassifier, test_loader: DataLoader, config: LoRATrainingConfig):\n    \"\"\"\n    📊 EVALUATE LORA MODEL\n    \"\"\"\n    classifier.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "save_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def save_lora_model(classifier: LoRAClassifier, lora_manager: LoRAManager, \n                   config: LoRATrainingConfig, final_accuracy: float):\n    \"\"\"\n    💾 SAVE LORA MODEL\n    Saves the LoRA-adapted model and LoRA weights separately.\n    \"\"\"\n    print(f\"💾 Saving LoRA model...\")\n    # Save LoRA weights\n    lora_weights = {}\n    for name, lora_layer in lora_manager.lora_layers.items():",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "test_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def test_lora_model(model_path: str = \"models/lora_sentiment_classifier.pt\"):\n    \"\"\"\n    🧪 TEST LORA MODEL\n    Tests the LoRA fine-tuned model on sample texts.\n    \"\"\"\n    print(\"🧪 Testing LoRA Model\")\n    print(\"=\" * 40)\n    # Load model\n    checkpoint = torch.load(model_path, map_location='cpu')\n    config = checkpoint['config']",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN LORA FINE-TUNING FUNCTION\n    \"\"\"\n    print(\"🎯 LORA FINE-TUNING FOR SENTIMENT ANALYSIS\")\n    print(\"=\" * 60)\n    # Set seed\n    set_seed(42)\n    # Create config\n    config = LoRATrainingConfig()",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "QLoRATrainingConfig",
        "kind": 6,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "class QLoRATrainingConfig:\n    \"\"\"\n    🎯 QLORA TRAINING CONFIGURATION\n    \"\"\"\n    # Model parameters\n    pretrained_model_path: str = \"models/final_model1.pt\"\n    # QLoRA parameters\n    lora_rank: int = 16\n    lora_alpha: float = 32.0\n    lora_dropout: float = 0.1",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "QLoRADataset",
        "kind": 6,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "class QLoRADataset(Dataset):\n    \"\"\"\n    📚 QLORA DATASET CLASS\n    Custom dataset for QLoRA fine-tuning on IMDB sentiment analysis.\n    \"\"\"\n    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 256):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "QLoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "class QLoRAClassifier(nn.Module):\n    \"\"\"\n    🎯 QLORA CLASSIFIER\n    Combines pre-trained model with QLoRA adaptation and classification head.\n    \"\"\"\n    def __init__(self, pretrained_model: MinimalLLM, num_classes: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Add classification head\n        self.classifier = nn.Sequential(",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "load_data_for_qlora",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def load_data_for_qlora(config: QLoRATrainingConfig):\n    \"\"\"\n    📊 LOAD DATA FOR QLORA FINE-TUNING\n    \"\"\"\n    print(\"📊 Loading data for QLoRA fine-tuning...\")\n    # Load dataset\n    if config.dataset_name == \"imdb\":\n        dataset = load_dataset(\"imdb\")\n        train_texts = dataset['train']['text'][:config.num_samples]\n        train_labels = dataset['train']['label'][:config.num_samples]",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "setup_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def setup_qlora_model(config: QLoRATrainingConfig, tokenizer):\n    \"\"\"\n    🏗️ SETUP QLORA MODEL\n    Loads pre-trained model and applies QLoRA adaptation.\n    \"\"\"\n    print(f\"🏗️ Setting up QLoRA model...\")\n    # Load pre-trained model\n    print(f\"📦 Loading pre-trained model from {config.pretrained_model_path}\")\n    checkpoint = torch.load(config.pretrained_model_path, map_location='cpu')\n    # Create model config",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "train_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def train_qlora_model(classifier: QLoRAClassifier, qlora_manager: QLoRAManager, \n                      train_loader: DataLoader, test_loader: DataLoader, config: QLoRATrainingConfig):\n    \"\"\"\n    🎯 TRAIN QLORA MODEL\n    Fine-tunes the model using QLoRA adaptation.\n    \"\"\"\n    print(f\"🎯 Starting QLoRA fine-tuning...\")\n    # Setup optimizer (only for trainable parameters)\n    trainable_params = qlora_manager.get_trainable_parameters()\n    trainable_params.extend([p for p in classifier.classifier.parameters()])",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "evaluate_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def evaluate_qlora_model(classifier: QLoRAClassifier, test_loader: DataLoader, config: QLoRATrainingConfig):\n    \"\"\"\n    📊 EVALUATE QLORA MODEL\n    \"\"\"\n    classifier.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "save_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def save_qlora_model(classifier: QLoRAClassifier, qlora_manager: QLoRAManager, \n                     config: QLoRATrainingConfig, final_accuracy: float):\n    \"\"\"\n    💾 SAVE QLORA MODEL\n    Saves the QLoRA-adapted model and QLoRA weights separately.\n    \"\"\"\n    print(f\"💾 Saving QLoRA model...\")\n    # Save QLoRA weights\n    qlora_weights = {}\n    for name, qlora_layer in qlora_manager.qlora_layers.items():",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "test_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def test_qlora_model(model_path: str = \"models/qlora_sentiment_classifier.pt\"):\n    \"\"\"\n    🧪 TEST QLORA MODEL\n    Tests the QLoRA fine-tuned model on sample texts.\n    \"\"\"\n    print(\"🧪 Testing QLoRA Model\")\n    print(\"=\" * 40)\n    # Load model\n    checkpoint = torch.load(model_path, map_location='cpu')\n    config = checkpoint['config']",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN QLORA FINE-TUNING FUNCTION\n    \"\"\"\n    print(\"🎯 QLORA FINE-TUNING FOR SENTIMENT ANALYSIS\")\n    print(\"=\" * 60)\n    # Set seed\n    set_seed(42)\n    # Create config\n    config = QLoRATrainingConfig()",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QuantizationConfig:\n    \"\"\"\n    🎯 QUANTIZATION CONFIGURATION\n    Configuration for different quantization techniques.\n    \"\"\"\n    # Basic quantization\n    bits: int = 8  # Number of bits for quantization (4, 8, 16)\n    symmetric: bool = True  # Symmetric vs asymmetric quantization\n    # LoRA parameters\n    lora_rank: int = 16  # Rank of LoRA adaptation",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationExpert",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QuantizationExpert:\n    \"\"\"\n    🎓 QUANTIZATION EXPERT CLASS\n    This class demonstrates different quantization techniques and their trade-offs.\n    \"\"\"\n    def __init__(self):\n        self.quantization_methods = {\n            'fp32': self._fp32_quantization,\n            'fp16': self._fp16_quantization,\n            'int8': self._int8_quantization,",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class LoRALayer(nn.Module):\n    \"\"\"\n    🎯 LORA LAYER IMPLEMENTATION\n    LoRA (Low-Rank Adaptation) decomposes weight updates into low-rank matrices.\n    Mathematical Foundation:\n    W = W₀ + ΔW = W₀ + BA\n    Where:\n    - W₀: Original frozen weights\n    - B: Low-rank matrix (d × r)\n    - A: Low-rank matrix (r × k)",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class LoRALinear(nn.Module):\n    \"\"\"\n    🎯 LORA LINEAR LAYER\n    Combines original linear layer with LoRA adaptation.\n    \"\"\"\n    def __init__(self, original_layer: nn.Linear, rank: int = 16, alpha: float = 32.0, dropout: float = 0.1):\n        super().__init__()\n        self.original_layer = original_layer\n        self.lora = LoRALayer(\n            original_layer.in_features,",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class LoRAManager:\n    \"\"\"\n    🎯 LORA MANAGER\n    Manages LoRA adaptation for entire models.\n    \"\"\"\n    def __init__(self, model: nn.Module, config: QuantizationConfig):\n        self.model = model\n        self.config = config\n        self.lora_layers = {}\n        self.original_layers = {}",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QLoRALayer(nn.Module):\n    \"\"\"\n    🎯 QLORA LAYER IMPLEMENTATION\n    QLoRA combines LoRA with 4-bit quantization for maximum efficiency.\n    Key Innovation:\n    - Quantizes base model to 4-bit\n    - Uses LoRA for adaptation\n    - Enables fine-tuning on consumer hardware\n    Memory Savings:\n    - 4-bit quantization: 8x reduction",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QLoRALinear(nn.Module):\n    \"\"\"\n    🎯 QLORA LINEAR LAYER\n    Combines quantized base weights with LoRA adaptation.\n    \"\"\"\n    def __init__(self, original_layer: nn.Linear, rank: int = 16, alpha: float = 32.0, \n                 dropout: float = 0.1, bits: int = 4):\n        super().__init__()\n        self.original_layer = original_layer\n        self.qlora = QLoRALayer(",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QLoRAManager:\n    \"\"\"\n    🎯 QLORA MANAGER\n    Manages QLoRA adaptation for entire models.\n    \"\"\"\n    def __init__(self, model: nn.Module, config: QuantizationConfig):\n        self.model = model\n        self.config = config\n        self.qlora_layers = {}\n        self.original_layers = {}",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationBenchmark",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QuantizationBenchmark:\n    \"\"\"\n    📊 QUANTIZATION BENCHMARK\n    Comprehensive benchmarking of different quantization techniques.\n    \"\"\"\n    def __init__(self):\n        self.results = {}\n    def benchmark_model(self, model: nn.Module, test_data: torch.Tensor, \n                       configs: List[QuantizationConfig]) -> Dict:\n        \"\"\"",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "demo_quantization",
        "kind": 2,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "def demo_quantization():\n    \"\"\"\n    🎯 QUANTIZATION DEMO\n    Demonstrates different quantization techniques on our Qwen3 model.\n    \"\"\"\n    print(\"🎯 QUANTIZATION EXPERT TUTORIAL\")\n    print(\"=\" * 50)\n    # Create a small model for demo\n    config = SmallModelConfig()\n    config.d_model = 64  # Smaller for demo",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "demo_lora",
        "kind": 2,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "def demo_lora():\n    \"\"\"\n    🎯 LORA DEMO\n    Demonstrates LoRA adaptation on our Qwen3 model.\n    \"\"\"\n    print(\"\\n🎯 LORA (LOW-RANK ADAPTATION) DEMO\")\n    print(\"=\" * 50)\n    # Create model\n    config = SmallModelConfig()\n    config.d_model = 128",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "demo_qlora",
        "kind": 2,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "def demo_qlora():\n    \"\"\"\n    🎯 QLORA DEMO\n    Demonstrates QLoRA adaptation on our Qwen3 model.\n    \"\"\"\n    print(\"\\n🎯 QLORA (QUANTIZED LORA) DEMO\")\n    print(\"=\" * 50)\n    # Create model\n    config = SmallModelConfig()\n    config.d_model = 128",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "class MinimalLLM(nn.Module):\n    \"\"\"\n    🏗️ COMPLETE QWEN3-STYLE LANGUAGE MODEL\n    This is the full language model that combines all components:\n    🧠 Architecture:\n    1. Token Embedding: Convert token IDs to vectors\n    2. Positional Dropout: Prevent overfitting on positions\n    3. Transformer Blocks: Stack of attention + feed-forward layers\n    4. Final Normalization: RMSNorm before output\n    5. Language Modeling Head: Convert vectors back to token probabilities",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def evaluate_model(model: nn.Module, val_loader: DataLoader, config: SmallModelConfig):\n    \"\"\"\n    📊 MODEL EVALUATION FUNCTION\n    This function evaluates the model's performance on validation data:\n    🎯 Metrics Computed:\n    1. Loss: Cross-entropy loss (lower is better)\n    2. Accuracy: Percentage of correct next-token predictions\n    3. Perplexity: exp(loss) - measures model's \"surprise\" (lower is better)\n    🔍 Why these metrics matter:\n    - Loss: Direct measure of how well the model predicts",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "setup_muon_optimizer",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def setup_muon_optimizer(model: nn.Module, config: SmallModelConfig):\n    \"\"\"\n    🚀 HYBRID OPTIMIZER SETUP\n    This function sets up a hybrid optimization strategy:\n    - Muon optimizer for 2D parameters (attention and feed-forward weights)\n    - AdamW optimizer for other parameters (embeddings, norms, biases)\n    🎯 Why hybrid approach:\n    - Muon works best on 2D matrices (attention, feed-forward)\n    - AdamW is better for 1D parameters (embeddings, biases)\n    - This gives us the best of both worlds",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def load_checkpoint(model_path: str, config: SmallModelConfig):\n    \"\"\"\n    📦 LOAD CHECKPOINT FOR RESUMING TRAINING\n    This function loads a previously trained model checkpoint and returns\n    the model, optimizers, schedulers, and training state.\n    \"\"\"\n    print(f\"📦 Loading checkpoint from {model_path}\")\n    # Load checkpoint with safe loading to handle import path changes\n    try:\n        # Try loading with weights_only=False to handle custom classes",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "resume_training",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def resume_training(config: SmallModelConfig, train_loader: DataLoader, val_loader: DataLoader, checkpoint_path: str):\n    \"\"\"\n    🔄 RESUME TRAINING FROM CHECKPOINT\n    This function resumes training from a previously saved checkpoint.\n    It loads the model state and continues training from where it left off.\n    Args:\n        config: Model configuration\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        checkpoint_path: Path to the checkpoint file",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def train_model(config: SmallModelConfig, train_loader: DataLoader, val_loader: DataLoader):\n    \"\"\"\n    🔄 COMPLETE TRAINING LOOP\n    This is the heart of the training process, implementing:\n    🎯 Key Features:\n    1. Gradient Accumulation: Simulate larger batch sizes\n    2. Mixed Precision: Faster training with minimal accuracy loss\n    3. Learning Rate Scheduling: Warmup + cosine decay\n    4. Gradient Clipping: Prevent gradient explosions\n    5. Model Checkpointing: Save best model",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def generate_text(model: nn.Module, tokenizer, prompt: str, max_length: int = 100,\n                 temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9):\n    \"\"\"\n    🔮 TEXT GENERATION FUNCTION\n    This function generates text using the trained model with advanced sampling:\n    🎯 Sampling Strategies:\n    1. Temperature Scaling: Controls randomness (0.1 = focused, 2.0 = random)\n    2. Top-k Sampling: Only consider top k most likely tokens\n    3. Top-p (Nucleus) Sampling: Consider tokens until cumulative probability reaches p\n    🔍 How it works:",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class Rotary(nn.Module):\n    \"\"\"\n    🔄 ROTARY POSITIONAL EMBEDDINGS (RoPE)\n    This is one of the most important innovations in modern transformers!\n    🎯 What RoPE does:\n    - Encodes position information by ROTATING vectors\n    - Unlike traditional positional embeddings that just ADD position info\n    - Allows the model to understand relative positions naturally\n    🧮 The Math:\n    - For each position i, we compute rotation angles based on i",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class Qwen3Attention(nn.Module):\n    \"\"\"\n    🎯 GROUPED-QUERY ATTENTION (GQA) IMPLEMENTATION\n    This is the heart of modern transformer attention mechanisms!\n    🧠 Key Innovations:\n    1. Grouped-Query Attention: Fewer KV heads than Query heads\n    2. QK-Normalization: Normalizes queries and keys for stability\n    3. RoPE: Rotary positional embeddings\n    4. Scaled dot-product attention: The core attention mechanism\n    🔍 How it works:",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class SwiGLUFeedForward(nn.Module):\n    \"\"\"\n    🔥 SWIGLU FEED-FORWARD NETWORK\n    SwiGLU is a modern activation function that combines:\n    - Swish activation: x * sigmoid(x) (smooth, non-monotonic)\n    - GLU (Gated Linear Unit): element-wise multiplication with a gate\n    🧮 The Math:\n    SwiGLU(x) = Swish(W1(x)) ⊙ W2(x)\n    Where:\n    - W1(x) is the \"gate\" (controls information flow)",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class TransformerBlock(nn.Module):\n    \"\"\"\n    🏗️ TRANSFORMER BLOCK - THE BUILDING BLOCK OF MODERN LLMs\n    This combines all the components into a complete transformer layer:\n    🧠 Architecture:\n    1. Pre-norm attention (RMSNorm before attention)\n    2. Residual connection (x + attention(x))\n    3. Pre-norm feed-forward (RMSNorm before SwiGLU)\n    4. Residual connection (x + feedforward(x))\n    🔍 Pre-norm vs Post-norm:",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class RMSNorm(nn.Module):\n    \"\"\"\n    📐 RMSNorm - ROOT MEAN SQUARE NORMALIZATION\n    This is a modern alternative to LayerNorm that's more efficient:\n    🧮 The Math:\n    RMSNorm(x) = x / sqrt(mean(x²) + ε) * g\n    Where:\n    - x is the input\n    - mean(x²) is the mean of squared values\n    - ε is a small constant (1e-6)",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    🔑 GROUPED-QUERY ATTENTION HELPER\n    This implements the key innovation in GQA:\n    - Fewer Key-Value heads than Query heads\n    - Each KV head is \"shared\" across multiple Query heads\n    - Massive memory savings with minimal performance loss\n    🧮 Example:\n    - 8 Query heads, 2 KV heads\n    - KV head 1 is used by Query heads 1,2,3,4",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "QuantizedModelServer",
        "kind": 6,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "class QuantizedModelServer:\n    \"\"\"\n    🎯 QUANTIZED MODEL SERVER\n    Serves different types of quantized models for inference.\n    \"\"\"\n    def __init__(self, model_path: str, model_type: str = \"lora\"):\n        self.model_path = model_path\n        self.model_type = model_type\n        self.model = None\n        self.tokenizer = None",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "health_check",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return jsonify({'status': 'healthy', 'model_type': server.model_type})\n@app.route('/info', methods=['GET'])\ndef model_info():\n    \"\"\"Get model information\"\"\"\n    return jsonify(server.get_model_info())\n@app.route('/generate', methods=['POST'])\ndef generate():\n    \"\"\"Generate text endpoint\"\"\"",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "model_info",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def model_info():\n    \"\"\"Get model information\"\"\"\n    return jsonify(server.get_model_info())\n@app.route('/generate', methods=['POST'])\ndef generate():\n    \"\"\"Generate text endpoint\"\"\"\n    try:\n        data = request.get_json()\n        prompt = data.get('prompt', '')\n        max_length = data.get('max_length', 100)",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "generate",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def generate():\n    \"\"\"Generate text endpoint\"\"\"\n    try:\n        data = request.get_json()\n        prompt = data.get('prompt', '')\n        max_length = data.get('max_length', 100)\n        temperature = data.get('temperature', 0.8)\n        top_k = data.get('top_k', 50)\n        top_p = data.get('top_p', 0.9)\n        if not prompt:",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "classify",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def classify():\n    \"\"\"Classify sentiment endpoint\"\"\"\n    try:\n        data = request.get_json()\n        text = data.get('text', '')\n        if not text:\n            return jsonify({'error': 'Text is required'}), 400\n        start_time = time.time()\n        result = server.classify_sentiment(text)\n        classification_time = time.time() - start_time",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "benchmark",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def benchmark():\n    \"\"\"Benchmark model performance\"\"\"\n    try:\n        data = request.get_json()\n        num_samples = data.get('num_samples', 10)\n        max_length = data.get('max_length', 50)\n        # Benchmark text generation\n        prompts = [\n            \"The weather today is\",\n            \"I think that\",",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def main():\n    \"\"\"Main function to start the server\"\"\"\n    global server\n    parser = argparse.ArgumentParser(description='Serve quantized models')\n    parser.add_argument('--model_path', required=True, help='Path to the quantized model')\n    parser.add_argument('--model_type', choices=['lora', 'qlora', 'quantized'], \n                       default='lora', help='Type of quantized model')\n    parser.add_argument('--host', default='0.0.0.0', help='Host to bind to')\n    parser.add_argument('--port', type=int, default=5000, help='Port to bind to')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "app = Flask(__name__)\nserver = None\n@app.route('/health', methods=['GET'])\ndef health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return jsonify({'status': 'healthy', 'model_type': server.model_type})\n@app.route('/info', methods=['GET'])\ndef model_info():\n    \"\"\"Get model information\"\"\"\n    return jsonify(server.get_model_info())",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "server = None\n@app.route('/health', methods=['GET'])\ndef health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return jsonify({'status': 'healthy', 'model_type': server.model_type})\n@app.route('/info', methods=['GET'])\ndef model_info():\n    \"\"\"Get model information\"\"\"\n    return jsonify(server.get_model_info())\n@app.route('/generate', methods=['POST'])",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n    📦 LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"\n    global model, tokenizer, config\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file {model_path} not found!\")\n    print(f\"📦 Loading model from {model_path}\")\n    # Load checkpoint",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "health_check",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def health_check():\n    \"\"\"\n    🏥 HEALTH CHECK ENDPOINT\n    Returns the health status of the model server.\n    \"\"\"\n    return jsonify({\n        'status': 'healthy',\n        'model_loaded': model is not None,\n        'device': str(next(model.parameters()).device) if model else None\n    })",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "generate",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def generate():\n    \"\"\"\n    🔮 TEXT GENERATION ENDPOINT\n    Generates text based on the provided prompt.\n    Expected JSON payload:\n    {\n        \"prompt\": \"Your text prompt here\",\n        \"max_length\": 100,\n        \"temperature\": 0.8,\n        \"top_k\": 50,",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "model_info",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def model_info():\n    \"\"\"\n    📊 MODEL INFORMATION ENDPOINT\n    Returns information about the loaded model.\n    \"\"\"\n    if model is None:\n        return jsonify({'error': 'Model not loaded'}), 500\n    return jsonify({\n        'model_type': 'Qwen3-style Language Model',\n        'parameters': sum(p.numel() for p in model.parameters()),",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "home",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def home():\n    \"\"\"\n    🏠 HOME ENDPOINT\n    Returns a simple HTML interface for testing the model.\n    \"\"\"\n    return '''\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <title>Qwen3 Model Server</title>",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def main():\n    \"\"\"\n    🚀 MAIN SERVER FUNCTION\n    Starts the Flask server to serve the Qwen3 model.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser(description='Serve Qwen3 model')\n    parser.add_argument('--model', default='models/final_model1.pt', help='Path to model file')\n    parser.add_argument('--host', default='0.0.0.0', help='Host to bind to')\n    parser.add_argument('--port', type=int, default=5003, help='Port to bind to')",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "app = Flask(__name__)\n# Global variables for model and tokenizer\nmodel = None\ntokenizer = None\nconfig = None\ndef load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n    📦 LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "model = None\ntokenizer = None\nconfig = None\ndef load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n    📦 LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"\n    global model, tokenizer, config\n    if not os.path.exists(model_path):",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "tokenizer = None\nconfig = None\ndef load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n    📦 LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"\n    global model, tokenizer, config\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file {model_path} not found!\")",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "config = None\ndef load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n    📦 LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"\n    global model, tokenizer, config\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file {model_path} not found!\")\n    print(f\"📦 Loading model from {model_path}\")",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.train_qwen3",
        "description": "qwen-llm.train_qwen3",
        "peekOfCode": "def main(resume_from: str = None):\n    \"\"\"\n    🎯 MAIN TRAINING FUNCTION\n    This function orchestrates the entire training process:\n    1. System check and configuration\n    2. Data loading and preparation\n    3. Model training (from scratch or resume)\n    4. Results reporting\n    Args:\n        resume_from: Path to checkpoint to resume training from",
        "detail": "qwen-llm.train_qwen3",
        "documentation": {}
    },
    {
        "label": "demo_inference",
        "kind": 2,
        "importPath": "qwen-llm.train_qwen3",
        "description": "qwen-llm.train_qwen3",
        "peekOfCode": "def demo_inference(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n    🎭 DEMO INFERENCE FUNCTION\n    This function demonstrates the trained model's capabilities\n    \"\"\"\n    print(\"🎭 Running inference demo\")\n    # Load model\n    if not os.path.exists(model_path):\n        print(f\"❌ Model file {model_path} not found!\")\n        print(\"💡 Please run training first with: python train_qwen3.py\")",
        "detail": "qwen-llm.train_qwen3",
        "documentation": {}
    },
    {
        "label": "interactive_inference",
        "kind": 2,
        "importPath": "qwen-llm.train_qwen3",
        "description": "qwen-llm.train_qwen3",
        "peekOfCode": "def interactive_inference(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n    🤖 INTERACTIVE INFERENCE SESSION\n    This function allows you to interact with the trained model\n    \"\"\"\n    print(\"🤖 Starting interactive inference session\")\n    print(\"Type 'quit' to exit\")\n    # Load model\n    if not os.path.exists(model_path):\n        print(f\"❌ Model file {model_path} not found!\")",
        "detail": "qwen-llm.train_qwen3",
        "documentation": {}
    },
    {
        "label": "convert_checkpoint",
        "kind": 2,
        "importPath": "convert_checkpoint",
        "description": "convert_checkpoint",
        "peekOfCode": "def convert_checkpoint(old_path: str, new_path: str):\n    \"\"\"\n    Convert old checkpoint to new format\n    \"\"\"\n    print(f\"🔄 Converting checkpoint from {old_path} to {new_path}\")\n    try:\n        # Try to load the old checkpoint\n        print(\"📦 Loading old checkpoint...\")\n        checkpoint = torch.load(old_path, map_location='cpu', weights_only=False)\n        # Extract only the essential data",
        "detail": "convert_checkpoint",
        "documentation": {}
    }
]