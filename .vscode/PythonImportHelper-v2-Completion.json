[
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "torch.utils.data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "BlockMask",
        "importPath": "torch.nn.attention.flex_attention",
        "description": "torch.nn.attention.flex_attention",
        "isExtraImport": true,
        "detail": "torch.nn.attention.flex_attention",
        "documentation": {}
    },
    {
        "label": "create_block_mask",
        "importPath": "torch.nn.attention.flex_attention",
        "description": "torch.nn.attention.flex_attention",
        "isExtraImport": true,
        "detail": "torch.nn.attention.flex_attention",
        "documentation": {}
    },
    {
        "label": "BlockMask",
        "importPath": "torch.nn.attention.flex_attention",
        "description": "torch.nn.attention.flex_attention",
        "isExtraImport": true,
        "detail": "torch.nn.attention.flex_attention",
        "documentation": {}
    },
    {
        "label": "create_block_mask",
        "importPath": "torch.nn.attention.flex_attention",
        "description": "torch.nn.attention.flex_attention",
        "isExtraImport": true,
        "detail": "torch.nn.attention.flex_attention",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "asynccontextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference",
        "description": "fast_inference",
        "isExtraImport": true,
        "detail": "fast_inference",
        "documentation": {}
    },
    {
        "label": "BenchmarkRunner",
        "importPath": "fast_inference.utils.benchmarking",
        "description": "fast_inference.utils.benchmarking",
        "isExtraImport": true,
        "detail": "fast_inference.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "BenchmarkRunner",
        "importPath": "fast_inference.utils.benchmarking",
        "description": "fast_inference.utils.benchmarking",
        "isExtraImport": true,
        "detail": "fast_inference.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "generate_test_prompts",
        "importPath": "fast_inference.utils.benchmarking",
        "description": "fast_inference.utils.benchmarking",
        "isExtraImport": true,
        "detail": "fast_inference.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "create_universal_fast_inference",
        "importPath": "fast_inference.core.engine.universal_engine",
        "description": "fast_inference.core.engine.universal_engine",
        "isExtraImport": true,
        "detail": "fast_inference.core.engine.universal_engine",
        "documentation": {}
    },
    {
        "label": "create_universal_vllm_style_engine",
        "importPath": "fast_inference.core.engine.universal_vllm_engine",
        "description": "fast_inference.core.engine.universal_vllm_engine",
        "isExtraImport": true,
        "detail": "fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "SchedulerPolicy",
        "importPath": "fast_inference.core.engine.universal_vllm_engine",
        "description": "fast_inference.core.engine.universal_vllm_engine",
        "isExtraImport": true,
        "detail": "fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "create_vllm_style_engine",
        "importPath": "fast_inference.core.engine.vllm_style_engine",
        "description": "fast_inference.core.engine.vllm_style_engine",
        "isExtraImport": true,
        "detail": "fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "SchedulerPolicy",
        "importPath": "fast_inference.core.engine.vllm_style_engine",
        "description": "fast_inference.core.engine.vllm_style_engine",
        "isExtraImport": true,
        "detail": "fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "Mock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "Mock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "fast_inference.core.engine",
        "description": "fast_inference.core.engine",
        "isExtraImport": true,
        "detail": "fast_inference.core.engine",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "sample_tokens",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "apply_repetition_penalty",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "apply_top_k_filtering",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "apply_top_p_filtering",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "sample_greedy",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "sample_random",
        "importPath": "fast_inference.utils.sampling",
        "description": "fast_inference.utils.sampling",
        "isExtraImport": true,
        "detail": "fast_inference.utils.sampling",
        "documentation": {}
    },
    {
        "label": "SimpleKVCache",
        "importPath": "fast_inference.core.cache",
        "description": "fast_inference.core.cache",
        "isExtraImport": true,
        "detail": "fast_inference.core.cache",
        "documentation": {}
    },
    {
        "label": "PagedKVCache",
        "importPath": "fast_inference.core.cache",
        "description": "fast_inference.core.cache",
        "isExtraImport": true,
        "detail": "fast_inference.core.cache",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "LoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "QLoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "LoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "QLoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "LoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "QLoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "LoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "QLoRATrainingConfig",
        "importPath": "lora_qlora.core.training.config",
        "description": "lora_qlora.core.training.config",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "LoRATrainer",
        "importPath": "lora_qlora.core.training.trainer",
        "description": "lora_qlora.core.training.trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "QLoRATrainer",
        "importPath": "lora_qlora.core.training.trainer",
        "description": "lora_qlora.core.training.trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "LoRATrainer",
        "importPath": "lora_qlora.core.training.trainer",
        "description": "lora_qlora.core.training.trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "QLoRATrainer",
        "importPath": "lora_qlora.core.training.trainer",
        "description": "lora_qlora.core.training.trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "LoRATrainer",
        "importPath": "lora_qlora.core.training.trainer",
        "description": "lora_qlora.core.training.trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "QLoRATrainer",
        "importPath": "lora_qlora.core.training.trainer",
        "description": "lora_qlora.core.training.trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "load_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "split_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "balance_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "load_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "split_data",
        "importPath": "lora_qlora.utils.data",
        "description": "lora_qlora.utils.data",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.data",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "lora_qlora.utils.config",
        "description": "lora_qlora.utils.config",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.config",
        "documentation": {}
    },
    {
        "label": "save_config",
        "importPath": "lora_qlora.utils.config",
        "description": "lora_qlora.utils.config",
        "isExtraImport": true,
        "detail": "lora_qlora.utils.config",
        "documentation": {}
    },
    {
        "label": "UniversalLoRATrainer",
        "importPath": "lora_qlora.core.training.universal_trainer",
        "description": "lora_qlora.core.training.universal_trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "fine_tune_huggingface_model",
        "importPath": "lora_qlora.core.training.universal_trainer",
        "description": "lora_qlora.core.training.universal_trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "fine_tune_custom_model",
        "importPath": "lora_qlora.core.training.universal_trainer",
        "description": "lora_qlora.core.training.universal_trainer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "unittest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unittest",
        "description": "unittest",
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "LoRADataset",
        "importPath": "lora_qlora.core.training.dataset",
        "description": "lora_qlora.core.training.dataset",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.dataset",
        "documentation": {}
    },
    {
        "label": "QLoRADataset",
        "importPath": "lora_qlora.core.training.dataset",
        "description": "lora_qlora.core.training.dataset",
        "isExtraImport": true,
        "detail": "lora_qlora.core.training.dataset",
        "documentation": {}
    },
    {
        "label": "LoRALayer",
        "importPath": "lora_qlora.core.lora.lora_layer",
        "description": "lora_qlora.core.lora.lora_layer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.lora.lora_layer",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "importPath": "lora_qlora.core.lora.lora_linear",
        "description": "lora_qlora.core.lora.lora_linear",
        "isExtraImport": true,
        "detail": "lora_qlora.core.lora.lora_linear",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "importPath": "lora_qlora.core.lora.lora_manager",
        "description": "lora_qlora.core.lora.lora_manager",
        "isExtraImport": true,
        "detail": "lora_qlora.core.lora.lora_manager",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "lora_qlora.core.quantization.quantization_expert",
        "description": "lora_qlora.core.quantization.quantization_expert",
        "isExtraImport": true,
        "detail": "lora_qlora.core.quantization.quantization_expert",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "lora_qlora.core.quantization.quantization_expert",
        "description": "lora_qlora.core.quantization.quantization_expert",
        "isExtraImport": true,
        "detail": "lora_qlora.core.quantization.quantization_expert",
        "documentation": {}
    },
    {
        "label": "QLoRALayer",
        "importPath": "lora_qlora.core.qlora.qlora_layer",
        "description": "lora_qlora.core.qlora.qlora_layer",
        "isExtraImport": true,
        "detail": "lora_qlora.core.qlora.qlora_layer",
        "documentation": {}
    },
    {
        "label": "QLoRALinear",
        "importPath": "lora_qlora.core.qlora.qlora_linear",
        "description": "lora_qlora.core.qlora.qlora_linear",
        "isExtraImport": true,
        "detail": "lora_qlora.core.qlora.qlora_linear",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "importPath": "lora_qlora.core.qlora.qlora_manager",
        "description": "lora_qlora.core.qlora.qlora_manager",
        "isExtraImport": true,
        "detail": "lora_qlora.core.qlora.qlora_manager",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "importPath": "simple_fast_inference",
        "description": "simple_fast_inference",
        "isExtraImport": true,
        "detail": "simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "importPath": "simple_fast_inference",
        "description": "simple_fast_inference",
        "isExtraImport": true,
        "detail": "simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "PretrainingConfig",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "PretrainingTrainer",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "PretrainingConfig",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "PretrainingTrainer",
        "importPath": "pretraining",
        "description": "pretraining",
        "isExtraImport": true,
        "detail": "pretraining",
        "documentation": {}
    },
    {
        "label": "triton",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "triton",
        "description": "triton",
        "detail": "triton",
        "documentation": {}
    },
    {
        "label": "triton.language",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "triton.language",
        "description": "triton.language",
        "detail": "triton.language",
        "documentation": {}
    },
    {
        "label": "vector_add",
        "importPath": "lessons.beginner.lesson_01_gpu_fundamentals",
        "description": "lessons.beginner.lesson_01_gpu_fundamentals",
        "isExtraImport": true,
        "detail": "lessons.beginner.lesson_01_gpu_fundamentals",
        "documentation": {}
    },
    {
        "label": "vector_add",
        "importPath": "lessons.beginner.lesson_01_gpu_fundamentals",
        "description": "lessons.beginner.lesson_01_gpu_fundamentals",
        "isExtraImport": true,
        "detail": "lessons.beginner.lesson_01_gpu_fundamentals",
        "documentation": {}
    },
    {
        "label": "coalesced_access_kernel",
        "importPath": "lessons.beginner.lesson_02_memory_management",
        "description": "lessons.beginner.lesson_02_memory_management",
        "isExtraImport": true,
        "detail": "lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "non_coalesced_access_kernel",
        "importPath": "lessons.beginner.lesson_02_memory_management",
        "description": "lessons.beginner.lesson_02_memory_management",
        "isExtraImport": true,
        "detail": "lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "coalesced_access_kernel",
        "importPath": "lessons.beginner.lesson_02_memory_management",
        "description": "lessons.beginner.lesson_02_memory_management",
        "isExtraImport": true,
        "detail": "lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "non_coalesced_access_kernel",
        "importPath": "lessons.beginner.lesson_02_memory_management",
        "description": "lessons.beginner.lesson_02_memory_management",
        "isExtraImport": true,
        "detail": "lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "element_wise_add_kernel",
        "importPath": "lessons.beginner.lesson_03_basic_operations",
        "description": "lessons.beginner.lesson_03_basic_operations",
        "isExtraImport": true,
        "detail": "lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "sum_reduction_kernel",
        "importPath": "lessons.beginner.lesson_03_basic_operations",
        "description": "lessons.beginner.lesson_03_basic_operations",
        "isExtraImport": true,
        "detail": "lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "element_wise_add_kernel",
        "importPath": "lessons.beginner.lesson_03_basic_operations",
        "description": "lessons.beginner.lesson_03_basic_operations",
        "isExtraImport": true,
        "detail": "lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "sum_reduction_kernel",
        "importPath": "lessons.beginner.lesson_03_basic_operations",
        "description": "lessons.beginner.lesson_03_basic_operations",
        "isExtraImport": true,
        "detail": "lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "optimized_attention",
        "importPath": "examples.llm_inference_optimization",
        "description": "examples.llm_inference_optimization",
        "isExtraImport": true,
        "detail": "examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "transformer_matmul",
        "importPath": "examples.llm_inference_optimization",
        "description": "examples.llm_inference_optimization",
        "isExtraImport": true,
        "detail": "examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "layer_norm",
        "importPath": "examples.llm_inference_optimization",
        "description": "examples.llm_inference_optimization",
        "isExtraImport": true,
        "detail": "examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "optimized_attention",
        "importPath": "examples.llm_inference_optimization",
        "description": "examples.llm_inference_optimization",
        "isExtraImport": true,
        "detail": "examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "transformer_matmul",
        "importPath": "examples.llm_inference_optimization",
        "description": "examples.llm_inference_optimization",
        "isExtraImport": true,
        "detail": "examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "layer_norm",
        "importPath": "examples.llm_inference_optimization",
        "description": "examples.llm_inference_optimization",
        "isExtraImport": true,
        "detail": "examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "basic_matmul",
        "importPath": "lessons.intermediate.lesson_04_matrix_operations",
        "description": "lessons.intermediate.lesson_04_matrix_operations",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "optimized_matmul",
        "importPath": "lessons.intermediate.lesson_04_matrix_operations",
        "description": "lessons.intermediate.lesson_04_matrix_operations",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "batch_matmul",
        "importPath": "lessons.intermediate.lesson_04_matrix_operations",
        "description": "lessons.intermediate.lesson_04_matrix_operations",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "matrix_transpose",
        "importPath": "lessons.intermediate.lesson_04_matrix_operations",
        "description": "lessons.intermediate.lesson_04_matrix_operations",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "basic_matmul",
        "importPath": "lessons.intermediate.lesson_04_matrix_operations",
        "description": "lessons.intermediate.lesson_04_matrix_operations",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "optimized_matmul",
        "importPath": "lessons.intermediate.lesson_04_matrix_operations",
        "description": "lessons.intermediate.lesson_04_matrix_operations",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "batch_matmul",
        "importPath": "lessons.intermediate.lesson_04_matrix_operations",
        "description": "lessons.intermediate.lesson_04_matrix_operations",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "matrix_transpose",
        "importPath": "lessons.intermediate.lesson_04_matrix_operations",
        "description": "lessons.intermediate.lesson_04_matrix_operations",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "shared_memory_matmul",
        "importPath": "lessons.intermediate.lesson_05_advanced_memory",
        "description": "lessons.intermediate.lesson_05_advanced_memory",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "cache_friendly_reduction",
        "importPath": "lessons.intermediate.lesson_05_advanced_memory",
        "description": "lessons.intermediate.lesson_05_advanced_memory",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "shared_memory_matmul",
        "importPath": "lessons.intermediate.lesson_05_advanced_memory",
        "description": "lessons.intermediate.lesson_05_advanced_memory",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "cache_friendly_reduction",
        "importPath": "lessons.intermediate.lesson_05_advanced_memory",
        "description": "lessons.intermediate.lesson_05_advanced_memory",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "fused_add_multiply",
        "importPath": "lessons.intermediate.lesson_06_kernel_fusion",
        "description": "lessons.intermediate.lesson_06_kernel_fusion",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "fused_matmul_activation",
        "importPath": "lessons.intermediate.lesson_06_kernel_fusion",
        "description": "lessons.intermediate.lesson_06_kernel_fusion",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "fused_loop",
        "importPath": "lessons.intermediate.lesson_06_kernel_fusion",
        "description": "lessons.intermediate.lesson_06_kernel_fusion",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "fused_add_multiply",
        "importPath": "lessons.intermediate.lesson_06_kernel_fusion",
        "description": "lessons.intermediate.lesson_06_kernel_fusion",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "fused_matmul_activation",
        "importPath": "lessons.intermediate.lesson_06_kernel_fusion",
        "description": "lessons.intermediate.lesson_06_kernel_fusion",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "fused_loop",
        "importPath": "lessons.intermediate.lesson_06_kernel_fusion",
        "description": "lessons.intermediate.lesson_06_kernel_fusion",
        "isExtraImport": true,
        "detail": "lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "BenchmarkSuite",
        "importPath": "utils.benchmarking",
        "description": "utils.benchmarking",
        "isExtraImport": true,
        "detail": "utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "BenchmarkSuite",
        "importPath": "utils.benchmarking",
        "description": "utils.benchmarking",
        "isExtraImport": true,
        "detail": "utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "BenchmarkResult",
        "importPath": "utils.benchmarking",
        "description": "utils.benchmarking",
        "isExtraImport": true,
        "detail": "utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "PerformanceProfiler",
        "importPath": "utils.profiling",
        "description": "utils.profiling",
        "isExtraImport": true,
        "detail": "utils.profiling",
        "documentation": {}
    },
    {
        "label": "PerformanceProfiler",
        "importPath": "utils.profiling",
        "description": "utils.profiling",
        "isExtraImport": true,
        "detail": "utils.profiling",
        "documentation": {}
    },
    {
        "label": "ProfileResult",
        "importPath": "utils.profiling",
        "description": "utils.profiling",
        "isExtraImport": true,
        "detail": "utils.profiling",
        "documentation": {}
    },
    {
        "label": "ValidationSuite",
        "importPath": "utils.validation",
        "description": "utils.validation",
        "isExtraImport": true,
        "detail": "utils.validation",
        "documentation": {}
    },
    {
        "label": "ValidationSuite",
        "importPath": "utils.validation",
        "description": "utils.validation",
        "isExtraImport": true,
        "detail": "utils.validation",
        "documentation": {}
    },
    {
        "label": "ValidationResult",
        "importPath": "utils.validation",
        "description": "utils.validation",
        "isExtraImport": true,
        "detail": "utils.validation",
        "documentation": {}
    },
    {
        "label": "DataGenerator",
        "importPath": "utils.data_generation",
        "description": "utils.data_generation",
        "isExtraImport": true,
        "detail": "utils.data_generation",
        "documentation": {}
    },
    {
        "label": "DataConfig",
        "importPath": "utils.data_generation",
        "description": "utils.data_generation",
        "isExtraImport": true,
        "detail": "utils.data_generation",
        "documentation": {}
    },
    {
        "label": "DataGenerator",
        "importPath": "utils.data_generation",
        "description": "utils.data_generation",
        "isExtraImport": true,
        "detail": "utils.data_generation",
        "documentation": {}
    },
    {
        "label": "DataConfig",
        "importPath": "utils.data_generation",
        "description": "utils.data_generation",
        "isExtraImport": true,
        "detail": "utils.data_generation",
        "documentation": {}
    },
    {
        "label": "PerformanceAnalyzer",
        "importPath": "utils.performance_analysis",
        "description": "utils.performance_analysis",
        "isExtraImport": true,
        "detail": "utils.performance_analysis",
        "documentation": {}
    },
    {
        "label": "PerformanceAnalyzer",
        "importPath": "utils.performance_analysis",
        "description": "utils.performance_analysis",
        "isExtraImport": true,
        "detail": "utils.performance_analysis",
        "documentation": {}
    },
    {
        "label": "PerformanceMetrics",
        "importPath": "utils.performance_analysis",
        "description": "utils.performance_analysis",
        "isExtraImport": true,
        "detail": "utils.performance_analysis",
        "documentation": {}
    },
    {
        "label": "BenchmarkSuite",
        "importPath": "triton_tutorials.utils.benchmarking",
        "description": "triton_tutorials.utils.benchmarking",
        "isExtraImport": true,
        "detail": "triton_tutorials.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "PerformanceProfiler",
        "importPath": "triton_tutorials.utils.profiling",
        "description": "triton_tutorials.utils.profiling",
        "isExtraImport": true,
        "detail": "triton_tutorials.utils.profiling",
        "documentation": {}
    },
    {
        "label": "PerformanceAnalyzer",
        "importPath": "triton_tutorials.utils.performance_analysis",
        "description": "triton_tutorials.utils.performance_analysis",
        "isExtraImport": true,
        "detail": "triton_tutorials.utils.performance_analysis",
        "documentation": {}
    },
    {
        "label": "ctypes",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ctypes",
        "description": "ctypes",
        "detail": "ctypes",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "psutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil",
        "description": "psutil",
        "detail": "psutil",
        "documentation": {}
    },
    {
        "label": "GPUtil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "GPUtil",
        "description": "GPUtil",
        "detail": "GPUtil",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "kind": 6,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "class SmallModelConfig:\n    \"\"\"\n    🎯 SMALL CONFIGURATION FOR FAST TRAINING\n    This configuration is optimized for:\n    - Fast training on CPU/limited GPU\n    - Learning the concepts without waiting hours\n    - Still demonstrating all key components\n    \"\"\"\n    # Model architecture - MUCH SMALLER\n    d_model: int = 128          # Reduced from 384 (3x smaller)",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "Muon",
        "kind": 6,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "class Muon(torch.optim.Optimizer):\n    \"\"\"\n    🚀 MUON OPTIMIZER: MomentUm Orthogonalized by Newton-schulz\n    This is a revolutionary optimizer that combines:\n    1. Momentum (like Adam) - remembers past gradients\n    2. Orthogonalization (Newton-Schulz) - makes gradients \"well-behaved\"\n    3. Adaptive learning rates - adjusts based on matrix dimensions\n    🎯 Why Muon is special:\n    - 30-50% faster convergence than Adam\n    - More stable training (fewer gradient explosions)",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "kind": 6,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "class TextTokenDataset(Dataset):\n    \"\"\"\n    📚 CUSTOM DATASET FOR LANGUAGE MODELING\n    This creates training examples for our language model:\n    🎯 What it does:\n    - Takes a long sequence of tokens\n    - Creates sliding windows of fixed length\n    - Each example: input sequence + target sequence (shifted by 1)\n    📖 Example:\n    Original text: \"The cat sat on the mat\"",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def set_seed(seed: int = 42):\n    \"\"\"Set all random seeds for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f\"🌱 Set all seeds to {seed}\")\n@dataclass",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    🔑 KEY COMPONENT: Grouped-Query Attention (GQA)\n    GQA is a memory-efficient attention mechanism where:\n    - We have fewer Key-Value heads than Query heads\n    - Each KV head is \"repeated\" to match the number of Query heads\n    - This reduces memory usage while maintaining performance\n    Example:\n    - 4 Query heads, 2 KV heads → each KV head repeated 2 times\n    - Memory savings: 50% reduction in KV cache memory",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "zeropower_via_newtonschulz5",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n    \"\"\"\n    🔬 NEWTON-SCHULZ ORTHOGONALIZATION\n    This is the mathematical heart of the Muon optimizer:\n    🎯 What it does:\n    - Takes a matrix G (gradients)\n    - Makes it \"orthogonal\" (like rotating it to be perfectly aligned)\n    - Uses Newton-Schulz iteration (a fast numerical method)\n    🧮 Why orthogonalization helps:\n    - Orthogonal matrices preserve vector lengths and angles",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def load_and_cache_data(config: SmallModelConfig, cache_dir: str = \"data_cache\"):\n    \"\"\"\n    📦 SMART DATA LOADING WITH CACHING\n    This function demonstrates modern ML data handling:\n    🎯 Key features:\n    1. Caching: Avoids reprocessing the same data\n    2. Streaming: Loads large datasets without memory issues\n    3. Tokenization: Converts text to numbers the model can understand\n    4. Efficient storage: Uses pickle for fast loading\n    🔄 The process:",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "CachedAttention",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.attention.cached_attention",
        "description": "qwen-llm.fast_inference.core.attention.cached_attention",
        "peekOfCode": "class CachedAttention(nn.Module):\n    \"\"\"\n    🎯 CACHED ATTENTION\n    Attention layer with simple KV caching for fast inference.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'SimpleKVCache'):\n        \"\"\"\n        Initialize cached attention layer.\n        Args:\n            config: Model configuration",
        "detail": "qwen-llm.fast_inference.core.attention.cached_attention",
        "documentation": {}
    },
    {
        "label": "OptimizedAttention",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.attention.optimized_attention",
        "description": "qwen-llm.fast_inference.core.attention.optimized_attention",
        "peekOfCode": "class OptimizedAttention(nn.Module):\n    \"\"\"\n    🎯 OPTIMIZED ATTENTION WITH PAGED KV CACHE\n    Advanced attention layer with paged KV caching for maximum performance.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'PagedKVCache'):\n        \"\"\"\n        Initialize optimized attention layer.\n        Args:\n            config: Model configuration",
        "detail": "qwen-llm.fast_inference.core.attention.optimized_attention",
        "documentation": {}
    },
    {
        "label": "PagedKVCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.cache.paged_cache",
        "description": "qwen-llm.fast_inference.core.cache.paged_cache",
        "peekOfCode": "class PagedKVCache(nn.Module):\n    \"\"\"\n    🧠 PAGED KV CACHE\n    Efficient memory management for KV cache using page-based allocation.\n    Inspired by vLLM's PagedAttention but simplified for our use case.\n    \"\"\"\n    def __init__(self, n_pages: int, page_size: int, n_heads: int, head_dim: int, dtype: torch.dtype, device: str):\n        \"\"\"\n        Initialize paged KV cache.\n        Args:",
        "detail": "qwen-llm.fast_inference.core.cache.paged_cache",
        "documentation": {}
    },
    {
        "label": "SimpleKVCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.cache.simple_cache",
        "description": "qwen-llm.fast_inference.core.cache.simple_cache",
        "peekOfCode": "class SimpleKVCache:\n    \"\"\"\n    🎯 SIMPLE KV CACHE\n    A straightforward KV cache implementation that stores key-value pairs\n    for each sequence position. Much simpler than paged attention but still effective.\n    \"\"\"\n    def __init__(self, max_seq_len: int, n_heads: int, head_dim: int, dtype: torch.dtype, device: str):\n        \"\"\"\n        Initialize simple KV cache.\n        Args:",
        "detail": "qwen-llm.fast_inference.core.cache.simple_cache",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "description": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "peekOfCode": "class Sequence:\n    \"\"\"Represents a single generation sequence.\"\"\"\n    seq_id: int\n    prompt: str\n    input_ids: torch.Tensor\n    output_ids: List[int]\n    sampling_params: 'SamplingParams'\n    batch_idx: int = -1\n    finished: bool = False\n    prefill_done: bool = False",
        "detail": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "documentation": {}
    },
    {
        "label": "FastInferenceEngine",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "description": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "peekOfCode": "class FastInferenceEngine:\n    \"\"\"\n    🚀 FAST INFERENCE ENGINE\n    High-performance inference engine with:\n    - Paged KV cache for memory efficiency\n    - Continuous batching for throughput\n    - CUDA graphs for optimization\n    - Dynamic scheduling\n    \"\"\"\n    def __init__(self, model: nn.Module, tokenizer, config, ",
        "detail": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "documentation": {}
    },
    {
        "label": "OptimizedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "description": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "peekOfCode": "class OptimizedTransformerBlock(nn.Module):\n    \"\"\"\n    🏗️ OPTIMIZED TRANSFORMER BLOCK\n    Transformer block with optimized attention and KV caching.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'PagedKVCache'):\n        super().__init__()\n        self.attention = OptimizedAttention(config, kv_cache)\n        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n        # Pre-norm architecture",
        "detail": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "documentation": {}
    },
    {
        "label": "create_fast_inference_engine",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "description": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "peekOfCode": "def create_fast_inference_engine(model_path: str, tokenizer_path: str, \n                                max_batch_size: int = 32, max_seq_len: int = 2048,\n                                n_pages: int = 1000, page_size: int = 128) -> FastInferenceEngine:\n    \"\"\"\n    Create a fast inference engine from saved model.\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        max_batch_size: Maximum batch size\n        max_seq_len: Maximum sequence length",
        "detail": "qwen-llm.fast_inference.core.engine.advanced_engine",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.simple_engine",
        "description": "qwen-llm.fast_inference.core.engine.simple_engine",
        "peekOfCode": "class SimpleFastInference:\n    \"\"\"\n    🚀 SIMPLE FAST INFERENCE ENGINE\n    A simplified but fast inference engine that adds KV caching to your existing model.\n    Much easier to understand and integrate than full vLLM-style implementations.\n    \"\"\"\n    def __init__(self, model: nn.Module, tokenizer, config, max_seq_len: int = 2048):\n        \"\"\"\n        Initialize simple fast inference engine.\n        Args:",
        "detail": "qwen-llm.fast_inference.core.engine.simple_engine",
        "documentation": {}
    },
    {
        "label": "CachedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.simple_engine",
        "description": "qwen-llm.fast_inference.core.engine.simple_engine",
        "peekOfCode": "class CachedTransformerBlock(nn.Module):\n    \"\"\"\n    🏗️ CACHED TRANSFORMER BLOCK\n    Transformer block with cached attention.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'SimpleKVCache'):\n        super().__init__()\n        self.attention = CachedAttention(config, kv_cache)\n        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n        # Pre-norm architecture",
        "detail": "qwen-llm.fast_inference.core.engine.simple_engine",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.core.engine.simple_engine",
        "description": "qwen-llm.fast_inference.core.engine.simple_engine",
        "peekOfCode": "def create_simple_fast_inference(model_path: str, tokenizer_path: str, \n                                max_seq_len: int = 2048) -> SimpleFastInference:\n    \"\"\"\n    Create a simple fast inference engine from saved model.\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        max_seq_len: Maximum sequence length\n    Returns:\n        SimpleFastInference instance",
        "detail": "qwen-llm.fast_inference.core.engine.simple_engine",
        "documentation": {}
    },
    {
        "label": "UniversalFastInference",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_engine",
        "peekOfCode": "class UniversalFastInference:\n    \"\"\"\n    🌐 UNIVERSAL FAST INFERENCE ENGINE\n    A universal inference engine that can work with ANY model:\n    - HuggingFace models (BERT, RoBERTa, GPT, LLaMA, etc.)\n    - Custom models (like your MinimalLLM)\n    - Any PyTorch model with transformer architecture\n    Key Features:\n    - Automatic model detection\n    - Universal KV caching",
        "detail": "qwen-llm.fast_inference.core.engine.universal_engine",
        "documentation": {}
    },
    {
        "label": "CachedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_engine",
        "peekOfCode": "class CachedTransformerBlock(nn.Module):\n    \"\"\"\n    🏗️ CACHED TRANSFORMER BLOCK\n    Universal transformer block with cached attention.\n    \"\"\"\n    def __init__(self, config, kv_cache: 'SimpleKVCache'):\n        super().__init__()\n        from ..attention.cached_attention import CachedAttention\n        from pretraining.core.model.components import SwiGLUFeedForward, RMSNorm\n        self.attention = CachedAttention(config, kv_cache)",
        "detail": "qwen-llm.fast_inference.core.engine.universal_engine",
        "documentation": {}
    },
    {
        "label": "create_universal_fast_inference",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_engine",
        "peekOfCode": "def create_universal_fast_inference(model: Union[nn.Module, str], tokenizer: Union[Any, str], \n                                   max_seq_len: int = 2048, model_type: str = \"auto\") -> UniversalFastInference:\n    \"\"\"\n    Create a universal fast inference engine.\n    Args:\n        model: Model instance, model path, or HuggingFace model name\n        tokenizer: Tokenizer instance, tokenizer path, or HuggingFace tokenizer name\n        max_seq_len: Maximum sequence length\n        model_type: Model type (\"auto\", \"huggingface\", \"custom\", \"minimal_llm\")\n    Returns:",
        "detail": "qwen-llm.fast_inference.core.engine.universal_engine",
        "documentation": {}
    },
    {
        "label": "SchedulerPolicy",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "class SchedulerPolicy(Enum):\n    \"\"\"Scheduling policies for request handling.\"\"\"\n    FCFS = \"first_come_first_served\"  # First Come First Served\n    PRIORITY = \"priority\"             # Priority-based\n    MEMORY_AWARE = \"memory_aware\"     # Memory-aware scheduling\n@dataclass\nclass Block:\n    \"\"\"Represents a memory block in the cache.\"\"\"\n    block_id: int\n    ref_count: int = 0",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "class Block:\n    \"\"\"Represents a memory block in the cache.\"\"\"\n    block_id: int\n    ref_count: int = 0\n    is_allocated: bool = False\n    sequence_ids: List[int] = field(default_factory=list)\n@dataclass\nclass SequenceMetadata:\n    \"\"\"Metadata for a generation sequence.\"\"\"\n    seq_id: int",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "SequenceMetadata",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "class SequenceMetadata:\n    \"\"\"Metadata for a generation sequence.\"\"\"\n    seq_id: int\n    prompt: str\n    input_ids: torch.Tensor\n    output_ids: List[int] = field(default_factory=list)\n    sampling_params: Dict[str, Any] = field(default_factory=dict)\n    blocks: List[int] = field(default_factory=list)  # Allocated block IDs\n    finished: bool = False\n    prefill_done: bool = False",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "UniversalPagedAttentionCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "class UniversalPagedAttentionCache:\n    \"\"\"\n    🌐 UNIVERSAL PAGED ATTENTION CACHE\n    Universal PagedAttention implementation that works with any model:\n    - Automatic model detection\n    - Universal block-wise memory management\n    - Memory fragmentation handling\n    - Efficient block allocation/deallocation\n    \"\"\"\n    def __init__(self, num_blocks: int, block_size: int, model_info: Dict[str, Any], ",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "UniversalVLLMStyleEngine",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "class UniversalVLLMStyleEngine:\n    \"\"\"\n    🌐 UNIVERSAL VLLM-STYLE INFERENCE ENGINE\n    Combines universal model support with vLLM features:\n    - Works with ANY model (MinimalLLM, HuggingFace, custom)\n    - True PagedAttention with block-wise memory management\n    - Advanced scheduling policies\n    - Async API support\n    - Production-ready features\n    \"\"\"",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "create_universal_vllm_style_engine",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "def create_universal_vllm_style_engine(model: Union[nn.Module, str], tokenizer: Union[Any, str], \n                                      **kwargs) -> UniversalVLLMStyleEngine:\n    \"\"\"\n    Create a universal vLLM-style inference engine.\n    Args:\n        model: Model instance, model path, or HuggingFace model name\n        tokenizer: Tokenizer instance, tokenizer path, or HuggingFace tokenizer name\n        **kwargs: Additional arguments\n    Returns:\n        UniversalVLLMStyleEngine instance",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "description": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SchedulerPolicy(Enum):\n    \"\"\"Scheduling policies for request handling.\"\"\"\n    FCFS = \"first_come_first_served\"  # First Come First Served\n    PRIORITY = \"priority\"             # Priority-based\n    MEMORY_AWARE = \"memory_aware\"     # Memory-aware scheduling\n@dataclass\nclass Block:\n    \"\"\"Represents a memory block in the cache.\"\"\"\n    block_id: int",
        "detail": "qwen-llm.fast_inference.core.engine.universal_vllm_engine",
        "documentation": {}
    },
    {
        "label": "SchedulerPolicy",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "class SchedulerPolicy(Enum):\n    \"\"\"Scheduling policies for request handling.\"\"\"\n    FCFS = \"first_come_first_served\"  # First Come First Served\n    PRIORITY = \"priority\"             # Priority-based\n    MEMORY_AWARE = \"memory_aware\"     # Memory-aware scheduling\n@dataclass\nclass Block:\n    \"\"\"Represents a memory block in the cache.\"\"\"\n    block_id: int\n    ref_count: int = 0",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "class Block:\n    \"\"\"Represents a memory block in the cache.\"\"\"\n    block_id: int\n    ref_count: int = 0\n    is_allocated: bool = False\n    sequence_ids: List[int] = field(default_factory=list)\n@dataclass\nclass SequenceMetadata:\n    \"\"\"Metadata for a generation sequence.\"\"\"\n    seq_id: int",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "SequenceMetadata",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "class SequenceMetadata:\n    \"\"\"Metadata for a generation sequence.\"\"\"\n    seq_id: int\n    prompt: str\n    input_ids: torch.Tensor\n    output_ids: List[int] = field(default_factory=list)\n    sampling_params: Dict[str, Any] = field(default_factory=dict)\n    blocks: List[int] = field(default_factory=list)  # Allocated block IDs\n    finished: bool = False\n    prefill_done: bool = False",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "PagedAttentionCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "class PagedAttentionCache:\n    \"\"\"\n    🧠 PAGED ATTENTION CACHE\n    True vLLM-style PagedAttention implementation with:\n    - Block-wise memory management\n    - Memory fragmentation handling\n    - Efficient block allocation/deallocation\n    \"\"\"\n    def __init__(self, num_blocks: int, block_size: int, num_heads: int, head_dim: int, \n                 dtype: torch.dtype, device: str):",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "VLLMStyleEngine",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "class VLLMStyleEngine:\n    \"\"\"\n    🚀 VLLM-STYLE INFERENCE ENGINE\n    Production-ready inference engine with:\n    - True PagedAttention\n    - Advanced scheduling policies\n    - Async API support\n    - Memory management\n    - Performance monitoring\n    \"\"\"",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "create_vllm_style_engine",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "def create_vllm_style_engine(model: nn.Module, tokenizer, config, **kwargs) -> VLLMStyleEngine:\n    \"\"\"\n    Create a vLLM-style inference engine.\n    Args:\n        model: Pre-trained model\n        tokenizer: Tokenizer for the model\n        config: Model configuration\n        **kwargs: Additional arguments\n    Returns:\n        VLLMStyleEngine instance",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "description": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SchedulerPolicy(Enum):\n    \"\"\"Scheduling policies for request handling.\"\"\"\n    FCFS = \"first_come_first_served\"  # First Come First Served\n    PRIORITY = \"priority\"             # Priority-based\n    MEMORY_AWARE = \"memory_aware\"     # Memory-aware scheduling\n@dataclass\nclass Block:\n    \"\"\"Represents a memory block in the cache.\"\"\"\n    block_id: int",
        "detail": "qwen-llm.fast_inference.core.engine.vllm_style_engine",
        "documentation": {}
    },
    {
        "label": "InferenceRequest",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.examples.advanced.production_server",
        "description": "qwen-llm.fast_inference.examples.advanced.production_server",
        "peekOfCode": "class InferenceRequest:\n    \"\"\"Request structure for inference.\"\"\"\n    prompt: str\n    max_new_tokens: int = 100\n    temperature: float = 0.8\n    top_k: int = 50\n    top_p: float = 0.9\n    request_id: Optional[str] = None\n@dataclass\nclass InferenceResponse:",
        "detail": "qwen-llm.fast_inference.examples.advanced.production_server",
        "documentation": {}
    },
    {
        "label": "InferenceResponse",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.examples.advanced.production_server",
        "description": "qwen-llm.fast_inference.examples.advanced.production_server",
        "peekOfCode": "class InferenceResponse:\n    \"\"\"Response structure for inference.\"\"\"\n    request_id: str\n    generated_text: str\n    generation_time: float\n    tokens_generated: int\n    success: bool\n    error_message: Optional[str] = None\nclass InferenceServer:\n    \"\"\"",
        "detail": "qwen-llm.fast_inference.examples.advanced.production_server",
        "documentation": {}
    },
    {
        "label": "InferenceServer",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.examples.advanced.production_server",
        "description": "qwen-llm.fast_inference.examples.advanced.production_server",
        "peekOfCode": "class InferenceServer:\n    \"\"\"\n    Production-ready inference server.\n    This class provides a robust inference server with:\n    - Request queuing and batching\n    - Error handling and recovery\n    - Performance monitoring\n    - Health checks\n    - Graceful shutdown\n    \"\"\"",
        "detail": "qwen-llm.fast_inference.examples.advanced.production_server",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "qwen-llm.fast_inference.examples.advanced.production_server",
        "description": "qwen-llm.fast_inference.examples.advanced.production_server",
        "peekOfCode": "logger = logging.getLogger(__name__)\n@dataclass\nclass InferenceRequest:\n    \"\"\"Request structure for inference.\"\"\"\n    prompt: str\n    max_new_tokens: int = 100\n    temperature: float = 0.8\n    top_k: int = 50\n    top_p: float = 0.9\n    request_id: Optional[str] = None",
        "detail": "qwen-llm.fast_inference.examples.advanced.production_server",
        "documentation": {}
    },
    {
        "label": "naive_generate_text",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "description": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "peekOfCode": "def naive_generate_text(model, tokenizer, prompt: str, max_length: int = 100,\n                       temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9) -> str:\n    \"\"\"\n    Naive text generation without KV caching (for comparison).\n    This is the original text generation method that processes the entire sequence\n    for each new token. Very slow but simple.\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    # Tokenize prompt",
        "detail": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "description": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "peekOfCode": "def main():\n    \"\"\"Main function demonstrating performance comparison.\"\"\"\n    print(\"📊 Performance Comparison Example\")\n    print(\"=\" * 50)\n    # Test prompts\n    test_prompts = [\n        \"Hello, how are you today?\",\n        \"Tell me a joke about programming\",\n        \"Write a short story about\",\n        \"Explain the concept of machine learning\",",
        "detail": "qwen-llm.fast_inference.examples.basic.performance_comparison",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.basic.quick_start",
        "description": "qwen-llm.fast_inference.examples.basic.quick_start",
        "peekOfCode": "def main():\n    \"\"\"Main function demonstrating basic usage.\"\"\"\n    print(\"🚀 Fast Inference Quick Start Example\")\n    print(\"=\" * 50)\n    # Example 1: Single text generation\n    print(\"\\n📝 Example 1: Single Text Generation\")\n    print(\"-\" * 40)\n    try:\n        # Create engine (you'll need to provide actual model paths)\n        engine = create_simple_fast_inference(",
        "detail": "qwen-llm.fast_inference.examples.basic.quick_start",
        "documentation": {}
    },
    {
        "label": "example_minimal_llm",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.universal_example",
        "description": "qwen-llm.fast_inference.examples.universal_example",
        "peekOfCode": "def example_minimal_llm():\n    \"\"\"\n    🎯 EXAMPLE: Your Custom MinimalLLM Model\n    \"\"\"\n    print(\"🎯 Example 1: Your Custom MinimalLLM Model\")\n    print(\"=\" * 50)\n    # Load your trained model\n    model_path = \"models/final_model1.pt\"\n    tokenizer_path = \"HuggingFaceTB/SmolLM-135M\"\n    if not os.path.exists(model_path):",
        "detail": "qwen-llm.fast_inference.examples.universal_example",
        "documentation": {}
    },
    {
        "label": "example_huggingface_models",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.universal_example",
        "description": "qwen-llm.fast_inference.examples.universal_example",
        "peekOfCode": "def example_huggingface_models():\n    \"\"\"\n    🎯 EXAMPLE: HuggingFace Models\n    \"\"\"\n    print(\"\\n🎯 Example 2: HuggingFace Models\")\n    print(\"=\" * 50)\n    # List of HuggingFace models to try\n    models_to_try = [\n        \"microsoft/DialoGPT-small\",\n        \"gpt2\",",
        "detail": "qwen-llm.fast_inference.examples.universal_example",
        "documentation": {}
    },
    {
        "label": "example_batch_generation",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.universal_example",
        "description": "qwen-llm.fast_inference.examples.universal_example",
        "peekOfCode": "def example_batch_generation():\n    \"\"\"\n    🎯 EXAMPLE: Batch Generation\n    \"\"\"\n    print(\"\\n🎯 Example 3: Batch Generation\")\n    print(\"=\" * 50)\n    # Use a small model for batch generation\n    model_name = \"distilgpt2\"\n    try:\n        # Create universal engine",
        "detail": "qwen-llm.fast_inference.examples.universal_example",
        "documentation": {}
    },
    {
        "label": "example_performance_comparison",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.universal_example",
        "description": "qwen-llm.fast_inference.examples.universal_example",
        "peekOfCode": "def example_performance_comparison():\n    \"\"\"\n    🎯 EXAMPLE: Performance Comparison\n    \"\"\"\n    print(\"\\n🎯 Example 4: Performance Comparison\")\n    print(\"=\" * 50)\n    import time\n    # Test with a small model\n    model_name = \"distilgpt2\"\n    try:",
        "detail": "qwen-llm.fast_inference.examples.universal_example",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.examples.universal_example",
        "description": "qwen-llm.fast_inference.examples.universal_example",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Run all examples to demonstrate universal fast inference.\n    \"\"\"\n    print(\"🌐 UNIVERSAL FAST INFERENCE EXAMPLES\")\n    print(\"=\" * 60)\n    print(\"This example demonstrates how to use the universal fast inference\")\n    print(\"engine with different types of models.\")\n    print()",
        "detail": "qwen-llm.fast_inference.examples.universal_example",
        "documentation": {}
    },
    {
        "label": "TestSimpleFastInference",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.integration.test_engine",
        "description": "qwen-llm.fast_inference.tests.integration.test_engine",
        "peekOfCode": "class TestSimpleFastInference:\n    \"\"\"Integration tests for SimpleFastInference.\"\"\"\n    @pytest.fixture\n    def mock_model(self):\n        \"\"\"Create a mock model for testing.\"\"\"\n        model = Mock()\n        model.parameters.return_value = [torch.tensor([1.0])]\n        model.dtype = torch.float16\n        model.transformer_blocks = [Mock() for _ in range(2)]\n        # Mock transformer block components",
        "detail": "qwen-llm.fast_inference.tests.integration.test_engine",
        "documentation": {}
    },
    {
        "label": "TestSimpleKVCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.unit.test_cache",
        "description": "qwen-llm.fast_inference.tests.unit.test_cache",
        "peekOfCode": "class TestSimpleKVCache:\n    \"\"\"Test cases for SimpleKVCache.\"\"\"\n    def test_initialization(self):\n        \"\"\"Test cache initialization.\"\"\"\n        cache = SimpleKVCache(\n            max_seq_len=100,\n            n_heads=8,\n            head_dim=64,\n            dtype=torch.float16,\n            device=\"cpu\"",
        "detail": "qwen-llm.fast_inference.tests.unit.test_cache",
        "documentation": {}
    },
    {
        "label": "TestPagedKVCache",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.unit.test_cache",
        "description": "qwen-llm.fast_inference.tests.unit.test_cache",
        "peekOfCode": "class TestPagedKVCache:\n    \"\"\"Test cases for PagedKVCache.\"\"\"\n    def test_initialization(self):\n        \"\"\"Test cache initialization.\"\"\"\n        cache = PagedKVCache(\n            n_pages=10,\n            page_size=128,\n            n_heads=8,\n            head_dim=64,\n            dtype=torch.float16,",
        "detail": "qwen-llm.fast_inference.tests.unit.test_cache",
        "documentation": {}
    },
    {
        "label": "TestSamplingParams",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "description": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "peekOfCode": "class TestSamplingParams:\n    \"\"\"Test cases for SamplingParams.\"\"\"\n    def test_default_initialization(self):\n        \"\"\"Test default parameter initialization.\"\"\"\n        params = SamplingParams()\n        assert params.max_new_tokens == 100\n        assert params.temperature == 0.8\n        assert params.top_k == 50\n        assert params.top_p == 0.9\n        assert params.repetition_penalty == 1.0",
        "detail": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "documentation": {}
    },
    {
        "label": "TestSamplingFunctions",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "description": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "peekOfCode": "class TestSamplingFunctions:\n    \"\"\"Test cases for sampling functions.\"\"\"\n    def test_sample_greedy(self):\n        \"\"\"Test greedy sampling.\"\"\"\n        logits = torch.tensor([[1.0, 2.0, 0.5, 3.0]])\n        tokens = sample_greedy(logits)\n        assert tokens.item() == 3  # Highest logit at index 3\n    def test_sample_random(self):\n        \"\"\"Test random sampling.\"\"\"\n        logits = torch.tensor([[1.0, 1.0, 1.0, 1.0]])",
        "detail": "qwen-llm.fast_inference.tests.unit.test_sampling",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def device():\n    \"\"\"Get the device for testing.\"\"\"\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n@pytest.fixture\ndef temp_dir():\n    \"\"\"Create a temporary directory for testing.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        yield tmpdir\n@pytest.fixture\ndef mock_model_config():",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "temp_dir",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def temp_dir():\n    \"\"\"Create a temporary directory for testing.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        yield tmpdir\n@pytest.fixture\ndef mock_model_config():\n    \"\"\"Create a mock model configuration.\"\"\"\n    config = Mock()\n    config.d_model = 512\n    config.n_heads = 8",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "mock_model_config",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def mock_model_config():\n    \"\"\"Create a mock model configuration.\"\"\"\n    config = Mock()\n    config.d_model = 512\n    config.n_heads = 8\n    config.n_kv_heads = 8\n    config.n_kv_groups = 1\n    config.d_k = 64\n    config.d_ff = 2048\n    config.rms_norm_eps = 1e-6",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "mock_tokenizer",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def mock_tokenizer():\n    \"\"\"Create a mock tokenizer.\"\"\"\n    tokenizer = Mock()\n    tokenizer.encode = Mock(return_value=torch.tensor([1, 2, 3, 4, 5]))\n    tokenizer.decode = Mock(return_value=\"Generated text\")\n    tokenizer.eos_token_id = 2\n    tokenizer.pad_token = None\n    return tokenizer\n@pytest.fixture\ndef sample_logits():",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "sample_logits",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def sample_logits():\n    \"\"\"Create sample logits for testing.\"\"\"\n    return torch.tensor([[1.0, 2.0, 0.5, 3.0, 1.5]])\n@pytest.fixture\ndef sample_kv_tensors():\n    \"\"\"Create sample K and V tensors for testing.\"\"\"\n    k = torch.randn(8, 10, 64, dtype=torch.float16)\n    v = torch.randn(8, 10, 64, dtype=torch.float16)\n    return k, v\n@pytest.fixture",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "sample_kv_tensors",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def sample_kv_tensors():\n    \"\"\"Create sample K and V tensors for testing.\"\"\"\n    k = torch.randn(8, 10, 64, dtype=torch.float16)\n    v = torch.randn(8, 10, 64, dtype=torch.float16)\n    return k, v\n@pytest.fixture\ndef sample_prompts():\n    \"\"\"Create sample prompts for testing.\"\"\"\n    return [\n        \"Hello, how are you?\",",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "sample_prompts",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def sample_prompts():\n    \"\"\"Create sample prompts for testing.\"\"\"\n    return [\n        \"Hello, how are you?\",\n        \"Tell me a joke about programming\",\n        \"Write a short story about\",\n        \"Explain the concept of machine learning\",\n        \"What is the meaning of life?\"\n    ]\n@pytest.fixture",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "sample_sampling_params",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def sample_sampling_params():\n    \"\"\"Create sample sampling parameters.\"\"\"\n    from fast_inference.utils.sampling import SamplingParams\n    return SamplingParams(\n        max_new_tokens=50,\n        temperature=0.8,\n        top_k=50,\n        top_p=0.9,\n        repetition_penalty=1.0\n    )",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "pytest_configure",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def pytest_configure(config):\n    \"\"\"Configure pytest markers.\"\"\"\n    config.addinivalue_line(\"markers\", \"slow: marks tests as slow\")\n    config.addinivalue_line(\"markers\", \"integration: marks tests as integration tests\")\n    config.addinivalue_line(\"markers\", \"unit: marks tests as unit tests\")\n    config.addinivalue_line(\"markers\", \"gpu: marks tests that require GPU\")\n    config.addinivalue_line(\"markers\", \"cpu: marks tests that run on CPU only\")\ndef pytest_collection_modifyitems(config, items):\n    \"\"\"Modify test collection to add markers based on test location.\"\"\"\n    for item in items:",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "pytest_collection_modifyitems",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.tests.conftest",
        "description": "qwen-llm.fast_inference.tests.conftest",
        "peekOfCode": "def pytest_collection_modifyitems(config, items):\n    \"\"\"Modify test collection to add markers based on test location.\"\"\"\n    for item in items:\n        # Add markers based on test file location\n        if \"unit\" in str(item.fspath):\n            item.add_marker(pytest.mark.unit)\n        elif \"integration\" in str(item.fspath):\n            item.add_marker(pytest.mark.integration)\n        # Add slow marker to tests that might take time\n        if \"slow\" in item.name or \"benchmark\" in item.name:",
        "detail": "qwen-llm.fast_inference.tests.conftest",
        "documentation": {}
    },
    {
        "label": "BenchmarkResult",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "class BenchmarkResult:\n    \"\"\"\n    Results from a benchmark run.\n    Attributes:\n        method_name: Name of the inference method\n        total_time: Total time in seconds\n        total_tokens: Total number of tokens generated\n        total_requests: Total number of requests processed\n        throughput_tokens_per_sec: Tokens per second\n        throughput_requests_per_sec: Requests per second",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "BenchmarkRunner",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "class BenchmarkRunner:\n    \"\"\"\n    Benchmark runner for inference methods.\n    This class provides utilities for running benchmarks and collecting\n    performance metrics across different inference methods.\n    \"\"\"\n    def __init__(self, device: str = \"auto\"):\n        \"\"\"\n        Initialize benchmark runner.\n        Args:",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "benchmark_inference",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "def benchmark_inference(engine, test_prompts: List[str], max_new_tokens: int = 50,\n                       method_name: str = \"Fast Inference\") -> BenchmarkResult:\n    \"\"\"\n    Quick benchmark function for inference engines.\n    Args:\n        engine: Inference engine with generate_batch method\n        test_prompts: List of test prompts\n        max_new_tokens: Maximum tokens to generate\n        method_name: Name for the benchmark\n    Returns:",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "compare_methods",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "def compare_methods(methods: Dict[str, Callable], test_prompts: List[str], \n                   max_new_tokens: int = 50) -> Dict[str, Any]:\n    \"\"\"\n    Compare multiple inference methods.\n    Args:\n        methods: Dictionary mapping method names to inference functions\n        test_prompts: List of test prompts\n        max_new_tokens: Maximum tokens to generate\n    Returns:\n        Comparison results",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "generate_test_prompts",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "def generate_test_prompts(num_prompts: int = 10, \n                         min_length: int = 20, \n                         max_length: int = 100) -> List[str]:\n    \"\"\"\n    Generate test prompts for benchmarking.\n    Args:\n        num_prompts: Number of prompts to generate\n        min_length: Minimum prompt length\n        max_length: Maximum prompt length\n    Returns:",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "create_performance_report",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "description": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "peekOfCode": "def create_performance_report(results: List[BenchmarkResult], \n                            output_file: Optional[str] = None) -> str:\n    \"\"\"\n    Create a detailed performance report.\n    Args:\n        results: List of benchmark results\n        output_file: Optional file to save report\n    Returns:\n        Report as string\n    \"\"\"",
        "detail": "qwen-llm.fast_inference.utils.benchmarking.benchmarking",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "kind": 6,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "class SamplingParams:\n    \"\"\"\n    Sampling parameters for text generation.\n    Attributes:\n        max_new_tokens: Maximum number of new tokens to generate\n        temperature: Sampling temperature (0.0 = greedy, >1.0 = more random)\n        top_k: Number of top tokens to consider (0 = no limit)\n        top_p: Cumulative probability threshold for nucleus sampling (1.0 = no limit)\n        repetition_penalty: Penalty for repeated tokens (1.0 = no penalty)\n        stop_token_ids: List of token IDs to stop generation at",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "sample_tokens",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def sample_tokens(logits: torch.Tensor, sampling_params: SamplingParams, \n                 previous_tokens: Optional[torch.Tensor] = None) -> torch.Tensor:\n    \"\"\"\n    Sample tokens from logits using the specified sampling parameters.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        sampling_params: Sampling parameters\n        previous_tokens: Previously generated tokens for repetition penalty\n    Returns:\n        Sampled token IDs (batch_size,)",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "apply_repetition_penalty",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def apply_repetition_penalty(logits: torch.Tensor, previous_tokens: torch.Tensor, \n                           penalty: float) -> torch.Tensor:\n    \"\"\"\n    Apply repetition penalty to logits.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        previous_tokens: Previously generated tokens\n        penalty: Repetition penalty factor\n    Returns:\n        Logits with repetition penalty applied",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "apply_top_k_filtering",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def apply_top_k_filtering(logits: torch.Tensor, top_k: int) -> torch.Tensor:\n    \"\"\"\n    Apply top-k filtering to logits.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        top_k: Number of top tokens to keep\n    Returns:\n        Logits with top-k filtering applied\n    \"\"\"\n    if top_k <= 0:",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "apply_top_p_filtering",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def apply_top_p_filtering(logits: torch.Tensor, top_p: float) -> torch.Tensor:\n    \"\"\"\n    Apply top-p (nucleus) filtering to logits.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        top_p: Cumulative probability threshold\n    Returns:\n        Logits with top-p filtering applied\n    \"\"\"\n    if top_p >= 1.0:",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "sample_greedy",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def sample_greedy(logits: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Greedy sampling (always pick the most likely token).\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n    Returns:\n        Greedily sampled token IDs (batch_size,)\n    \"\"\"\n    return logits.argmax(dim=-1)\ndef sample_random(logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "sample_random",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def sample_random(logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n    \"\"\"\n    Random sampling with temperature.\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        temperature: Sampling temperature\n    Returns:\n        Randomly sampled token IDs (batch_size,)\n    \"\"\"\n    if temperature > 0:",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "sample_beam_search",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.utils.sampling.sampling",
        "description": "qwen-llm.fast_inference.utils.sampling.sampling",
        "peekOfCode": "def sample_beam_search(logits: torch.Tensor, beam_size: int = 4, \n                      length_penalty: float = 1.0) -> List[torch.Tensor]:\n    \"\"\"\n    Beam search sampling (simplified version).\n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        beam_size: Number of beams to maintain\n        length_penalty: Length penalty factor\n    Returns:\n        List of beam sequences",
        "detail": "qwen-llm.fast_inference.utils.sampling.sampling",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def create_engine(args):\n    \"\"\"Create inference engine based on arguments.\"\"\"\n    if args.advanced:\n        return create_fast_inference_engine(\n            model_path=args.model_path,\n            tokenizer_path=args.tokenizer_path,\n            max_batch_size=args.batch_size,\n            max_seq_len=args.max_seq_len,\n            n_pages=args.n_pages,\n            page_size=args.page_size",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "cmd_generate",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def cmd_generate(args):\n    \"\"\"Generate text from prompts.\"\"\"\n    print(\"🚀 Fast Inference - Text Generation\")\n    print(\"=\" * 40)\n    try:\n        # Create engine\n        print(f\"Loading model from {args.model_path}...\")\n        engine = create_engine(args)\n        print(\"✅ Model loaded successfully!\")\n        # Create sampling parameters",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "cmd_benchmark",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def cmd_benchmark(args):\n    \"\"\"Run performance benchmark.\"\"\"\n    print(\"📊 Fast Inference - Performance Benchmark\")\n    print(\"=\" * 45)\n    try:\n        # Create engine\n        print(f\"Loading model from {args.model_path}...\")\n        engine = create_engine(args)\n        print(\"✅ Model loaded successfully!\")\n        # Generate test prompts",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "cmd_compare",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def cmd_compare(args):\n    \"\"\"Compare different inference methods.\"\"\"\n    print(\"⚖️ Fast Inference - Method Comparison\")\n    print(\"=\" * 40)\n    try:\n        # Create engines\n        print(f\"Loading models from {args.model_path}...\")\n        # Simple engine\n        simple_engine = create_simple_fast_inference(\n            model_path=args.model_path,",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.fast_inference.cli",
        "description": "qwen-llm.fast_inference.cli",
        "peekOfCode": "def main():\n    \"\"\"Main CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Fast Inference Engine CLI\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  # Generate text from a prompt\n  fast-inference generate --model-path model.pt --tokenizer-path tokenizer --prompts \"Hello, world!\"\n  # Generate from file",
        "detail": "qwen-llm.fast_inference.cli",
        "documentation": {}
    },
    {
        "label": "LoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.lora.lora_layer",
        "description": "qwen-llm.lora_qlora.core.lora.lora_layer",
        "peekOfCode": "class LoRALayer(nn.Module):\n    \"\"\"\n    🎯 LORA LAYER IMPLEMENTATION\n    LoRA (Low-Rank Adaptation) decomposes weight updates into low-rank matrices.\n    Mathematical Foundation:\n    W = W₀ + ΔW = W₀ + BA\n    Where:\n    - W₀: Original frozen weights\n    - B: Low-rank matrix (d × r)\n    - A: Low-rank matrix (r × k)",
        "detail": "qwen-llm.lora_qlora.core.lora.lora_layer",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.lora.lora_linear",
        "description": "qwen-llm.lora_qlora.core.lora.lora_linear",
        "peekOfCode": "class LoRALinear(nn.Module):\n    \"\"\"\n    🎯 LORA LINEAR LAYER\n    Combines original linear layer with LoRA adaptation.\n    This layer maintains the original linear layer's functionality while adding\n    LoRA adaptation for efficient fine-tuning.\n    \"\"\"\n    def __init__(self, original_layer: nn.Linear, rank: int = 16, alpha: float = 32.0, dropout: float = 0.1):\n        \"\"\"\n        Initialize LoRA linear layer.",
        "detail": "qwen-llm.lora_qlora.core.lora.lora_linear",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.lora.lora_manager",
        "description": "qwen-llm.lora_qlora.core.lora.lora_manager",
        "peekOfCode": "class LoRAManager:\n    \"\"\"\n    🎯 LORA MANAGER\n    Manages LoRA adaptation for entire models.\n    This class provides functionality to:\n    - Apply LoRA to specified modules in a model\n    - Track and manage LoRA layers\n    - Analyze parameter counts and memory usage\n    - Save and load LoRA weights\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.core.lora.lora_manager",
        "documentation": {}
    },
    {
        "label": "QLoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.qlora.qlora_layer",
        "description": "qwen-llm.lora_qlora.core.qlora.qlora_layer",
        "peekOfCode": "class QLoRALayer(nn.Module):\n    \"\"\"\n    🎯 QLORA LAYER\n    QLoRA layer combining 4-bit quantization with LoRA adaptation.\n    This layer provides:\n    - 4-bit quantized weights for memory efficiency\n    - LoRA adaptation for fine-tuning\n    - Efficient forward pass with quantized operations\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, rank: int = 16, alpha: float = 1.0, dropout: float = 0.0):",
        "detail": "qwen-llm.lora_qlora.core.qlora.qlora_layer",
        "documentation": {}
    },
    {
        "label": "QLoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.qlora.qlora_linear",
        "description": "qwen-llm.lora_qlora.core.qlora.qlora_linear",
        "peekOfCode": "class QLoRALinear(nn.Module):\n    \"\"\"\n    🎯 QLORA LINEAR LAYER\n    QLoRA linear layer combining 4-bit quantization with LoRA adaptation.\n    This layer provides:\n    - 4-bit quantized weights for memory efficiency\n    - LoRA adaptation for fine-tuning\n    - Efficient forward pass with quantized operations\n    \"\"\"\n    def __init__(self, original_layer: nn.Linear, rank: int = 16, alpha: float = 1.0, dropout: float = 0.0):",
        "detail": "qwen-llm.lora_qlora.core.qlora.qlora_linear",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.qlora.qlora_manager",
        "description": "qwen-llm.lora_qlora.core.qlora.qlora_manager",
        "peekOfCode": "class QLoRAManager:\n    \"\"\"\n    🎯 QLORA MANAGER\n    Manages QLoRA adaptation for entire models.\n    This class provides functionality to:\n    - Apply QLoRA to specified modules in a model\n    - Track and manage QLoRA layers\n    - Analyze parameter counts and memory usage\n    - Save and load QLoRA weights\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.core.qlora.qlora_manager",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.quantization.quantization_expert",
        "description": "qwen-llm.lora_qlora.core.quantization.quantization_expert",
        "peekOfCode": "class QuantizationConfig:\n    \"\"\"\n    🎯 QUANTIZATION CONFIGURATION\n    Configuration for different quantization techniques.\n    \"\"\"\n    # Basic quantization\n    bits: int = 8  # Number of bits for quantization (4, 8, 16)\n    symmetric: bool = True  # Symmetric vs asymmetric quantization\n    # LoRA parameters\n    lora_rank: int = 16  # Rank of LoRA adaptation",
        "detail": "qwen-llm.lora_qlora.core.quantization.quantization_expert",
        "documentation": {}
    },
    {
        "label": "QuantizationExpert",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.quantization.quantization_expert",
        "description": "qwen-llm.lora_qlora.core.quantization.quantization_expert",
        "peekOfCode": "class QuantizationExpert:\n    \"\"\"\n    🎓 QUANTIZATION EXPERT CLASS\n    This class demonstrates different quantization techniques and their trade-offs.\n    \"\"\"\n    def __init__(self):\n        self.quantization_methods = {\n            'fp32': self._fp32_quantization,\n            'fp16': self._fp16_quantization,\n            'int8': self._int8_quantization,",
        "detail": "qwen-llm.lora_qlora.core.quantization.quantization_expert",
        "documentation": {}
    },
    {
        "label": "LoRATrainingConfig",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.config",
        "description": "qwen-llm.lora_qlora.core.training.config",
        "peekOfCode": "class LoRATrainingConfig:\n    \"\"\"\n    🎯 LORA TRAINING CONFIGURATION\n    Configuration for LoRA fine-tuning following the original approach.\n    \"\"\"\n    # Model parameters\n    pretrained_model_path: str = \"models/final_model1.pt\"\n    tokenizer_path: str = \"HuggingFaceTB/SmolLM-135M\"\n    # LoRA parameters\n    lora_rank: int = 16",
        "detail": "qwen-llm.lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "QLoRATrainingConfig",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.config",
        "description": "qwen-llm.lora_qlora.core.training.config",
        "peekOfCode": "class QLoRATrainingConfig:\n    \"\"\"\n    🎯 QLORA TRAINING CONFIGURATION\n    Configuration for QLoRA fine-tuning following the original approach.\n    \"\"\"\n    # Model parameters\n    pretrained_model_path: str = \"models/final_model1.pt\"\n    tokenizer_path: str = \"HuggingFaceTB/SmolLM-135M\"\n    # QLoRA parameters\n    lora_rank: int = 16",
        "detail": "qwen-llm.lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "create_lora_config",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.core.training.config",
        "description": "qwen-llm.lora_qlora.core.training.config",
        "peekOfCode": "def create_lora_config(\n    pretrained_model_path: str = \"models/final_model1.pt\",\n    lora_rank: int = 16,\n    lora_alpha: float = 32.0,\n    learning_rate: float = 1e-4,\n    num_epochs: int = 3,\n    batch_size: int = 8,\n    num_samples: int = 1000\n) -> LoRATrainingConfig:\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "create_qlora_config",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.core.training.config",
        "description": "qwen-llm.lora_qlora.core.training.config",
        "peekOfCode": "def create_qlora_config(\n    pretrained_model_path: str = \"models/final_model1.pt\",\n    lora_rank: int = 16,\n    lora_alpha: float = 32.0,\n    qlora_bits: int = 4,\n    learning_rate: float = 2e-4,\n    num_epochs: int = 3,\n    batch_size: int = 8,\n    num_samples: int = 1000\n) -> QLoRATrainingConfig:",
        "detail": "qwen-llm.lora_qlora.core.training.config",
        "documentation": {}
    },
    {
        "label": "LoRADataset",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.dataset",
        "description": "qwen-llm.lora_qlora.core.training.dataset",
        "peekOfCode": "class LoRADataset(Dataset):\n    \"\"\"\n    📚 LORA DATASET CLASS\n    Custom dataset for LoRA fine-tuning on IMDB sentiment analysis.\n    \"\"\"\n    def __init__(self, texts: List[str], labels: List[int], tokenizer: AutoTokenizer, max_length: int = 256):\n        \"\"\"\n        Initialize LoRA dataset.\n        Args:\n            texts: List of text samples",
        "detail": "qwen-llm.lora_qlora.core.training.dataset",
        "documentation": {}
    },
    {
        "label": "QLoRADataset",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.dataset",
        "description": "qwen-llm.lora_qlora.core.training.dataset",
        "peekOfCode": "class QLoRADataset(Dataset):\n    \"\"\"\n    📚 QLORA DATASET CLASS\n    Custom dataset for QLoRA fine-tuning on IMDB sentiment analysis.\n    \"\"\"\n    def __init__(self, texts: List[str], labels: List[int], tokenizer: AutoTokenizer, max_length: int = 256):\n        \"\"\"\n        Initialize QLoRA dataset.\n        Args:\n            texts: List of text samples",
        "detail": "qwen-llm.lora_qlora.core.training.dataset",
        "documentation": {}
    },
    {
        "label": "load_data_for_lora",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.core.training.dataset",
        "description": "qwen-llm.lora_qlora.core.training.dataset",
        "peekOfCode": "def load_data_for_lora(tokenizer: AutoTokenizer, max_length: int = 256, num_samples: int = 1000):\n    \"\"\"\n    📊 LOAD DATA FOR LORA FINE-TUNING\n    Loads IMDB dataset for LoRA fine-tuning.\n    Args:\n        tokenizer: Tokenizer for encoding text\n        max_length: Maximum sequence length\n        num_samples: Number of samples to use\n    Returns:\n        Tuple of (train_dataset, test_dataset)",
        "detail": "qwen-llm.lora_qlora.core.training.dataset",
        "documentation": {}
    },
    {
        "label": "load_data_for_qlora",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.core.training.dataset",
        "description": "qwen-llm.lora_qlora.core.training.dataset",
        "peekOfCode": "def load_data_for_qlora(tokenizer: AutoTokenizer, max_length: int = 256, num_samples: int = 1000):\n    \"\"\"\n    📊 LOAD DATA FOR QLORA FINE-TUNING\n    Loads IMDB dataset for QLoRA fine-tuning.\n    Args:\n        tokenizer: Tokenizer for encoding text\n        max_length: Maximum sequence length\n        num_samples: Number of samples to use\n    Returns:\n        Tuple of (train_dataset, test_dataset)",
        "detail": "qwen-llm.lora_qlora.core.training.dataset",
        "documentation": {}
    },
    {
        "label": "LoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.trainer",
        "description": "qwen-llm.lora_qlora.core.training.trainer",
        "peekOfCode": "class LoRAClassifier(nn.Module):\n    \"\"\"\n    🎯 LORA CLASSIFIER\n    Combines pre-trained model with LoRA adaptation and classification head.\n    \"\"\"\n    def __init__(self, pretrained_model: MinimalLLM, num_classes: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Add classification head\n        self.classifier = nn.Sequential(",
        "detail": "qwen-llm.lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "QLoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.trainer",
        "description": "qwen-llm.lora_qlora.core.training.trainer",
        "peekOfCode": "class QLoRAClassifier(nn.Module):\n    \"\"\"\n    🎯 QLORA CLASSIFIER\n    Combines pre-trained model with QLoRA adaptation and classification head.\n    \"\"\"\n    def __init__(self, pretrained_model: MinimalLLM, num_classes: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Add classification head\n        self.classifier = nn.Sequential(",
        "detail": "qwen-llm.lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "LoRATrainer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.trainer",
        "description": "qwen-llm.lora_qlora.core.training.trainer",
        "peekOfCode": "class LoRATrainer:\n    \"\"\"\n    🎯 LORA TRAINER\n    Trainer class for LoRA fine-tuning following the original approach.\n    \"\"\"\n    def __init__(self, config: LoRATrainingConfig):\n        \"\"\"\n        Initialize LoRA trainer.\n        Args:\n            config: LoRA training configuration",
        "detail": "qwen-llm.lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "QLoRATrainer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.trainer",
        "description": "qwen-llm.lora_qlora.core.training.trainer",
        "peekOfCode": "class QLoRATrainer:\n    \"\"\"\n    🎯 QLORA TRAINER\n    Trainer class for QLoRA fine-tuning following the original approach.\n    \"\"\"\n    def __init__(self, config: QLoRATrainingConfig):\n        \"\"\"\n        Initialize QLoRA trainer.\n        Args:\n            config: QLoRA training configuration",
        "detail": "qwen-llm.lora_qlora.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "UniversalLoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "description": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "peekOfCode": "class UniversalLoRAClassifier(nn.Module):\n    \"\"\"\n    🎯 UNIVERSAL LORA CLASSIFIER\n    Works with any base model (HuggingFace or custom).\n    \"\"\"\n    def __init__(self, base_model: nn.Module, num_classes: int = 2, dropout: float = 0.1, \n                 model_type: str = \"custom\", hidden_size: Optional[int] = None):\n        super().__init__()\n        self.base_model = base_model\n        self.model_type = model_type",
        "detail": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "UniversalQLoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "description": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "peekOfCode": "class UniversalQLoRAClassifier(nn.Module):\n    \"\"\"\n    🎯 UNIVERSAL QLORA CLASSIFIER\n    Works with any base model (HuggingFace or custom).\n    \"\"\"\n    def __init__(self, base_model: nn.Module, num_classes: int = 2, dropout: float = 0.1, \n                 model_type: str = \"custom\", hidden_size: Optional[int] = None):\n        super().__init__()\n        self.base_model = base_model\n        self.model_type = model_type",
        "detail": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "UniversalLoRATrainer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "description": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "peekOfCode": "class UniversalLoRATrainer:\n    \"\"\"\n    🎯 UNIVERSAL LORA TRAINER\n    Can work with any base model (HuggingFace or custom).\n    \"\"\"\n    def __init__(self, config: LoRATrainingConfig):\n        \"\"\"\n        Initialize Universal LoRA trainer.\n        Args:\n            config: LoRA training configuration",
        "detail": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "fine_tune_huggingface_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "description": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "peekOfCode": "def fine_tune_huggingface_model(model_name: str, config: LoRATrainingConfig):\n    \"\"\"\n    Fine-tune a HuggingFace model with LoRA.\n    Args:\n        model_name: HuggingFace model name (e.g., \"bert-base-uncased\")\n        config: LoRA training configuration\n    \"\"\"\n    trainer = UniversalLoRATrainer(config)\n    trainer.setup_model(model_name_or_path=model_name, model_type=\"huggingface\")\n    trainer.load_data()",
        "detail": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "fine_tune_custom_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "description": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "peekOfCode": "def fine_tune_custom_model(model_path: str, config: LoRATrainingConfig):\n    \"\"\"\n    Fine-tune your custom model with LoRA.\n    Args:\n        model_path: Path to your custom model\n        config: LoRA training configuration\n    \"\"\"\n    config.pretrained_model_path = model_path\n    trainer = UniversalLoRATrainer(config)\n    trainer.setup_model(model_type=\"custom\")",
        "detail": "qwen-llm.lora_qlora.core.training.universal_trainer",
        "documentation": {}
    },
    {
        "label": "compare_lora_qlora",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "description": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "peekOfCode": "def compare_lora_qlora():\n    \"\"\"Compare LoRA and QLoRA performance.\"\"\"\n    print(\"🎯 LoRA vs QLoRA Comparison\")\n    print(\"=\" * 50)\n    # Common configuration\n    common_config = {\n        'model_name': \"Qwen/Qwen2.5-0.5B\",\n        'data_path': \"data/classification_data.json\",\n        'num_epochs': 2,\n        'batch_size': 8,",
        "detail": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "documentation": {}
    },
    {
        "label": "hyperparameter_search",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "description": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "peekOfCode": "def hyperparameter_search():\n    \"\"\"Perform hyperparameter search.\"\"\"\n    print(\"🎯 Hyperparameter Search\")\n    print(\"=\" * 50)\n    # Define search space\n    lora_ranks = [8, 16, 32, 64]\n    lora_alphas = [16.0, 32.0, 64.0, 128.0]\n    learning_rates = [1e-4, 2e-4, 5e-4, 1e-3]\n    best_config = None\n    best_score = 0.0",
        "detail": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "documentation": {}
    },
    {
        "label": "custom_data_processing",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "description": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "peekOfCode": "def custom_data_processing():\n    \"\"\"Demonstrate custom data processing.\"\"\"\n    print(\"🎯 Custom Data Processing\")\n    print(\"=\" * 50)\n    # Load and preprocess data\n    data = load_data(\"data/classification_data.json\")\n    print(f\"Original data size: {len(data)}\")\n    # Preprocess\n    processed_data = preprocess_data(data)\n    print(f\"Processed data size: {len(processed_data)}\")",
        "detail": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "description": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "peekOfCode": "def main():\n    \"\"\"Main function for advanced examples.\"\"\"\n    print(\"🎯 Advanced LoRA/QLoRA Examples\")\n    print(\"=\" * 50)\n    # Run examples\n    compare_lora_qlora()\n    print(\"\\n\" + \"=\" * 50)\n    hyperparameter_search()\n    print(\"\\n\" + \"=\" * 50)\n    custom_data_processing()",
        "detail": "qwen-llm.lora_qlora.examples.advanced.custom_training",
        "documentation": {}
    },
    {
        "label": "example_huggingface_models",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "description": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "peekOfCode": "def example_huggingface_models():\n    \"\"\"\n    🎯 EXAMPLE: Fine-tune HuggingFace models with LoRA\n    \"\"\"\n    print(\"🎯 HuggingFace Models with LoRA\")\n    print(\"=\" * 50)\n    # Example 1: BERT\n    print(\"\\n1. Fine-tuning BERT with LoRA:\")\n    config = LoRATrainingConfig(\n        num_epochs=1,",
        "detail": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "documentation": {}
    },
    {
        "label": "example_custom_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "description": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "peekOfCode": "def example_custom_model():\n    \"\"\"\n    🎯 EXAMPLE: Fine-tune your custom model with LoRA\n    \"\"\"\n    print(\"\\n🎯 Custom Model with LoRA\")\n    print(\"=\" * 50)\n    config = LoRATrainingConfig(\n        pretrained_model_path=\"models/final_model1.pt\",\n        num_epochs=1,\n        batch_size=4,",
        "detail": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "documentation": {}
    },
    {
        "label": "example_manual_setup",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "description": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "peekOfCode": "def example_manual_setup():\n    \"\"\"\n    🎯 EXAMPLE: Manual setup for more control\n    \"\"\"\n    print(\"\\n🎯 Manual Setup Example\")\n    print(\"=\" * 50)\n    # Create config\n    config = LoRATrainingConfig(\n        num_epochs=1,\n        batch_size=4,",
        "detail": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "description": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Demonstrates different ways to use the universal trainer.\n    \"\"\"\n    print(\"🎯 Universal LoRA Fine-tuning Examples\")\n    print(\"=\" * 60)\n    # Example 1: HuggingFace models\n    example_huggingface_models()\n    # Example 2: Custom model",
        "detail": "qwen-llm.lora_qlora.examples.advanced.universal_example",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.basic.lora_example",
        "description": "qwen-llm.lora_qlora.examples.basic.lora_example",
        "peekOfCode": "def main():\n    \"\"\"Main function for LoRA example.\"\"\"\n    print(\"🎯 LoRA Fine-tuning Example\")\n    print(\"=\" * 50)\n    # Configuration following original approach\n    config = LoRATrainingConfig(\n        pretrained_model_path=\"models/final_model1.pt\",\n        num_epochs=1,\n        batch_size=4,\n        learning_rate=1e-4,",
        "detail": "qwen-llm.lora_qlora.examples.basic.lora_example",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.examples.basic.qlora_example",
        "description": "qwen-llm.lora_qlora.examples.basic.qlora_example",
        "peekOfCode": "def main():\n    \"\"\"Main function for QLoRA example.\"\"\"\n    print(\"🎯 QLoRA Fine-tuning Example\")\n    print(\"=\" * 50)\n    # Configuration following original approach\n    config = QLoRATrainingConfig(\n        pretrained_model_path=\"models/final_model1.pt\",\n        num_epochs=1,\n        batch_size=4,\n        learning_rate=2e-4,",
        "detail": "qwen-llm.lora_qlora.examples.basic.qlora_example",
        "documentation": {}
    },
    {
        "label": "TestTrainingPipeline",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.integration.test_training_pipeline",
        "description": "qwen-llm.lora_qlora.tests.integration.test_training_pipeline",
        "peekOfCode": "class TestTrainingPipeline(unittest.TestCase):\n    \"\"\"Test cases for training pipeline.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        # Create temporary directory\n        self.temp_dir = tempfile.mkdtemp()\n        # Create sample data\n        self.sample_data = [\n            {\"text\": \"This is a positive review\", \"label\": 1},\n            {\"text\": \"This is a negative review\", \"label\": 0},",
        "detail": "qwen-llm.lora_qlora.tests.integration.test_training_pipeline",
        "documentation": {}
    },
    {
        "label": "TestLoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "description": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "peekOfCode": "class TestLoRALayer(unittest.TestCase):\n    \"\"\"Test cases for LoRALayer.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.in_features = 128\n        self.out_features = 64\n        self.rank = 16\n        self.alpha = 32.0\n        self.dropout = 0.1\n        self.lora_layer = LoRALayer(",
        "detail": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "documentation": {}
    },
    {
        "label": "TestLoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "description": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "peekOfCode": "class TestLoRALinear(unittest.TestCase):\n    \"\"\"Test cases for LoRALinear.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.in_features = 128\n        self.out_features = 64\n        self.rank = 16\n        self.alpha = 32.0\n        self.dropout = 0.1\n        # Create original linear layer",
        "detail": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "documentation": {}
    },
    {
        "label": "TestLoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "description": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "peekOfCode": "class TestLoRAManager(unittest.TestCase):\n    \"\"\"Test cases for LoRAManager.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        # Create a simple model\n        self.model = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),",
        "detail": "qwen-llm.lora_qlora.tests.unit.test_lora",
        "documentation": {}
    },
    {
        "label": "TestQLoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "description": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "peekOfCode": "class TestQLoRALayer(unittest.TestCase):\n    \"\"\"Test cases for QLoRALayer.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.in_features = 128\n        self.out_features = 64\n        self.rank = 16\n        self.alpha = 32.0\n        self.dropout = 0.1\n        self.qlora_layer = QLoRALayer(",
        "detail": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "documentation": {}
    },
    {
        "label": "TestQLoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "description": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "peekOfCode": "class TestQLoRALinear(unittest.TestCase):\n    \"\"\"Test cases for QLoRALinear.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.in_features = 128\n        self.out_features = 64\n        self.rank = 16\n        self.alpha = 32.0\n        self.dropout = 0.1\n        # Create original linear layer",
        "detail": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "documentation": {}
    },
    {
        "label": "TestQLoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "description": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "peekOfCode": "class TestQLoRAManager(unittest.TestCase):\n    \"\"\"Test cases for QLoRAManager.\"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        # Create a simple model\n        self.model = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),",
        "detail": "qwen-llm.lora_qlora.tests.unit.test_qlora",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.config.config",
        "description": "qwen-llm.lora_qlora.utils.config.config",
        "peekOfCode": "def load_config(config_path: Union[str, Path]) -> Dict[str, Any]:\n    \"\"\"\n    🎯 LOAD CONFIGURATION\n    Load configuration from file.\n    Args:\n        config_path: Path to configuration file\n    Returns:\n        Configuration dictionary\n    \"\"\"\n    config_path = Path(config_path)",
        "detail": "qwen-llm.lora_qlora.utils.config.config",
        "documentation": {}
    },
    {
        "label": "save_config",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.config.config",
        "description": "qwen-llm.lora_qlora.utils.config.config",
        "peekOfCode": "def save_config(config: Dict[str, Any], config_path: Union[str, Path], format: str = 'json'):\n    \"\"\"\n    🎯 SAVE CONFIGURATION\n    Save configuration to file.\n    Args:\n        config: Configuration dictionary\n        config_path: Path to save configuration\n        format: File format ('json' or 'yaml')\n    \"\"\"\n    config_path = Path(config_path)",
        "detail": "qwen-llm.lora_qlora.utils.config.config",
        "documentation": {}
    },
    {
        "label": "merge_configs",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.config.config",
        "description": "qwen-llm.lora_qlora.utils.config.config",
        "peekOfCode": "def merge_configs(base_config: Dict[str, Any], override_config: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    🎯 MERGE CONFIGURATIONS\n    Merge two configurations with override taking precedence.\n    Args:\n        base_config: Base configuration\n        override_config: Override configuration\n    Returns:\n        Merged configuration\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.config.config",
        "documentation": {}
    },
    {
        "label": "validate_config",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.config.config",
        "description": "qwen-llm.lora_qlora.utils.config.config",
        "peekOfCode": "def validate_config(config: Dict[str, Any], required_keys: list) -> bool:\n    \"\"\"\n    🎯 VALIDATE CONFIGURATION\n    Validate that configuration contains required keys.\n    Args:\n        config: Configuration to validate\n        required_keys: List of required keys\n    Returns:\n        True if valid, False otherwise\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.config.config",
        "documentation": {}
    },
    {
        "label": "get_config_value",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.config.config",
        "description": "qwen-llm.lora_qlora.utils.config.config",
        "peekOfCode": "def get_config_value(config: Dict[str, Any], key: str, default: Any = None) -> Any:\n    \"\"\"\n    🎯 GET CONFIG VALUE\n    Get configuration value with default fallback.\n    Args:\n        config: Configuration dictionary\n        key: Key to retrieve\n        default: Default value if key not found\n    Returns:\n        Configuration value or default",
        "detail": "qwen-llm.lora_qlora.utils.config.config",
        "documentation": {}
    },
    {
        "label": "update_config",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.config.config",
        "description": "qwen-llm.lora_qlora.utils.config.config",
        "peekOfCode": "def update_config(config: Dict[str, Any], updates: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    🎯 UPDATE CONFIGURATION\n    Update configuration with new values.\n    Args:\n        config: Configuration to update\n        updates: Updates to apply\n    Returns:\n        Updated configuration\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.config.config",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.data.data",
        "description": "qwen-llm.lora_qlora.utils.data.data",
        "peekOfCode": "def load_data(data_path: str, format: str = 'auto') -> List[Dict[str, Any]]:\n    \"\"\"\n    🎯 LOAD DATA\n    Load data from file.\n    Args:\n        data_path: Path to data file\n        format: Data format ('json', 'csv', 'auto')\n    Returns:\n        List of data samples\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.data.data",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.data.data",
        "description": "qwen-llm.lora_qlora.utils.data.data",
        "peekOfCode": "def preprocess_data(data: List[Dict[str, Any]], text_field: str = 'text', label_field: str = 'label') -> List[Dict[str, Any]]:\n    \"\"\"\n    🎯 PREPROCESS DATA\n    Preprocess data for training.\n    Args:\n        data: Raw data\n        text_field: Name of text field\n        label_field: Name of label field\n    Returns:\n        Preprocessed data",
        "detail": "qwen-llm.lora_qlora.utils.data.data",
        "documentation": {}
    },
    {
        "label": "split_data",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.data.data",
        "description": "qwen-llm.lora_qlora.utils.data.data",
        "peekOfCode": "def split_data(data: List[Dict[str, Any]], train_split: float = 0.8, val_split: float = 0.1, test_split: float = 0.1, random_seed: int = 42) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n    \"\"\"\n    🎯 SPLIT DATA\n    Split data into train, validation, and test sets.\n    Args:\n        data: Data to split\n        train_split: Training set proportion\n        val_split: Validation set proportion\n        test_split: Test set proportion\n        random_seed: Random seed for reproducibility",
        "detail": "qwen-llm.lora_qlora.utils.data.data",
        "documentation": {}
    },
    {
        "label": "balance_data",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.data.data",
        "description": "qwen-llm.lora_qlora.utils.data.data",
        "peekOfCode": "def balance_data(data: List[Dict[str, Any]], label_field: str = 'label', method: str = 'undersample') -> List[Dict[str, Any]]:\n    \"\"\"\n    🎯 BALANCE DATA\n    Balance data by class distribution.\n    Args:\n        data: Data to balance\n        label_field: Name of label field\n        method: Balancing method ('undersample', 'oversample')\n    Returns:\n        Balanced data",
        "detail": "qwen-llm.lora_qlora.utils.data.data",
        "documentation": {}
    },
    {
        "label": "filter_data",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.data.data",
        "description": "qwen-llm.lora_qlora.utils.data.data",
        "peekOfCode": "def filter_data(data: List[Dict[str, Any]], min_length: int = 10, max_length: int = 1000) -> List[Dict[str, Any]]:\n    \"\"\"\n    🎯 FILTER DATA\n    Filter data by text length.\n    Args:\n        data: Data to filter\n        min_length: Minimum text length\n        max_length: Maximum text length\n    Returns:\n        Filtered data",
        "detail": "qwen-llm.lora_qlora.utils.data.data",
        "documentation": {}
    },
    {
        "label": "get_data_stats",
        "kind": 2,
        "importPath": "qwen-llm.lora_qlora.utils.data.data",
        "description": "qwen-llm.lora_qlora.utils.data.data",
        "peekOfCode": "def get_data_stats(data: List[Dict[str, Any]], label_field: str = 'label') -> Dict[str, Any]:\n    \"\"\"\n    🎯 GET DATA STATISTICS\n    Get statistics about the data.\n    Args:\n        data: Data to analyze\n        label_field: Name of label field\n    Returns:\n        Data statistics\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.data.data",
        "documentation": {}
    },
    {
        "label": "ModelServer",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.utils.serving.serving",
        "description": "qwen-llm.lora_qlora.utils.serving.serving",
        "peekOfCode": "class ModelServer:\n    \"\"\"\n    🎯 MODEL SERVER\n    Server for serving trained LoRA and QLoRA models using your custom MinimalLLM.\n    This class provides:\n    - Model loading and initialization\n    - Inference capabilities\n    - Batch processing\n    - Performance monitoring\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.serving.serving",
        "documentation": {}
    },
    {
        "label": "InferenceEngine",
        "kind": 6,
        "importPath": "qwen-llm.lora_qlora.utils.serving.serving",
        "description": "qwen-llm.lora_qlora.utils.serving.serving",
        "peekOfCode": "class InferenceEngine:\n    \"\"\"\n    🎯 INFERENCE ENGINE\n    High-performance inference engine for LoRA and QLoRA models.\n    This class provides:\n    - Optimized inference\n    - Caching mechanisms\n    - Performance monitoring\n    - Batch processing\n    \"\"\"",
        "detail": "qwen-llm.lora_qlora.utils.serving.serving",
        "documentation": {}
    },
    {
        "label": "naive_generate_text",
        "kind": 2,
        "importPath": "qwen-llm.original_files.compare_inference",
        "description": "qwen-llm.original_files.compare_inference",
        "peekOfCode": "def naive_generate_text(model: MinimalLLM, tokenizer, prompt: str, max_length: int = 100,\n                       temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9) -> str:\n    \"\"\"\n    🐌 NAIVE TEXT GENERATION (NO KV CACHE)\n    This is the original text generation method that processes the entire sequence\n    for each new token. Very slow but simple.\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    # Tokenize prompt",
        "detail": "qwen-llm.original_files.compare_inference",
        "documentation": {}
    },
    {
        "label": "compare_inference_methods",
        "kind": 2,
        "importPath": "qwen-llm.original_files.compare_inference",
        "description": "qwen-llm.original_files.compare_inference",
        "peekOfCode": "def compare_inference_methods(model_path: str, tokenizer_path: str, \n                            test_prompts: List[str], max_new_tokens: int = 50):\n    \"\"\"\n    Compare different inference methods\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        test_prompts: List of test prompts\n        max_new_tokens: Maximum tokens to generate\n    \"\"\"",
        "detail": "qwen-llm.original_files.compare_inference",
        "documentation": {}
    },
    {
        "label": "run_quick_test",
        "kind": 2,
        "importPath": "qwen-llm.original_files.compare_inference",
        "description": "qwen-llm.original_files.compare_inference",
        "peekOfCode": "def run_quick_test():\n    \"\"\"Run a quick test with simple prompts\"\"\"\n    test_prompts = [\n        \"Hello, how are you?\",\n        \"Tell me a joke about\",\n        \"Write a short story about\",\n        \"Explain the concept of\",\n        \"What is the meaning of\"\n    ]\n    # You'll need to provide actual model paths",
        "detail": "qwen-llm.original_files.compare_inference",
        "documentation": {}
    },
    {
        "label": "run_comprehensive_benchmark",
        "kind": 2,
        "importPath": "qwen-llm.original_files.compare_inference",
        "description": "qwen-llm.original_files.compare_inference",
        "peekOfCode": "def run_comprehensive_benchmark():\n    \"\"\"Run a comprehensive benchmark with more prompts\"\"\"\n    test_prompts = [\n        \"The quick brown fox jumps over the lazy dog. This is a test of\",\n        \"In a world where artificial intelligence has become\",\n        \"The ancient library contained thousands of books about\",\n        \"As the sun set over the mountains, the travelers\",\n        \"The scientist discovered a new element that could\",\n        \"Once upon a time, in a distant galaxy\",\n        \"The recipe for the perfect chocolate cake includes\",",
        "detail": "qwen-llm.original_files.compare_inference",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class SamplingParams:\n    \"\"\"Sampling parameters for text generation\"\"\"\n    max_new_tokens: int = 100\n    temperature: float = 0.8\n    top_k: int = 50\n    top_p: float = 0.9\n    repetition_penalty: float = 1.0\n    stop_token_ids: List[int] = None\n    def __post_init__(self):\n        if self.stop_token_ids is None:",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class Sequence:\n    \"\"\"Represents a single generation sequence\"\"\"\n    seq_id: int\n    prompt: str\n    input_ids: torch.Tensor\n    output_ids: List[int]\n    sampling_params: SamplingParams\n    batch_idx: int = -1\n    finished: bool = False\n    prefill_done: bool = False",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "PagedKVCache",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class PagedKVCache(nn.Module):\n    \"\"\"\n    🧠 PAGED KV CACHE\n    Efficient memory management for KV cache using page-based allocation.\n    Inspired by vLLM's PagedAttention but simplified for our use case.\n    \"\"\"\n    def __init__(self, n_pages: int, page_size: int, n_heads: int, head_dim: int, dtype: torch.dtype, device: str):\n        super().__init__()\n        self.n_pages = n_pages\n        self.page_size = page_size",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "OptimizedAttention",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class OptimizedAttention(nn.Module):\n    \"\"\"\n    🎯 OPTIMIZED ATTENTION WITH KV CACHE\n    Combines GQA with efficient KV caching for fast inference.\n    \"\"\"\n    def __init__(self, config: SmallModelConfig, kv_cache: PagedKVCache):\n        super().__init__()\n        self.d_model = config.d_model\n        self.n_heads = config.n_heads\n        self.n_kv_heads = config.n_kv_heads",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "OptimizedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class OptimizedTransformerBlock(nn.Module):\n    \"\"\"\n    🏗️ OPTIMIZED TRANSFORMER BLOCK\n    Transformer block with optimized attention and KV caching.\n    \"\"\"\n    def __init__(self, config: SmallModelConfig, kv_cache: PagedKVCache):\n        super().__init__()\n        self.attention = OptimizedAttention(config, kv_cache)\n        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n        # Pre-norm architecture",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "FastInferenceEngine",
        "kind": 6,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "class FastInferenceEngine:\n    \"\"\"\n    🚀 FAST INFERENCE ENGINE\n    High-performance inference engine with:\n    - Paged KV cache for memory efficiency\n    - Continuous batching for throughput\n    - CUDA graphs for optimization\n    - Dynamic scheduling\n    \"\"\"\n    def __init__(self, model: MinimalLLM, tokenizer, config: SmallModelConfig, ",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "create_fast_inference_engine",
        "kind": 2,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "def create_fast_inference_engine(model_path: str, tokenizer_path: str, \n                                max_batch_size: int = 32, max_seq_len: int = 2048,\n                                n_pages: int = 1000, page_size: int = 128) -> FastInferenceEngine:\n    \"\"\"\n    Create a fast inference engine from saved model\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        max_batch_size: Maximum batch size\n        max_seq_len: Maximum sequence length",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "benchmark_inference_engine",
        "kind": 2,
        "importPath": "qwen-llm.original_files.fast_inference",
        "description": "qwen-llm.original_files.fast_inference",
        "peekOfCode": "def benchmark_inference_engine(engine: FastInferenceEngine, num_requests: int = 100, \n                              max_input_len: int = 512, max_output_len: int = 256):\n    \"\"\"\n    Benchmark the inference engine\n    Args:\n        engine: FastInferenceEngine instance\n        num_requests: Number of requests to process\n        max_input_len: Maximum input length\n        max_output_len: Maximum output length\n    \"\"\"",
        "detail": "qwen-llm.original_files.fast_inference",
        "documentation": {}
    },
    {
        "label": "SimpleKVCache",
        "kind": 6,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "class SimpleKVCache:\n    \"\"\"\n    🎯 SIMPLE KV CACHE\n    A straightforward KV cache implementation that stores key-value pairs\n    for each sequence position. Much simpler than paged attention but still effective.\n    \"\"\"\n    def __init__(self, max_seq_len: int, n_heads: int, head_dim: int, dtype: torch.dtype, device: str):\n        self.max_seq_len = max_seq_len\n        self.n_heads = n_heads\n        self.head_dim = head_dim",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "CachedAttention",
        "kind": 6,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "class CachedAttention(nn.Module):\n    \"\"\"\n    🎯 CACHED ATTENTION\n    Attention layer with simple KV caching for fast inference.\n    \"\"\"\n    def __init__(self, config: SmallModelConfig, kv_cache: SimpleKVCache):\n        super().__init__()\n        self.d_model = config.d_model\n        self.n_heads = config.n_heads\n        self.n_kv_heads = config.n_kv_heads",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "CachedTransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "class CachedTransformerBlock(nn.Module):\n    \"\"\"\n    🏗️ CACHED TRANSFORMER BLOCK\n    Transformer block with cached attention.\n    \"\"\"\n    def __init__(self, config: SmallModelConfig, kv_cache: SimpleKVCache):\n        super().__init__()\n        self.attention = CachedAttention(config, kv_cache)\n        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n        # Pre-norm architecture",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "SimpleFastInference",
        "kind": 6,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "class SimpleFastInference:\n    \"\"\"\n    🚀 SIMPLE FAST INFERENCE ENGINE\n    A simplified but fast inference engine that adds KV caching to your existing model.\n    Much easier to understand and integrate than full vLLM-style implementations.\n    \"\"\"\n    def __init__(self, model: MinimalLLM, tokenizer, config: SmallModelConfig, \n                 max_seq_len: int = 2048):\n        self.model = model\n        self.tokenizer = tokenizer",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "create_simple_fast_inference",
        "kind": 2,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "def create_simple_fast_inference(model_path: str, tokenizer_path: str, \n                                max_seq_len: int = 2048) -> SimpleFastInference:\n    \"\"\"\n    Create a simple fast inference engine from saved model\n    Args:\n        model_path: Path to saved model\n        tokenizer_path: Path to tokenizer\n        max_seq_len: Maximum sequence length\n    Returns:\n        SimpleFastInference instance",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "benchmark_simple_inference",
        "kind": 2,
        "importPath": "qwen-llm.original_files.simple_fast_inference",
        "description": "qwen-llm.original_files.simple_fast_inference",
        "peekOfCode": "def benchmark_simple_inference(engine: SimpleFastInference, num_requests: int = 10, \n                              max_input_len: int = 100, max_output_len: int = 100):\n    \"\"\"\n    Benchmark the simple inference engine\n    Args:\n        engine: SimpleFastInference instance\n        num_requests: Number of requests to process\n        max_input_len: Maximum input length\n        max_output_len: Maximum output length\n    \"\"\"",
        "detail": "qwen-llm.original_files.simple_fast_inference",
        "documentation": {}
    },
    {
        "label": "PretrainingConfig",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.config.config",
        "description": "qwen-llm.pretraining.core.config.config",
        "peekOfCode": "class PretrainingConfig:\n    \"\"\"\n    🎯 PRETRAINING CONFIGURATION\n    This configuration is optimized for:\n    - Fast training on CPU/limited GPU\n    - Learning the concepts without waiting hours\n    - Still demonstrating all key components\n    \"\"\"\n    # Model architecture - MUCH SMALLER\n    d_model: int = 128          # Reduced from 384 (3x smaller)",
        "detail": "qwen-llm.pretraining.core.config.config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.core.config.config",
        "description": "qwen-llm.pretraining.core.config.config",
        "peekOfCode": "def set_seed(seed: int = 42):\n    \"\"\"Set all random seeds for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f\"🌱 Set all seeds to {seed}\")\n@dataclass",
        "detail": "qwen-llm.pretraining.core.config.config",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.model.components",
        "description": "qwen-llm.pretraining.core.model.components",
        "peekOfCode": "class Rotary(nn.Module):\n    \"\"\"\n    🔄 ROTARY POSITIONAL EMBEDDINGS (RoPE)\n    This is one of the most important innovations in modern transformers!\n    🎯 What RoPE does:\n    - Encodes position information by ROTATING vectors\n    - Unlike traditional positional embeddings that just ADD position info\n    - Allows the model to understand relative positions naturally\n    🧮 The Math:\n    - For each position i, we compute rotation angles based on i",
        "detail": "qwen-llm.pretraining.core.model.components",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.model.components",
        "description": "qwen-llm.pretraining.core.model.components",
        "peekOfCode": "class Qwen3Attention(nn.Module):\n    \"\"\"\n    🎯 GROUPED-QUERY ATTENTION (GQA) IMPLEMENTATION\n    This is the heart of modern transformer attention mechanisms!\n    🧠 Key Innovations:\n    1. Grouped-Query Attention: Fewer KV heads than Query heads\n    2. QK-Normalization: Normalizes queries and keys for stability\n    3. RoPE: Rotary positional embeddings\n    4. Scaled dot-product attention: The core attention mechanism\n    🔍 How it works:",
        "detail": "qwen-llm.pretraining.core.model.components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.model.components",
        "description": "qwen-llm.pretraining.core.model.components",
        "peekOfCode": "class SwiGLUFeedForward(nn.Module):\n    \"\"\"\n    🔥 SWIGLU FEED-FORWARD NETWORK\n    SwiGLU is a modern activation function that combines:\n    - Swish activation: x * sigmoid(x) (smooth, non-monotonic)\n    - GLU (Gated Linear Unit): element-wise multiplication with a gate\n    🧮 The Math:\n    SwiGLU(x) = Swish(W1(x)) ⊙ W2(x)\n    Where:\n    - W1(x) is the \"gate\" (controls information flow)",
        "detail": "qwen-llm.pretraining.core.model.components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.model.components",
        "description": "qwen-llm.pretraining.core.model.components",
        "peekOfCode": "class TransformerBlock(nn.Module):\n    \"\"\"\n    🏗️ TRANSFORMER BLOCK - THE BUILDING BLOCK OF MODERN LLMs\n    This combines all the components into a complete transformer layer:\n    🧠 Architecture:\n    1. Pre-norm attention (RMSNorm before attention)\n    2. Residual connection (x + attention(x))\n    3. Pre-norm feed-forward (RMSNorm before SwiGLU)\n    4. Residual connection (x + feedforward(x))\n    🔍 Pre-norm vs Post-norm:",
        "detail": "qwen-llm.pretraining.core.model.components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.model.components",
        "description": "qwen-llm.pretraining.core.model.components",
        "peekOfCode": "class RMSNorm(nn.Module):\n    \"\"\"\n    📐 RMSNorm - ROOT MEAN SQUARE NORMALIZATION\n    This is a modern alternative to LayerNorm that's more efficient:\n    🧮 The Math:\n    RMSNorm(x) = x / sqrt(mean(x²) + ε) * g\n    Where:\n    - x is the input\n    - mean(x²) is the mean of squared values\n    - ε is a small constant (1e-6)",
        "detail": "qwen-llm.pretraining.core.model.components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.core.model.components",
        "description": "qwen-llm.pretraining.core.model.components",
        "peekOfCode": "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    🔑 GROUPED-QUERY ATTENTION HELPER\n    This implements the key innovation in GQA:\n    - Fewer Key-Value heads than Query heads\n    - Each KV head is \"shared\" across multiple Query heads\n    - Massive memory savings with minimal performance loss\n    🧮 Example:\n    - 8 Query heads, 2 KV heads\n    - KV head 1 is used by Query heads 1,2,3,4",
        "detail": "qwen-llm.pretraining.core.model.components",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.model.minimal_llm",
        "description": "qwen-llm.pretraining.core.model.minimal_llm",
        "peekOfCode": "class MinimalLLM(nn.Module):\n    \"\"\"\n    🏗️ COMPLETE QWEN3-STYLE LANGUAGE MODEL\n    This is the full language model that combines all components:\n    🧠 Architecture:\n    1. Token Embedding: Convert token IDs to vectors\n    2. Positional Dropout: Prevent overfitting on positions\n    3. Transformer Blocks: Stack of attention + feed-forward layers\n    4. Final Normalization: RMSNorm before output\n    5. Language Modeling Head: Convert vectors back to token probabilities",
        "detail": "qwen-llm.pretraining.core.model.minimal_llm",
        "documentation": {}
    },
    {
        "label": "Muon",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.training.optimizer",
        "description": "qwen-llm.pretraining.core.training.optimizer",
        "peekOfCode": "class Muon(torch.optim.Optimizer):\n    \"\"\"\n    🚀 MUON OPTIMIZER: MomentUm Orthogonalized by Newton-schulz\n    This is a revolutionary optimizer that combines:\n    1. Momentum (like Adam) - remembers past gradients\n    2. Orthogonalization (Newton-Schulz) - makes gradients \"well-behaved\"\n    3. Adaptive learning rates - adjusts based on matrix dimensions\n    🎯 Why Muon is special:\n    - 30-50% faster convergence than Adam\n    - More stable training (fewer gradient explosions)",
        "detail": "qwen-llm.pretraining.core.training.optimizer",
        "documentation": {}
    },
    {
        "label": "zeropower_via_newtonschulz5",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.core.training.optimizer",
        "description": "qwen-llm.pretraining.core.training.optimizer",
        "peekOfCode": "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n    \"\"\"\n    🔬 NEWTON-SCHULZ ORTHOGONALIZATION\n    This is the mathematical heart of the Muon optimizer:\n    🎯 What it does:\n    - Takes a matrix G (gradients)\n    - Makes it \"orthogonal\" (like rotating it to be perfectly aligned)\n    - Uses Newton-Schulz iteration (a fast numerical method)\n    🧮 Why orthogonalization helps:\n    - Orthogonal matrices preserve vector lengths and angles",
        "detail": "qwen-llm.pretraining.core.training.optimizer",
        "documentation": {}
    },
    {
        "label": "setup_muon_optimizer",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.core.training.optimizer",
        "description": "qwen-llm.pretraining.core.training.optimizer",
        "peekOfCode": "def setup_muon_optimizer(model: nn.Module, config):\n    \"\"\"\n    🚀 HYBRID OPTIMIZER SETUP\n    This function sets up a hybrid optimization strategy:\n    - Muon optimizer for 2D parameters (attention and feed-forward weights)\n    - AdamW optimizer for other parameters (embeddings, norms, biases)\n    🎯 Why hybrid approach:\n    - Muon works best on 2D matrices (attention, feed-forward)\n    - AdamW is better for 1D parameters (embeddings, biases)\n    - This gives us the best of both worlds",
        "detail": "qwen-llm.pretraining.core.training.optimizer",
        "documentation": {}
    },
    {
        "label": "PretrainingTrainer",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.core.training.trainer",
        "description": "qwen-llm.pretraining.core.training.trainer",
        "peekOfCode": "class PretrainingTrainer:\n    \"\"\"\n    🎯 PRETRAINING TRAINER\n    Complete training pipeline for pretraining language models.\n    \"\"\"\n    def __init__(self, config):\n        self.config = config\n        self.model = None\n        self.optimizers = None\n        self.schedulers = None",
        "detail": "qwen-llm.pretraining.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.core.training.trainer",
        "description": "qwen-llm.pretraining.core.training.trainer",
        "peekOfCode": "def evaluate_model(model: nn.Module, val_loader: DataLoader, config):\n    \"\"\"\n    📊 MODEL EVALUATION FUNCTION\n    This function evaluates the model's performance on validation data:\n    🎯 Metrics Computed:\n    1. Loss: Cross-entropy loss (lower is better)\n    2. Accuracy: Percentage of correct next-token predictions\n    3. Perplexity: exp(loss) - measures model's \"surprise\" (lower is better)\n    🔍 Why these metrics matter:\n    - Loss: Direct measure of how well the model predicts",
        "detail": "qwen-llm.pretraining.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.core.training.trainer",
        "description": "qwen-llm.pretraining.core.training.trainer",
        "peekOfCode": "def load_checkpoint(model_path: str, config):\n    \"\"\"\n    📦 LOAD CHECKPOINT FOR RESUMING TRAINING\n    This function loads a previously trained model checkpoint and returns\n    the model, optimizers, schedulers, and training state.\n    \"\"\"\n    print(f\"📦 Loading checkpoint from {model_path}\")\n    # Load checkpoint with safe loading to handle import path changes\n    try:\n        # Try loading with weights_only=False to handle custom classes",
        "detail": "qwen-llm.pretraining.core.training.trainer",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.examples.advanced.resume_training",
        "description": "qwen-llm.pretraining.examples.advanced.resume_training",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 RESUME TRAINING EXAMPLE\n    This function demonstrates how to resume training from a checkpoint.\n    \"\"\"\n    print(\"🔄 RESUME TRAINING EXAMPLE\")\n    print(\"=\" * 50)\n    # Check if checkpoint exists\n    checkpoint_path = \"models/best_model1.pt\"\n    if not os.path.exists(checkpoint_path):",
        "detail": "qwen-llm.pretraining.examples.advanced.resume_training",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.examples.basic.inference_example",
        "description": "qwen-llm.pretraining.examples.basic.inference_example",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 INFERENCE EXAMPLE\n    This function demonstrates how to load and use a trained model.\n    \"\"\"\n    print(\"🎭 INFERENCE EXAMPLE\")\n    print(\"=\" * 50)\n    # Check if model exists\n    model_path = \"models/final_model1.pt\"\n    if not os.path.exists(model_path):",
        "detail": "qwen-llm.pretraining.examples.basic.inference_example",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.examples.basic.train_example",
        "description": "qwen-llm.pretraining.examples.basic.train_example",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN TRAINING FUNCTION\n    This function demonstrates the complete pretraining pipeline.\n    \"\"\"\n    print(\"🚀 PRETRAINING EXAMPLE\")\n    print(\"=\" * 50)\n    # Set seed for reproducibility\n    set_seed(42)\n    # Create configuration",
        "detail": "qwen-llm.pretraining.examples.basic.train_example",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "kind": 6,
        "importPath": "qwen-llm.pretraining.utils.data",
        "description": "qwen-llm.pretraining.utils.data",
        "peekOfCode": "class TextTokenDataset(Dataset):\n    \"\"\"\n    📚 CUSTOM DATASET FOR LANGUAGE MODELING\n    This creates training examples for our language model:\n    🎯 What it does:\n    - Takes a long sequence of tokens\n    - Creates sliding windows of fixed length\n    - Each example: input sequence + target sequence (shifted by 1)\n    📖 Example:\n    Original text: \"The cat sat on the mat\"",
        "detail": "qwen-llm.pretraining.utils.data",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.utils.data",
        "description": "qwen-llm.pretraining.utils.data",
        "peekOfCode": "def load_and_cache_data(config, cache_dir: str = \"data_cache\"):\n    \"\"\"\n    📦 SMART DATA LOADING WITH CACHING\n    This function demonstrates modern ML data handling:\n    🎯 Key features:\n    1. Caching: Avoids reprocessing the same data\n    2. Streaming: Loads large datasets without memory issues\n    3. Tokenization: Converts text to numbers the model can understand\n    4. Efficient storage: Uses pickle for fast loading\n    🔄 The process:",
        "detail": "qwen-llm.pretraining.utils.data",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "kind": 2,
        "importPath": "qwen-llm.pretraining.utils.generation",
        "description": "qwen-llm.pretraining.utils.generation",
        "peekOfCode": "def generate_text(model, tokenizer, prompt: str, max_length: int = 100,\n                 temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9):\n    \"\"\"\n    🔮 TEXT GENERATION FUNCTION\n    This function generates text using the trained model with advanced sampling:\n    🎯 Sampling Strategies:\n    1. Temperature Scaling: Controls randomness (0.1 = focused, 2.0 = random)\n    2. Top-k Sampling: Only consider top k most likely tokens\n    3. Top-p (Nucleus) Sampling: Consider tokens until cumulative probability reaches p\n    🔍 How it works:",
        "detail": "qwen-llm.pretraining.utils.generation",
        "documentation": {}
    },
    {
        "label": "BenchmarkBeginnerLessons",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.benchmarks.benchmark_beginner",
        "description": "qwen-llm.triton_tutorials.benchmarks.benchmark_beginner",
        "peekOfCode": "class BenchmarkBeginnerLessons:\n    \"\"\"\n    📊 BENCHMARK SUITE FOR BEGINNER LESSONS\n    Benchmarks for lessons 1-3 (GPU Fundamentals, Memory Management, Basic Operations)\n    \"\"\"\n    def __init__(self, warmup_runs: int = 10, benchmark_runs: int = 100):\n        \"\"\"\n        Initialize the benchmark suite.\n        Args:\n            warmup_runs: Number of warmup runs",
        "detail": "qwen-llm.triton_tutorials.benchmarks.benchmark_beginner",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.benchmarks.benchmark_beginner",
        "description": "qwen-llm.triton_tutorials.benchmarks.benchmark_beginner",
        "peekOfCode": "def main():\n    \"\"\"Main function to run beginner lesson benchmarks.\"\"\"\n    benchmark_suite = BenchmarkBeginnerLessons()\n    benchmark_suite.run_all_benchmarks()\nif __name__ == \"__main__\":\n    main()",
        "detail": "qwen-llm.triton_tutorials.benchmarks.benchmark_beginner",
        "documentation": {}
    },
    {
        "label": "BenchmarkExamples",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.benchmarks.benchmark_examples",
        "description": "qwen-llm.triton_tutorials.benchmarks.benchmark_examples",
        "peekOfCode": "class BenchmarkExamples:\n    \"\"\"\n    📊 BENCHMARK SUITE FOR EXAMPLES\n    Benchmarks for the example implementations.\n    \"\"\"\n    def __init__(self, warmup_runs: int = 10, benchmark_runs: int = 100):\n        \"\"\"\n        Initialize the benchmark suite.\n        Args:\n            warmup_runs: Number of warmup runs",
        "detail": "qwen-llm.triton_tutorials.benchmarks.benchmark_examples",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.benchmarks.benchmark_examples",
        "description": "qwen-llm.triton_tutorials.benchmarks.benchmark_examples",
        "peekOfCode": "def main():\n    \"\"\"Main function to run example benchmarks.\"\"\"\n    benchmark_suite = BenchmarkExamples()\n    benchmark_suite.run_all_benchmarks()\nif __name__ == \"__main__\":\n    main()",
        "detail": "qwen-llm.triton_tutorials.benchmarks.benchmark_examples",
        "documentation": {}
    },
    {
        "label": "BenchmarkIntermediateLessons",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.benchmarks.benchmark_intermediate",
        "description": "qwen-llm.triton_tutorials.benchmarks.benchmark_intermediate",
        "peekOfCode": "class BenchmarkIntermediateLessons:\n    \"\"\"\n    📊 BENCHMARK SUITE FOR INTERMEDIATE LESSONS\n    Benchmarks for lessons 4-6 (Matrix Operations, Advanced Memory, Kernel Fusion)\n    \"\"\"\n    def __init__(self, warmup_runs: int = 10, benchmark_runs: int = 100):\n        \"\"\"\n        Initialize the benchmark suite.\n        Args:\n            warmup_runs: Number of warmup runs",
        "detail": "qwen-llm.triton_tutorials.benchmarks.benchmark_intermediate",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.benchmarks.benchmark_intermediate",
        "description": "qwen-llm.triton_tutorials.benchmarks.benchmark_intermediate",
        "peekOfCode": "def main():\n    \"\"\"Main function to run intermediate lesson benchmarks.\"\"\"\n    benchmark_suite = BenchmarkIntermediateLessons()\n    benchmark_suite.run_all_benchmarks()\nif __name__ == \"__main__\":\n    main()",
        "detail": "qwen-llm.triton_tutorials.benchmarks.benchmark_intermediate",
        "documentation": {}
    },
    {
        "label": "BenchmarkUtilities",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.benchmarks.benchmark_utils",
        "description": "qwen-llm.triton_tutorials.benchmarks.benchmark_utils",
        "peekOfCode": "class BenchmarkUtilities:\n    \"\"\"\n    📊 BENCHMARK SUITE FOR UTILITIES\n    Benchmarks for the utility modules.\n    \"\"\"\n    def __init__(self, warmup_runs: int = 10, benchmark_runs: int = 100):\n        \"\"\"\n        Initialize the benchmark suite.\n        Args:\n            warmup_runs: Number of warmup runs",
        "detail": "qwen-llm.triton_tutorials.benchmarks.benchmark_utils",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.benchmarks.benchmark_utils",
        "description": "qwen-llm.triton_tutorials.benchmarks.benchmark_utils",
        "peekOfCode": "def main():\n    \"\"\"Main function to run utility benchmarks.\"\"\"\n    benchmark_suite = BenchmarkUtilities()\n    benchmark_suite.run_all_benchmarks()\nif __name__ == \"__main__\":\n    main()",
        "detail": "qwen-llm.triton_tutorials.benchmarks.benchmark_utils",
        "documentation": {}
    },
    {
        "label": "basic_attention_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "description": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "peekOfCode": "def basic_attention_kernel(\n    q_ptr, k_ptr, v_ptr, output_ptr,\n    batch_size, num_heads, seq_len, head_dim,\n    stride_qb, stride_qh, stride_qs, stride_qd,\n    stride_kb, stride_kh, stride_ks, stride_kd,\n    stride_vb, stride_vh, stride_vs, stride_vd,\n    stride_ob, stride_oh, stride_os, stride_od,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"",
        "detail": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "documentation": {}
    },
    {
        "label": "basic_attention",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "description": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "peekOfCode": "def basic_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 BASIC ATTENTION WRAPPER\n    Wrapper function for basic attention mechanism.\n    \"\"\"\n    # Input validation\n    assert q.is_cuda and k.is_cuda and v.is_cuda, \"Input tensors must be on GPU!\"\n    assert q.shape == k.shape == v.shape, \"Input tensors must have the same shape!\"\n    batch_size, num_heads, seq_len, head_dim = q.shape\n    # Create output tensor",
        "detail": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "documentation": {}
    },
    {
        "label": "optimized_attention_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "description": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "peekOfCode": "def optimized_attention_kernel(\n    q_ptr, k_ptr, v_ptr, output_ptr,\n    batch_size, num_heads, seq_len, head_dim,\n    stride_qb, stride_qh, stride_qs, stride_qd,\n    stride_kb, stride_kh, stride_ks, stride_kd,\n    stride_vb, stride_vh, stride_vs, stride_vd,\n    stride_ob, stride_oh, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,",
        "detail": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "documentation": {}
    },
    {
        "label": "optimized_attention",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "description": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "peekOfCode": "def optimized_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🚀 OPTIMIZED ATTENTION WRAPPER\n    Wrapper function for optimized attention mechanism.\n    \"\"\"\n    # Input validation\n    assert q.is_cuda and k.is_cuda and v.is_cuda, \"Input tensors must be on GPU!\"\n    assert q.shape == k.shape == v.shape, \"Input tensors must have the same shape!\"\n    batch_size, num_heads, seq_len, head_dim = q.shape\n    # Create output tensor",
        "detail": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "documentation": {}
    },
    {
        "label": "flash_attention_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "description": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "peekOfCode": "def flash_attention_kernel(\n    q_ptr, k_ptr, v_ptr, output_ptr,\n    batch_size, num_heads, seq_len, head_dim,\n    stride_qb, stride_qh, stride_qs, stride_qd,\n    stride_kb, stride_kh, stride_ks, stride_kd,\n    stride_vb, stride_vh, stride_vs, stride_vd,\n    stride_ob, stride_oh, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,",
        "detail": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "documentation": {}
    },
    {
        "label": "flash_attention",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "description": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "peekOfCode": "def flash_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🔥 FLASH ATTENTION WRAPPER\n    Wrapper function for FlashAttention mechanism.\n    \"\"\"\n    # Input validation\n    assert q.is_cuda and k.is_cuda and v.is_cuda, \"Input tensors must be on GPU!\"\n    assert q.shape == k.shape == v.shape, \"Input tensors must have the same shape!\"\n    batch_size, num_heads, seq_len, head_dim = q.shape\n    # Create output tensor",
        "detail": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "documentation": {}
    },
    {
        "label": "test_attention_mechanisms",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "description": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "peekOfCode": "def test_attention_mechanisms():\n    \"\"\"\n    🧪 TEST ATTENTION MECHANISMS\n    Tests various attention mechanisms and compares performance.\n    \"\"\"\n    print(\"🧪 Testing Attention Mechanisms:\")\n    print(\"=\" * 50)\n    # Test configuration\n    batch_size, num_heads, seq_len, head_dim = 2, 8, 128, 64\n    # Create test data",
        "detail": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "documentation": {}
    },
    {
        "label": "benchmark_attention_mechanisms",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "description": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "peekOfCode": "def benchmark_attention_mechanisms():\n    \"\"\"\n    📊 BENCHMARK ATTENTION MECHANISMS\n    Benchmarks various attention mechanisms and compares performance.\n    \"\"\"\n    print(\"\\n📊 Benchmarking Attention Mechanisms:\")\n    print(\"=\" * 50)\n    # Test different configurations\n    configs = [\n        (2, 8, 128, 64),   # batch_size, num_heads, seq_len, head_dim",
        "detail": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "description": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the attention optimization examples.\n    \"\"\"\n    print(\"🎯 ATTENTION OPTIMIZATION EXAMPLES\")\n    print(\"=\" * 70)\n    print(\"This module demonstrates various attention optimization techniques.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.examples.attention_optimization",
        "documentation": {}
    },
    {
        "label": "attention_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "description": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "peekOfCode": "def attention_kernel(\n    q_ptr, k_ptr, v_ptr, output_ptr,\n    seq_len, head_dim,\n    stride_qb, stride_qh, stride_qs, stride_qd,\n    stride_kb, stride_kh, stride_ks, stride_kd,\n    stride_vb, stride_vh, stride_vs, stride_vd,\n    stride_ob, stride_oh, stride_os, stride_od,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"",
        "detail": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "optimized_attention",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "description": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "peekOfCode": "def optimized_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 OPTIMIZED ATTENTION WRAPPER\n    Wrapper function for the optimized attention kernel.\n    \"\"\"\n    batch_size, num_heads, seq_len, head_dim = q.shape\n    # Create output tensor\n    output = torch.empty_like(q)\n    # Calculate strides\n    stride_qb, stride_qh, stride_qs, stride_qd = q.stride()",
        "detail": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "transformer_matmul_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "description": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "peekOfCode": "def transformer_matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):",
        "detail": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "transformer_matmul",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "description": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "peekOfCode": "def transformer_matmul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 TRANSFORMER MATRIX MULTIPLICATION WRAPPER\n    Wrapper function for transformer matrix multiplication.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape[1] == b.shape[0], \"Inner dimensions must match!\"\n    M, K = a.shape\n    _, N = b.shape",
        "detail": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "layer_norm_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "description": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "peekOfCode": "def layer_norm_kernel(\n    input_ptr, output_ptr, weight_ptr, bias_ptr,\n    n_elements, eps,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 LAYER NORMALIZATION KERNEL\n    Implements layer normalization:\n    y = (x - mean) / sqrt(var + eps) * weight + bias\n    \"\"\"",
        "detail": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "layer_norm",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "description": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "peekOfCode": "def layer_norm(input_tensor: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n    \"\"\"\n    🎯 LAYER NORMALIZATION WRAPPER\n    Wrapper function for layer normalization.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda, \"Input tensor must be on GPU!\"\n    assert input_tensor.shape[-1] == weight.shape[0], \"Weight dimension must match input dimension!\"\n    assert input_tensor.shape[-1] == bias.shape[0], \"Bias dimension must match input dimension!\"\n    # Flatten input for kernel",
        "detail": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "test_llm_optimizations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "description": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "peekOfCode": "def test_llm_optimizations():\n    \"\"\"\n    🧪 TEST LLM OPTIMIZATION KERNELS\n    Tests the optimized kernels and validates correctness.\n    \"\"\"\n    print(\"🧪 Testing LLM Optimization Kernels:\")\n    print(\"=\" * 50)\n    # Test attention kernel\n    print(\"\\n📊 Test: Optimized Attention\")\n    batch_size, num_heads, seq_len, head_dim = 2, 8, 128, 64",
        "detail": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "benchmark_llm_optimizations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "description": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "peekOfCode": "def benchmark_llm_optimizations():\n    \"\"\"\n    📊 BENCHMARK LLM OPTIMIZATION KERNELS\n    Compares performance between Triton and PyTorch for LLM operations.\n    \"\"\"\n    print(\"\\n📊 Benchmarking LLM Optimization Kernels:\")\n    print(\"=\" * 50)\n    # Benchmark transformer matrix multiplication\n    print(\"\\n📈 Benchmark: Transformer Matrix Multiplication\")\n    sizes = [",
        "detail": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "description": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the LLM inference optimization example.\n    \"\"\"\n    print(\"🚀 LLM INFERENCE OPTIMIZATION WITH TRITON\")\n    print(\"=\" * 70)\n    print(\"This example demonstrates Triton kernels for LLM inference optimization.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.examples.llm_inference_optimization",
        "documentation": {}
    },
    {
        "label": "basic_matmul_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "description": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "peekOfCode": "def basic_matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🧠 BASIC MATRIX MULTIPLICATION KERNEL",
        "detail": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "documentation": {}
    },
    {
        "label": "basic_matmul",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "description": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "peekOfCode": "def basic_matmul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🧠 BASIC MATRIX MULTIPLICATION WRAPPER\n    Wrapper function for basic matrix multiplication.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape[1] == b.shape[0], \"Inner dimensions must match!\"\n    M, K = a.shape\n    _, N = b.shape",
        "detail": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "documentation": {}
    },
    {
        "label": "optimized_matmul_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "description": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "peekOfCode": "def optimized_matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):",
        "detail": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "documentation": {}
    },
    {
        "label": "optimized_matmul",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "description": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "peekOfCode": "def optimized_matmul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🚀 OPTIMIZED MATRIX MULTIPLICATION WRAPPER\n    Wrapper function for optimized matrix multiplication.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape[1] == b.shape[0], \"Inner dimensions must match!\"\n    M, K = a.shape\n    _, N = b.shape",
        "detail": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "documentation": {}
    },
    {
        "label": "fused_matmul_activation_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "description": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "peekOfCode": "def fused_matmul_activation_kernel(\n    a_ptr, b_ptr, output_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    activation_type: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,",
        "detail": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "documentation": {}
    },
    {
        "label": "fused_matmul_activation",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "description": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "peekOfCode": "def fused_matmul_activation(a: torch.Tensor, b: torch.Tensor, activation: str = \"relu\") -> torch.Tensor:\n    \"\"\"\n    🔥 FUSED MATRIX MULTIPLICATION + ACTIVATION WRAPPER\n    Wrapper function for fused matrix multiplication + activation.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape[1] == b.shape[0], \"Inner dimensions must match!\"\n    M, K = a.shape\n    _, N = b.shape",
        "detail": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "documentation": {}
    },
    {
        "label": "batch_matmul_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "description": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "peekOfCode": "def batch_matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    batch_size, M, N, K,\n    stride_ab, stride_am, stride_ak,\n    stride_bb, stride_bk, stride_bn,\n    stride_cb, stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):",
        "detail": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "documentation": {}
    },
    {
        "label": "batch_matmul",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "description": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "peekOfCode": "def batch_matmul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 BATCH MATRIX MULTIPLICATION WRAPPER\n    Wrapper function for batch matrix multiplication.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape[0] == b.shape[0], \"Batch sizes must match!\"\n    assert a.shape[2] == b.shape[1], \"Inner dimensions must match!\"\n    batch_size, M, K = a.shape",
        "detail": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "documentation": {}
    },
    {
        "label": "test_matrix_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "description": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "peekOfCode": "def test_matrix_operations():\n    \"\"\"\n    🧪 TEST MATRIX OPERATIONS\n    Tests various matrix operations and validates correctness.\n    \"\"\"\n    print(\"🧪 Testing Matrix Operations:\")\n    print(\"=\" * 50)\n    # Test basic matrix multiplication\n    print(\"\\n📊 Test: Basic Matrix Multiplication\")\n    M, K, N = 256, 128, 192",
        "detail": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "documentation": {}
    },
    {
        "label": "benchmark_matrix_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "description": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "peekOfCode": "def benchmark_matrix_operations():\n    \"\"\"\n    📊 BENCHMARK MATRIX OPERATIONS\n    Benchmarks various matrix operations and compares performance.\n    \"\"\"\n    print(\"\\n📊 Benchmarking Matrix Operations:\")\n    print(\"=\" * 50)\n    # Test different matrix sizes\n    sizes = [\n        (256, 256, 256),",
        "detail": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "description": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the matrix optimization examples.\n    \"\"\"\n    print(\"🔢 MATRIX OPTIMIZATION EXAMPLES\")\n    print(\"=\" * 70)\n    print(\"This module demonstrates various matrix optimization techniques.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.examples.matrix_optimization",
        "documentation": {}
    },
    {
        "label": "basic_vector_add_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "description": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "peekOfCode": "def basic_vector_add_kernel(\n    a_ptr, b_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🧠 BASIC VECTOR ADDITION KERNEL\n    Basic vector addition implementation.\n    \"\"\"\n    pid = tl.program_id(axis=0)",
        "detail": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "documentation": {}
    },
    {
        "label": "basic_vector_add",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "description": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "peekOfCode": "def basic_vector_add(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🧠 BASIC VECTOR ADDITION WRAPPER\n    Wrapper function for basic vector addition.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape == b.shape, \"Input tensors must have the same shape!\"\n    n_elements = a.numel()\n    output = torch.empty_like(a)",
        "detail": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "documentation": {}
    },
    {
        "label": "optimized_vector_add_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "description": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "peekOfCode": "def optimized_vector_add_kernel(\n    a_ptr, b_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🚀 OPTIMIZED VECTOR ADDITION KERNEL\n    Optimized vector addition with:\n    - Coalesced memory access\n    - Optimal block size",
        "detail": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "documentation": {}
    },
    {
        "label": "optimized_vector_add",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "description": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "peekOfCode": "def optimized_vector_add(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🚀 OPTIMIZED VECTOR ADDITION WRAPPER\n    Wrapper function for optimized vector addition.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape == b.shape, \"Input tensors must have the same shape!\"\n    n_elements = a.numel()\n    output = torch.empty_like(a)",
        "detail": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "documentation": {}
    },
    {
        "label": "fused_vector_operations_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "description": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "peekOfCode": "def fused_vector_operations_kernel(\n    a_ptr, b_ptr, c_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🔥 FUSED VECTOR OPERATIONS KERNEL\n    Fused vector operations:\n    output = (a + b) * c + a * 2.0\n    \"\"\"",
        "detail": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "documentation": {}
    },
    {
        "label": "fused_vector_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "description": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "peekOfCode": "def fused_vector_operations(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🔥 FUSED VECTOR OPERATIONS WRAPPER\n    Wrapper function for fused vector operations.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda and c.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape == b.shape == c.shape, \"Input tensors must have the same shape!\"\n    n_elements = a.numel()\n    output = torch.empty_like(a)",
        "detail": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "documentation": {}
    },
    {
        "label": "autotuned_vector_add_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "description": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "peekOfCode": "def autotuned_vector_add_kernel(\n    a_ptr, b_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    NUM_WARPS: tl.constexpr,\n):\n    \"\"\"\n    🎯 AUTOTUNED VECTOR ADDITION KERNEL\n    Autotuned vector addition with configurable parameters.\n    \"\"\"",
        "detail": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "documentation": {}
    },
    {
        "label": "autotuned_vector_add",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "description": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "peekOfCode": "def autotuned_vector_add(a: torch.Tensor, b: torch.Tensor, \n                        block_size: int = 256, num_warps: int = 4) -> torch.Tensor:\n    \"\"\"\n    🎯 AUTOTUNED VECTOR ADDITION WRAPPER\n    Wrapper function for autotuned vector addition.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape == b.shape, \"Input tensors must have the same shape!\"\n    n_elements = a.numel()",
        "detail": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "documentation": {}
    },
    {
        "label": "test_performance_optimizations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "description": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "peekOfCode": "def test_performance_optimizations():\n    \"\"\"\n    🧪 TEST PERFORMANCE OPTIMIZATIONS\n    Tests various performance optimization techniques.\n    \"\"\"\n    print(\"🧪 Testing Performance Optimizations:\")\n    print(\"=\" * 50)\n    # Test basic vector addition\n    print(\"\\n📊 Test: Basic Vector Addition\")\n    size = 1024",
        "detail": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "documentation": {}
    },
    {
        "label": "benchmark_performance_optimizations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "description": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "peekOfCode": "def benchmark_performance_optimizations():\n    \"\"\"\n    📊 BENCHMARK PERFORMANCE OPTIMIZATIONS\n    Benchmarks various performance optimization techniques.\n    \"\"\"\n    print(\"\\n📊 Benchmarking Performance Optimizations:\")\n    print(\"=\" * 50)\n    # Test different sizes\n    sizes = [1024, 4096, 16384, 65536, 262144]\n    for size in sizes:",
        "detail": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "documentation": {}
    },
    {
        "label": "profile_performance_optimizations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "description": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "peekOfCode": "def profile_performance_optimizations():\n    \"\"\"\n    🔍 PROFILE PERFORMANCE OPTIMIZATIONS\n    Profiles various performance optimization techniques.\n    \"\"\"\n    print(\"\\n🔍 Profiling Performance Optimizations:\")\n    print(\"=\" * 50)\n    # Create profiler\n    profiler = PerformanceProfiler()\n    # Test configuration",
        "detail": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "documentation": {}
    },
    {
        "label": "analyze_performance_optimizations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "description": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "peekOfCode": "def analyze_performance_optimizations():\n    \"\"\"\n    📈 ANALYZE PERFORMANCE OPTIMIZATIONS\n    Analyzes various performance optimization techniques.\n    \"\"\"\n    print(\"\\n📈 Analyzing Performance Optimizations:\")\n    print(\"=\" * 50)\n    # Create analyzer\n    analyzer = PerformanceAnalyzer()\n    # Test different sizes",
        "detail": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "description": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the performance optimization examples.\n    \"\"\"\n    print(\"🚀 PERFORMANCE OPTIMIZATION EXAMPLES\")\n    print(\"=\" * 70)\n    print(\"This module demonstrates various performance optimization techniques.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.examples.performance_optimization",
        "documentation": {}
    },
    {
        "label": "explain_attention_mechanisms",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "peekOfCode": "def explain_attention_mechanisms():\n    \"\"\"\n    📚 ATTENTION MECHANISMS FUNDAMENTALS\n    Advanced attention mechanisms and optimization techniques.\n    \"\"\"\n    print(\"🧠 Advanced Attention Mechanisms:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🎯 Attention Mechanism Components:\n    1. Query (Q): What information are we looking for?",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "documentation": {}
    },
    {
        "label": "advanced_attention_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "peekOfCode": "def advanced_attention_kernel(\n    q_ptr, k_ptr, v_ptr, output_ptr,\n    batch_size, num_heads, seq_len, head_dim,\n    stride_qb, stride_qh, stride_qs, stride_qd,\n    stride_kb, stride_kh, stride_ks, stride_kd,\n    stride_vb, stride_vh, stride_vs, stride_vd,\n    stride_ob, stride_oh, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "documentation": {}
    },
    {
        "label": "advanced_attention",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "peekOfCode": "def advanced_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🚀 ADVANCED ATTENTION WRAPPER\n    Wrapper function for advanced attention mechanism.\n    \"\"\"\n    # Input validation\n    assert q.is_cuda and k.is_cuda and v.is_cuda, \"Input tensors must be on GPU!\"\n    assert q.shape == k.shape == v.shape, \"Input tensors must have the same shape!\"\n    batch_size, num_heads, seq_len, head_dim = q.shape\n    # Create output tensor",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "documentation": {}
    },
    {
        "label": "flash_attention_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "peekOfCode": "def flash_attention_kernel(\n    q_ptr, k_ptr, v_ptr, output_ptr,\n    batch_size, num_heads, seq_len, head_dim,\n    stride_qb, stride_qh, stride_qs, stride_qd,\n    stride_kb, stride_kh, stride_ks, stride_kd,\n    stride_vb, stride_vh, stride_vs, stride_vd,\n    stride_ob, stride_oh, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "documentation": {}
    },
    {
        "label": "flash_attention",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "peekOfCode": "def flash_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🔥 FLASH ATTENTION WRAPPER\n    Wrapper function for FlashAttention mechanism.\n    \"\"\"\n    # Input validation\n    assert q.is_cuda and k.is_cuda and v.is_cuda, \"Input tensors must be on GPU!\"\n    assert q.shape == k.shape == v.shape, \"Input tensors must have the same shape!\"\n    batch_size, num_heads, seq_len, head_dim = q.shape\n    # Create output tensor",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "documentation": {}
    },
    {
        "label": "causal_attention_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "peekOfCode": "def causal_attention_kernel(\n    q_ptr, k_ptr, v_ptr, output_ptr,\n    batch_size, num_heads, seq_len, head_dim,\n    stride_qb, stride_qh, stride_qs, stride_qd,\n    stride_kb, stride_kh, stride_ks, stride_kd,\n    stride_vb, stride_vh, stride_vs, stride_vd,\n    stride_ob, stride_oh, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "documentation": {}
    },
    {
        "label": "causal_attention",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "peekOfCode": "def causal_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 CAUSAL ATTENTION WRAPPER\n    Wrapper function for causal attention mechanism.\n    \"\"\"\n    # Input validation\n    assert q.is_cuda and k.is_cuda and v.is_cuda, \"Input tensors must be on GPU!\"\n    assert q.shape == k.shape == v.shape, \"Input tensors must have the same shape!\"\n    batch_size, num_heads, seq_len, head_dim = q.shape\n    # Create output tensor",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "documentation": {}
    },
    {
        "label": "test_attention_mechanisms",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "peekOfCode": "def test_attention_mechanisms():\n    \"\"\"\n    🧪 TEST ATTENTION MECHANISMS\n    Tests various attention mechanisms and validates correctness.\n    \"\"\"\n    print(\"🧪 Testing Attention Mechanisms:\")\n    print(\"=\" * 50)\n    # Test configuration\n    batch_size, num_heads, seq_len, head_dim = 2, 8, 128, 64\n    # Create test data",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "documentation": {}
    },
    {
        "label": "benchmark_attention_mechanisms",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "peekOfCode": "def benchmark_attention_mechanisms():\n    \"\"\"\n    📊 BENCHMARK ATTENTION MECHANISMS\n    Benchmarks various attention mechanisms and compares performance.\n    \"\"\"\n    print(\"\\n📊 Benchmarking Attention Mechanisms:\")\n    print(\"=\" * 50)\n    # Test different configurations\n    configs = [\n        (2, 8, 128, 64),   # batch_size, num_heads, seq_len, head_dim",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the complete lesson 7 tutorial.\n    \"\"\"\n    print(\"🚀 LESSON 7: ATTENTION MECHANISMS & FLASHATTENTION\")\n    print(\"=\" * 70)\n    print(\"This lesson covers advanced attention mechanisms and optimization techniques.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_07_attention",
        "documentation": {}
    },
    {
        "label": "explain_moe_architecture",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "peekOfCode": "def explain_moe_architecture():\n    \"\"\"\n    📚 MOE ARCHITECTURE FUNDAMENTALS\n    Mixture of Experts architecture and optimization techniques.\n    \"\"\"\n    print(\"🧠 MoE Architecture Fundamentals:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🎯 MoE Components:\n    1. Router: Determines which experts to use",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "documentation": {}
    },
    {
        "label": "expert_routing_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "peekOfCode": "def expert_routing_kernel(\n    input_ptr, router_weights_ptr, expert_indices_ptr, expert_weights_ptr,\n    batch_size, seq_len, hidden_dim, num_experts, top_k,\n    stride_ib, stride_is, stride_id,\n    stride_rb, stride_rs, stride_rd,\n    stride_eib, stride_eis, stride_eik,\n    stride_ewb, stride_ews, stride_ewk,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "documentation": {}
    },
    {
        "label": "expert_routing",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "peekOfCode": "def expert_routing(input_tensor: torch.Tensor, router_weights: torch.Tensor, \n                  num_experts: int, top_k: int = 2) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    🎯 EXPERT ROUTING WRAPPER\n    Wrapper function for expert routing.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda and router_weights.is_cuda, \"Input tensors must be on GPU!\"\n    assert input_tensor.shape[2] == router_weights.shape[1], \"Hidden dimensions must match!\"\n    batch_size, seq_len, hidden_dim = input_tensor.shape",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "documentation": {}
    },
    {
        "label": "expert_computation_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "peekOfCode": "def expert_computation_kernel(\n    input_ptr, expert_weights_ptr, expert_output_ptr,\n    batch_size, seq_len, hidden_dim, num_experts, top_k,\n    stride_ib, stride_is, stride_id,\n    stride_ewb, stride_ews, stride_ewk,\n    stride_eob, stride_eos, stride_eod,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🚀 EXPERT COMPUTATION KERNEL",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "documentation": {}
    },
    {
        "label": "expert_computation",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "peekOfCode": "def expert_computation(input_tensor: torch.Tensor, expert_weights: torch.Tensor, \n                      num_experts: int, top_k: int = 2) -> torch.Tensor:\n    \"\"\"\n    🚀 EXPERT COMPUTATION WRAPPER\n    Wrapper function for expert computation.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda and expert_weights.is_cuda, \"Input tensors must be on GPU!\"\n    assert input_tensor.shape[0] == expert_weights.shape[0], \"Batch sizes must match!\"\n    assert input_tensor.shape[1] == expert_weights.shape[1], \"Sequence lengths must match!\"",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "documentation": {}
    },
    {
        "label": "load_balancing_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "peekOfCode": "def load_balancing_kernel(\n    expert_indices_ptr, expert_weights_ptr, load_balance_loss_ptr,\n    batch_size, seq_len, num_experts, top_k,\n    stride_eib, stride_eis, stride_eik,\n    stride_ewb, stride_ews, stride_ewk,\n    stride_lbb, stride_lbs,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🔥 LOAD BALANCING KERNEL",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "documentation": {}
    },
    {
        "label": "load_balancing",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "peekOfCode": "def load_balancing(expert_indices: torch.Tensor, expert_weights: torch.Tensor, \n                  num_experts: int, top_k: int = 2) -> torch.Tensor:\n    \"\"\"\n    🔥 LOAD BALANCING WRAPPER\n    Wrapper function for load balancing.\n    \"\"\"\n    # Input validation\n    assert expert_indices.is_cuda and expert_weights.is_cuda, \"Input tensors must be on GPU!\"\n    assert expert_indices.shape == expert_weights.shape, \"Expert indices and weights must have the same shape!\"\n    batch_size, seq_len, _ = expert_indices.shape",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "documentation": {}
    },
    {
        "label": "moe_layer",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "peekOfCode": "def moe_layer(input_tensor: torch.Tensor, router_weights: torch.Tensor, \n              num_experts: int, top_k: int = 2) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    🎯 COMPLETE MOE LAYER\n    Complete MoE layer implementation with:\n    - Expert routing\n    - Expert computation\n    - Load balancing\n    \"\"\"\n    # Input validation",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "documentation": {}
    },
    {
        "label": "test_moe_implementation",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "peekOfCode": "def test_moe_implementation():\n    \"\"\"\n    🧪 TEST MOE IMPLEMENTATION\n    Tests MoE implementation and validates correctness.\n    \"\"\"\n    print(\"🧪 Testing MoE Implementation:\")\n    print(\"=\" * 50)\n    # Test configuration\n    batch_size, seq_len, hidden_dim = 2, 128, 512\n    num_experts = 8",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "documentation": {}
    },
    {
        "label": "benchmark_moe_implementation",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "peekOfCode": "def benchmark_moe_implementation():\n    \"\"\"\n    📊 BENCHMARK MOE IMPLEMENTATION\n    Benchmarks MoE implementation and compares performance.\n    \"\"\"\n    print(\"\\n📊 Benchmarking MoE Implementation:\")\n    print(\"=\" * 50)\n    # Test different configurations\n    configs = [\n        (2, 128, 512, 8, 2),   # batch_size, seq_len, hidden_dim, num_experts, top_k",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the complete lesson 8 tutorial.\n    \"\"\"\n    print(\"🚀 LESSON 8: MOE (MIXTURE OF EXPERTS) IMPLEMENTATION\")\n    print(\"=\" * 70)\n    print(\"This lesson covers MoE architecture and optimization techniques.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_08_moe",
        "documentation": {}
    },
    {
        "label": "explain_advanced_optimizations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "peekOfCode": "def explain_advanced_optimizations():\n    \"\"\"\n    📚 ADVANCED OPTIMIZATION TECHNIQUES\n    Advanced optimization techniques and best practices.\n    \"\"\"\n    print(\"🧠 Advanced Optimization Techniques:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🎯 Advanced Optimization Areas:\n    1. Autotuning: Automatic kernel parameter optimization",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "documentation": {}
    },
    {
        "label": "autotuned_vector_add_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "peekOfCode": "def autotuned_vector_add_kernel(\n    a_ptr, b_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🔧 AUTOTUNED VECTOR ADDITION KERNEL\n    Implements autotuned vector addition with:\n    - Automatic block size optimization\n    - Memory access pattern tuning",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "documentation": {}
    },
    {
        "label": "autotuned_vector_add",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "peekOfCode": "def autotuned_vector_add(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🔧 AUTOTUNED VECTOR ADDITION WRAPPER\n    Wrapper function with autotuning capabilities.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape == b.shape, \"Input tensors must have the same shape!\"\n    n_elements = a.numel()\n    # Create output tensor",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "documentation": {}
    },
    {
        "label": "production_optimized_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "peekOfCode": "def production_optimized_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    batch_size, seq_len, hidden_dim,\n    stride_ib, stride_is, stride_id,\n    stride_wb, stride_ws, stride_wd,\n    stride_bb, stride_bs, stride_bd,\n    stride_ob, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "documentation": {}
    },
    {
        "label": "production_optimized_layer",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "peekOfCode": "def production_optimized_layer(input_tensor: torch.Tensor, weight: torch.Tensor, \n                              bias: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🚀 PRODUCTION OPTIMIZED LAYER\n    Wrapper function for production-optimized layer.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda and weight.is_cuda and bias.is_cuda, \"Input tensors must be on GPU!\"\n    assert input_tensor.shape[2] == weight.shape[1], \"Hidden dimensions must match!\"\n    batch_size, seq_len, hidden_dim = input_tensor.shape",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "documentation": {}
    },
    {
        "label": "scalable_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "peekOfCode": "def scalable_kernel(\n    input_ptr, output_ptr,\n    batch_size, seq_len, hidden_dim,\n    stride_ib, stride_is, stride_id,\n    stride_ob, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"\n    🎯 SCALABLE KERNEL",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "documentation": {}
    },
    {
        "label": "scalable_computation",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "peekOfCode": "def scalable_computation(input_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 SCALABLE COMPUTATION\n    Wrapper function for scalable computation.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda, \"Input tensor must be on GPU!\"\n    batch_size, seq_len, hidden_dim = input_tensor.shape\n    # Create output tensor\n    output = torch.empty_like(input_tensor)",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "documentation": {}
    },
    {
        "label": "profile_kernel_performance",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "peekOfCode": "def profile_kernel_performance(kernel_func, *args, **kwargs):\n    \"\"\"\n    📊 PROFILE KERNEL PERFORMANCE\n    Profiles kernel performance and provides detailed analysis.\n    \"\"\"\n    print(\"📊 Kernel Performance Profiling:\")\n    print(\"=\" * 50)\n    # Warmup\n    for _ in range(10):\n        _ = kernel_func(*args, **kwargs)",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "documentation": {}
    },
    {
        "label": "test_advanced_optimizations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "peekOfCode": "def test_advanced_optimizations():\n    \"\"\"\n    🧪 TEST ADVANCED OPTIMIZATIONS\n    Tests advanced optimization techniques and validates correctness.\n    \"\"\"\n    print(\"🧪 Testing Advanced Optimizations:\")\n    print(\"=\" * 50)\n    # Test configuration\n    batch_size, seq_len, hidden_dim = 2, 128, 512\n    # Create test data",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "documentation": {}
    },
    {
        "label": "benchmark_advanced_optimizations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "peekOfCode": "def benchmark_advanced_optimizations():\n    \"\"\"\n    📊 BENCHMARK ADVANCED OPTIMIZATIONS\n    Benchmarks advanced optimization techniques and compares performance.\n    \"\"\"\n    print(\"\\n📊 Benchmarking Advanced Optimizations:\")\n    print(\"=\" * 50)\n    # Test different configurations\n    configs = [\n        (2, 128, 512),   # batch_size, seq_len, hidden_dim",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "description": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the complete lesson 9 tutorial.\n    \"\"\"\n    print(\"🚀 LESSON 9: ADVANCED OPTIMIZATION TECHNIQUES\")\n    print(\"=\" * 70)\n    print(\"This lesson covers advanced optimization techniques and best practices.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.lessons.advanced.lesson_09_advanced_optimizations",
        "documentation": {}
    },
    {
        "label": "explain_gpu_architecture",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "peekOfCode": "def explain_gpu_architecture():\n    \"\"\"\n    📚 EXPLAINING GPU ARCHITECTURE\n    GPUs are massively parallel processors designed for:\n    - High throughput (many operations per second)\n    - High memory bandwidth (fast data access)\n    - Parallel execution (thousands of threads)\n    Key Components:\n    1. Streaming Multiprocessors (SMs) - Compute units\n    2. CUDA Cores - Individual processing units",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "documentation": {}
    },
    {
        "label": "explain_memory_hierarchy",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "peekOfCode": "def explain_memory_hierarchy():\n    \"\"\"\n    📚 MEMORY HIERARCHY EXPLANATION\n    Understanding memory hierarchy is crucial for writing efficient kernels:\n    1. Registers: Fastest, private to each thread\n    2. Shared Memory: Fast, shared among threads in a block\n    3. L1 Cache: Fast, shared among threads in an SM\n    4. L2 Cache: Medium speed, shared across SMs\n    5. Global Memory: Slowest, accessible by all threads\n    Key Principles:",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "documentation": {}
    },
    {
        "label": "explain_triton_concepts",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "peekOfCode": "def explain_triton_concepts():\n    \"\"\"\n    📚 TRITON LANGUAGE CONCEPTS\n    Triton is a Python-like language for writing CUDA kernels:\n    Key Concepts:\n    1. @triton.jit decorator - Marks functions as kernels\n    2. tl.constexpr - Compile-time constants\n    3. tl.program_id() - Gets the current program/block ID\n    4. tl.arange() - Creates ranges of indices\n    5. tl.load() / tl.store() - Memory operations",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "documentation": {}
    },
    {
        "label": "vector_add_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "peekOfCode": "def vector_add_kernel(\n    # Input and output pointers\n    x_ptr,           # Pointer to first input vector\n    y_ptr,           # Pointer to second input vector  \n    output_ptr,      # Pointer to output vector\n    # Vector size\n    n_elements,      # Total number of elements\n    # Strides (how much to increment pointer for next element)\n    x_stride: tl.constexpr,      # Stride for x vector\n    y_stride: tl.constexpr,      # Stride for y vector",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "documentation": {}
    },
    {
        "label": "vector_add",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "peekOfCode": "def vector_add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 VECTOR ADDITION WRAPPER\n    This function:\n    1. Validates inputs\n    2. Creates output tensor\n    3. Launches the kernel with appropriate grid size\n    4. Returns the result\n    Args:\n        x: First input vector (must be on GPU)",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "documentation": {}
    },
    {
        "label": "test_vector_addition",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "peekOfCode": "def test_vector_addition():\n    \"\"\"\n    🧪 TEST VECTOR ADDITION KERNEL\n    Tests the kernel with various input sizes and validates correctness.\n    \"\"\"\n    print(\"\\n🧪 Testing Vector Addition Kernel:\")\n    print(\"=\" * 50)\n    test_cases = [\n        (1024, \"Power of 2 size\"),\n        (1000, \"Non-power of 2 size\"),",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "documentation": {}
    },
    {
        "label": "benchmark_vector_addition",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "peekOfCode": "def benchmark_vector_addition():\n    \"\"\"\n    📊 BENCHMARK VECTOR ADDITION\n    Compares performance between Triton kernel and PyTorch.\n    \"\"\"\n    print(\"\\n📊 Benchmarking Vector Addition:\")\n    print(\"=\" * 50)\n    sizes = [1024, 4096, 16384, 65536, 262144]\n    for size in sizes:\n        print(f\"\\n📈 Size: {size:,} elements\")",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "documentation": {}
    },
    {
        "label": "explain_program_ids",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "peekOfCode": "def explain_program_ids():\n    \"\"\"\n    📚 UNDERSTANDING PROGRAM IDS AND BLOCKS\n    This is crucial for understanding how parallel execution works in Triton.\n    \"\"\"\n    print(\"\\n🎓 Understanding Program IDs and Blocks:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🧠 Key Concepts:\n    1. Grid: The entire collection of blocks that execute your kernel",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the complete lesson 1 tutorial.\n    \"\"\"\n    print(\"🎯 LESSON 1: GPU FUNDAMENTALS & TRITON BASICS\")\n    print(\"=\" * 70)\n    print(\"Welcome to your first Triton tutorial!\")\n    print(\"This lesson will teach you the fundamentals of GPU programming with Triton.\")\n    # Check CUDA availability",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_01_gpu_fundamentals",
        "documentation": {}
    },
    {
        "label": "explain_memory_coalescing",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def explain_memory_coalescing():\n    \"\"\"\n    📚 MEMORY COALESCING EXPLANATION\n    Memory coalescing is crucial for achieving high memory bandwidth:\n    ✅ Coalesced Access (Good):\n    - Threads access consecutive memory locations\n    - GPU can combine multiple requests into one\n    - Achieves near-peak memory bandwidth\n    ❌ Non-Coalesced Access (Bad):\n    - Threads access scattered memory locations",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "coalesced_access_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def coalesced_access_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    stride: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 COALESCED MEMORY ACCESS KERNEL\n    Demonstrates proper memory coalescing by accessing consecutive elements.",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "non_coalesced_access_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def non_coalesced_access_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    stride: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 NON-COALESCED MEMORY ACCESS KERNEL\n    Demonstrates poor memory access patterns (for comparison).",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "benchmark_memory_access",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def benchmark_memory_access():\n    \"\"\"\n    📊 BENCHMARK COALESCED VS NON-COALESCED ACCESS\n    Compares performance between coalesced and non-coalesced memory access.\n    \"\"\"\n    print(\"\\n📊 Benchmarking Memory Access Patterns:\")\n    print(\"=\" * 50)\n    sizes = [1024, 4096, 16384, 65536]\n    for size in sizes:\n        print(f\"\\n📈 Size: {size:,} elements\")",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "explain_data_types",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def explain_data_types():\n    \"\"\"\n    📚 DATA TYPES IN TRITON\n    Different data types have different characteristics:\n    - Memory usage\n    - Precision\n    - Performance\n    - Hardware support\n    \"\"\"\n    print(\"\\n🎯 Data Types in Triton:\")",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "data_type_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def data_type_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 DATA TYPE DEMONSTRATION KERNEL\n    Shows how to work with different data types in Triton.\n    \"\"\"",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "test_data_types",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def test_data_types():\n    \"\"\"\n    🧪 TEST DIFFERENT DATA TYPES\n    Tests the kernel with different data types and compares performance.\n    \"\"\"\n    print(\"\\n🧪 Testing Different Data Types:\")\n    print(\"=\" * 50)\n    data_types = [\n        (torch.float32, \"float32\"),\n        (torch.float16, \"float16\"),",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "explain_strides",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def explain_strides():\n    \"\"\"\n    📚 UNDERSTANDING STRIDES\n    Strides determine how to access elements in multi-dimensional tensors:\n    - stride[i] = number of elements to skip to get to next element in dimension i\n    - Contiguous tensors have predictable stride patterns\n    - Non-contiguous tensors require careful stride handling\n    \"\"\"\n    print(\"\\n🎯 Understanding Strides:\")\n    print(\"=\" * 50)",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "stride_aware_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def stride_aware_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    input_stride: tl.constexpr,\n    output_stride: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 STRIDE-AWARE KERNEL",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "test_strides",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def test_strides():\n    \"\"\"\n    🧪 TEST DIFFERENT STRIDE PATTERNS\n    Tests the kernel with different stride patterns.\n    \"\"\"\n    print(\"\\n🧪 Testing Stride Patterns:\")\n    print(\"=\" * 50)\n    # Test 1: Contiguous tensor\n    print(\"\\n📊 Test 1: Contiguous Tensor\")\n    x = torch.randn(1024, device='cuda')",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "explain_memory_bandwidth",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def explain_memory_bandwidth():\n    \"\"\"\n    📚 MEMORY BANDWIDTH OPTIMIZATION\n    Key strategies for maximizing memory bandwidth:\n    1. Use appropriate data types\n    2. Ensure memory coalescing\n    3. Minimize memory transactions\n    4. Use shared memory when beneficial\n    5. Consider memory alignment\n    \"\"\"",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "bandwidth_test_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def bandwidth_test_kernel(\n    input_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 BANDWIDTH TEST KERNEL\n    Simple kernel to test memory bandwidth.\n    \"\"\"",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "measure_memory_bandwidth",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def measure_memory_bandwidth():\n    \"\"\"\n    📊 MEASURE MEMORY BANDWIDTH\n    Measures achieved memory bandwidth for different configurations.\n    \"\"\"\n    print(\"\\n📊 Measuring Memory Bandwidth:\")\n    print(\"=\" * 50)\n    sizes = [1024, 4096, 16384, 65536, 262144, 1048576]\n    for size in sizes:\n        print(f\"\\n📈 Size: {size:,} elements\")",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the complete lesson 2 tutorial.\n    \"\"\"\n    print(\"🎯 LESSON 2: MEMORY MANAGEMENT & DATA TYPES\")\n    print(\"=\" * 70)\n    print(\"This lesson covers memory optimization techniques in Triton.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_02_memory_management",
        "documentation": {}
    },
    {
        "label": "explain_element_wise_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def explain_element_wise_operations():\n    \"\"\"\n    📚 ELEMENT-WISE OPERATIONS\n    Element-wise operations are fundamental building blocks for more complex kernels.\n    They operate on corresponding elements of tensors.\n    \"\"\"\n    print(\"🎯 Element-wise Operations:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🧠 What are Element-wise Operations?",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "element_wise_add_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def element_wise_add_kernel(\n    a_ptr, b_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 ELEMENT-WISE ADDITION KERNEL\n    Adds corresponding elements of two tensors.\n    \"\"\"\n    pid = tl.program_id(axis=0)",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "element_wise_multiply_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def element_wise_multiply_kernel(\n    a_ptr, b_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 ELEMENT-WISE MULTIPLICATION KERNEL\n    Multiplies corresponding elements of two tensors.\n    \"\"\"\n    pid = tl.program_id(axis=0)",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "element_wise_activation_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def element_wise_activation_kernel(\n    input_ptr, output_ptr,\n    n_elements,\n    activation_type: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 ELEMENT-WISE ACTIVATION KERNEL\n    Applies activation functions element-wise.\n    \"\"\"",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "test_element_wise_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def test_element_wise_operations():\n    \"\"\"\n    🧪 TEST ELEMENT-WISE OPERATIONS\n    Tests various element-wise operations and validates correctness.\n    \"\"\"\n    print(\"\\n🧪 Testing Element-wise Operations:\")\n    print(\"=\" * 50)\n    size = 1024\n    a = torch.randn(size, device='cuda', dtype=torch.float32)\n    b = torch.randn(size, device='cuda', dtype=torch.float32)",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "explain_reduction_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def explain_reduction_operations():\n    \"\"\"\n    📚 REDUCTION OPERATIONS\n    Reduction operations combine multiple elements into a single result.\n    They are more complex than element-wise operations because they require\n    communication between threads.\n    \"\"\"\n    print(\"\\n🎯 Reduction Operations:\")\n    print(\"=\" * 50)\n    print(\"\"\"",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "sum_reduction_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def sum_reduction_kernel(\n    input_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 SUM REDUCTION KERNEL\n    Computes the sum of all elements in the input tensor.\n    \"\"\"\n    pid = tl.program_id(axis=0)",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "max_reduction_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def max_reduction_kernel(\n    input_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 MAX REDUCTION KERNEL\n    Finds the maximum value in the input tensor.\n    \"\"\"\n    pid = tl.program_id(axis=0)",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "mean_reduction_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def mean_reduction_kernel(\n    input_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 MEAN REDUCTION KERNEL\n    Computes the mean of all elements in the input tensor.\n    \"\"\"\n    pid = tl.program_id(axis=0)",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "test_reduction_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def test_reduction_operations():\n    \"\"\"\n    🧪 TEST REDUCTION OPERATIONS\n    Tests various reduction operations and validates correctness.\n    \"\"\"\n    print(\"\\n🧪 Testing Reduction Operations:\")\n    print(\"=\" * 50)\n    size = 1024\n    x = torch.randn(size, device='cuda', dtype=torch.float32)\n    # Test sum reduction",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "explain_broadcasting",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def explain_broadcasting():\n    \"\"\"\n    📚 BROADCASTING IN TRITON\n    Broadcasting allows operations between tensors of different shapes.\n    It's a powerful feature that enables efficient operations.\n    \"\"\"\n    print(\"\\n🎯 Broadcasting in Triton:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🧠 What is Broadcasting?",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "broadcasting_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def broadcasting_kernel(\n    a_ptr, b_ptr, output_ptr,\n    M, N,\n    stride_am, stride_an,\n    stride_bm, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "test_broadcasting",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def test_broadcasting():\n    \"\"\"\n    🧪 TEST BROADCASTING\n    Tests broadcasting operations and validates correctness.\n    \"\"\"\n    print(\"\\n🧪 Testing Broadcasting:\")\n    print(\"=\" * 50)\n    # Test case 1: [3, 1] + [1, 4] = [3, 4]\n    print(\"\\n📊 Test: [3, 1] + [1, 4] = [3, 4]\")\n    a = torch.randn(3, 1, device='cuda', dtype=torch.float32)",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "explain_error_handling",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def explain_error_handling():\n    \"\"\"\n    📚 ERROR HANDLING AND DEBUGGING\n    Debugging Triton kernels can be challenging. Here are some strategies:\n    \"\"\"\n    print(\"\\n🎯 Error Handling and Debugging:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🐛 Common Debugging Strategies:\n    1. Validate Inputs:",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "debug_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def debug_kernel(\n    input_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 DEBUG KERNEL\n    Demonstrates proper error handling and debugging techniques.\n    \"\"\"\n    pid = tl.program_id(axis=0)",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "test_error_handling",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def test_error_handling():\n    \"\"\"\n    🧪 TEST ERROR HANDLING\n    Tests error handling and edge cases.\n    \"\"\"\n    print(\"\\n🧪 Testing Error Handling:\")\n    print(\"=\" * 50)\n    # Test with edge cases\n    test_cases = [\n        (1, \"Single element\"),",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "explain_performance_optimization",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def explain_performance_optimization():\n    \"\"\"\n    📚 PERFORMANCE OPTIMIZATION\n    Key strategies for optimizing basic operations:\n    \"\"\"\n    print(\"\\n🎯 Performance Optimization:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🚀 Optimization Strategies:\n    1. Block Size Tuning:",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "benchmark_basic_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def benchmark_basic_operations():\n    \"\"\"\n    📊 BENCHMARK BASIC OPERATIONS\n    Compares performance between Triton and PyTorch for basic operations.\n    \"\"\"\n    print(\"\\n📊 Benchmarking Basic Operations:\")\n    print(\"=\" * 50)\n    sizes = [1024, 4096, 16384, 65536]\n    for size in sizes:\n        print(f\"\\n📈 Size: {size:,} elements\")",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "description": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the complete lesson 3 tutorial.\n    \"\"\"\n    print(\"🎯 LESSON 3: BASIC OPERATIONS & KERNELS\")\n    print(\"=\" * 70)\n    print(\"This lesson covers basic operations and kernel development in Triton.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.lessons.beginner.lesson_03_basic_operations",
        "documentation": {}
    },
    {
        "label": "explain_custom_cuda_integration",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "peekOfCode": "def explain_custom_cuda_integration():\n    \"\"\"\n    📚 CUSTOM CUDA KERNELS & TRITON INTEGRATION\n    Custom CUDA kernel development and Triton integration techniques.\n    \"\"\"\n    print(\"🧠 Custom CUDA Kernels & Triton Integration:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🎯 Custom CUDA Development:\n    1. CUDA C++ Kernel Development:",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "documentation": {}
    },
    {
        "label": "create_custom_cuda_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "peekOfCode": "def create_custom_cuda_kernel():\n    \"\"\"\n    🔧 CREATE CUSTOM CUDA KERNEL\n    Creates a custom CUDA kernel for demonstration purposes.\n    \"\"\"\n    # CUDA kernel source code\n    cuda_kernel_source = \"\"\"\n    extern \"C\" __global__ void custom_vector_add(\n        const float* a,\n        const float* b,",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "documentation": {}
    },
    {
        "label": "triton_cuda_integration_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "peekOfCode": "def triton_cuda_integration_kernel(\n    a_ptr, b_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🔧 TRITON-CUDA INTEGRATION KERNEL\n    Implements Triton-CUDA integration with:\n    - Hybrid kernel architecture\n    - Performance optimization",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "documentation": {}
    },
    {
        "label": "triton_cuda_integration",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "peekOfCode": "def triton_cuda_integration(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🔧 TRITON-CUDA INTEGRATION WRAPPER\n    Wrapper function for Triton-CUDA integration.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape == b.shape, \"Input tensors must have the same shape!\"\n    n_elements = a.numel()\n    # Create output tensor",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "documentation": {}
    },
    {
        "label": "hybrid_kernel_architecture",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "peekOfCode": "def hybrid_kernel_architecture(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    batch_size, seq_len, hidden_dim,\n    stride_ib, stride_is, stride_id,\n    stride_wb, stride_ws, stride_wd,\n    stride_bb, stride_bs, stride_bd,\n    stride_ob, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "documentation": {}
    },
    {
        "label": "hybrid_kernel_layer",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "peekOfCode": "def hybrid_kernel_layer(input_tensor: torch.Tensor, weight: torch.Tensor, \n                       bias: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🚀 HYBRID KERNEL LAYER\n    Wrapper function for hybrid kernel layer.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda and weight.is_cuda and bias.is_cuda, \"Input tensors must be on GPU!\"\n    assert input_tensor.shape[2] == weight.shape[1], \"Hidden dimensions must match!\"\n    batch_size, seq_len, hidden_dim = input_tensor.shape",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "documentation": {}
    },
    {
        "label": "performance_optimized_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "peekOfCode": "def performance_optimized_kernel(\n    input_ptr, output_ptr,\n    batch_size, seq_len, hidden_dim,\n    stride_ib, stride_is, stride_id,\n    stride_ob, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"\n    🎯 PERFORMANCE OPTIMIZED KERNEL",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "documentation": {}
    },
    {
        "label": "performance_optimized_layer",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "peekOfCode": "def performance_optimized_layer(input_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 PERFORMANCE OPTIMIZED LAYER\n    Wrapper function for performance optimized layer.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda, \"Input tensor must be on GPU!\"\n    batch_size, seq_len, hidden_dim = input_tensor.shape\n    # Create output tensor\n    output = torch.empty_like(input_tensor)",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "documentation": {}
    },
    {
        "label": "memory_optimized_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "peekOfCode": "def memory_optimized_kernel(\n    input_ptr, output_ptr,\n    batch_size, seq_len, hidden_dim,\n    stride_ib, stride_is, stride_id,\n    stride_ob, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"\n    🔥 MEMORY OPTIMIZED KERNEL",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "documentation": {}
    },
    {
        "label": "memory_optimized_layer",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "peekOfCode": "def memory_optimized_layer(input_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🔥 MEMORY OPTIMIZED LAYER\n    Wrapper function for memory optimized layer.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda, \"Input tensor must be on GPU!\"\n    batch_size, seq_len, hidden_dim = input_tensor.shape\n    # Create output tensor\n    output = torch.empty_like(input_tensor)",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "documentation": {}
    },
    {
        "label": "test_custom_cuda_integration",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "peekOfCode": "def test_custom_cuda_integration():\n    \"\"\"\n    🧪 TEST CUSTOM CUDA INTEGRATION\n    Tests custom CUDA integration and validates correctness.\n    \"\"\"\n    print(\"🧪 Testing Custom CUDA Integration:\")\n    print(\"=\" * 50)\n    # Test configuration\n    batch_size, seq_len, hidden_dim = 2, 128, 512\n    # Create test data",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "documentation": {}
    },
    {
        "label": "benchmark_custom_cuda_integration",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "peekOfCode": "def benchmark_custom_cuda_integration():\n    \"\"\"\n    📊 BENCHMARK CUSTOM CUDA INTEGRATION\n    Benchmarks custom CUDA integration and compares performance.\n    \"\"\"\n    print(\"\\n📊 Benchmarking Custom CUDA Integration:\")\n    print(\"=\" * 50)\n    # Test different configurations\n    configs = [\n        (2, 128, 512),   # batch_size, seq_len, hidden_dim",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the complete lesson 10 tutorial.\n    \"\"\"\n    print(\"🚀 LESSON 10: CUSTOM CUDA KERNELS & TRITON INTEGRATION\")\n    print(\"=\" * 70)\n    print(\"This lesson covers custom CUDA kernel development and Triton integration.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_10_custom_cuda",
        "documentation": {}
    },
    {
        "label": "explain_multi_gpu_distributed",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "peekOfCode": "def explain_multi_gpu_distributed():\n    \"\"\"\n    📚 MULTI-GPU & DISTRIBUTED COMPUTING\n    Multi-GPU and distributed computing fundamentals.\n    \"\"\"\n    print(\"🧠 Multi-GPU & Distributed Computing:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🎯 Multi-GPU Computing:\n    1. Multi-GPU Architecture:",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "documentation": {}
    },
    {
        "label": "multi_gpu_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "peekOfCode": "def multi_gpu_kernel(\n    input_ptr, output_ptr,\n    batch_size, seq_len, hidden_dim,\n    stride_ib, stride_is, stride_id,\n    stride_ob, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"\n    🔧 MULTI-GPU KERNEL",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "documentation": {}
    },
    {
        "label": "multi_gpu_computation",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "peekOfCode": "def multi_gpu_computation(input_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🔧 MULTI-GPU COMPUTATION\n    Wrapper function for multi-GPU computation.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda, \"Input tensor must be on GPU!\"\n    batch_size, seq_len, hidden_dim = input_tensor.shape\n    # Create output tensor\n    output = torch.empty_like(input_tensor)",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "documentation": {}
    },
    {
        "label": "distributed_computation_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "peekOfCode": "def distributed_computation_kernel(\n    input_ptr, output_ptr,\n    batch_size, seq_len, hidden_dim,\n    stride_ib, stride_is, stride_id,\n    stride_ob, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"\n    🚀 DISTRIBUTED COMPUTATION KERNEL",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "documentation": {}
    },
    {
        "label": "distributed_computation",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "peekOfCode": "def distributed_computation(input_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🚀 DISTRIBUTED COMPUTATION\n    Wrapper function for distributed computation.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda, \"Input tensor must be on GPU!\"\n    batch_size, seq_len, hidden_dim = input_tensor.shape\n    # Create output tensor\n    output = torch.empty_like(input_tensor)",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "documentation": {}
    },
    {
        "label": "load_balancing_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "peekOfCode": "def load_balancing_kernel(\n    input_ptr, output_ptr,\n    batch_size, seq_len, hidden_dim,\n    stride_ib, stride_is, stride_id,\n    stride_ob, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"\n    🎯 LOAD BALANCING KERNEL",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "documentation": {}
    },
    {
        "label": "load_balancing_computation",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "peekOfCode": "def load_balancing_computation(input_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 LOAD BALANCING COMPUTATION\n    Wrapper function for load balancing computation.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda, \"Input tensor must be on GPU!\"\n    batch_size, seq_len, hidden_dim = input_tensor.shape\n    # Create output tensor\n    output = torch.empty_like(input_tensor)",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "documentation": {}
    },
    {
        "label": "communication_optimization_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "peekOfCode": "def communication_optimization_kernel(\n    input_ptr, output_ptr,\n    batch_size, seq_len, hidden_dim,\n    stride_ib, stride_is, stride_id,\n    stride_ob, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"\n    🔥 COMMUNICATION OPTIMIZATION KERNEL",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "documentation": {}
    },
    {
        "label": "communication_optimization",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "peekOfCode": "def communication_optimization(input_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🔥 COMMUNICATION OPTIMIZATION\n    Wrapper function for communication optimization.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda, \"Input tensor must be on GPU!\"\n    batch_size, seq_len, hidden_dim = input_tensor.shape\n    # Create output tensor\n    output = torch.empty_like(input_tensor)",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "documentation": {}
    },
    {
        "label": "test_multi_gpu_distributed",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "peekOfCode": "def test_multi_gpu_distributed():\n    \"\"\"\n    🧪 TEST MULTI-GPU DISTRIBUTED\n    Tests multi-GPU distributed computing and validates correctness.\n    \"\"\"\n    print(\"🧪 Testing Multi-GPU Distributed Computing:\")\n    print(\"=\" * 50)\n    # Test configuration\n    batch_size, seq_len, hidden_dim = 2, 128, 512\n    # Create test data",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "documentation": {}
    },
    {
        "label": "benchmark_multi_gpu_distributed",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "peekOfCode": "def benchmark_multi_gpu_distributed():\n    \"\"\"\n    📊 BENCHMARK MULTI-GPU DISTRIBUTED\n    Benchmarks multi-GPU distributed computing and compares performance.\n    \"\"\"\n    print(\"\\n📊 Benchmarking Multi-GPU Distributed Computing:\")\n    print(\"=\" * 50)\n    # Test different configurations\n    configs = [\n        (2, 128, 512),   # batch_size, seq_len, hidden_dim",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the complete lesson 11 tutorial.\n    \"\"\"\n    print(\"🚀 LESSON 11: MULTI-GPU & DISTRIBUTED COMPUTING\")\n    print(\"=\" * 70)\n    print(\"This lesson covers multi-GPU and distributed computing techniques.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_11_multi_gpu",
        "documentation": {}
    },
    {
        "label": "PerformanceMonitor",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "peekOfCode": "class PerformanceMonitor:\n    \"\"\"\n    🎯 PERFORMANCE MONITOR\n    Monitors kernel performance and provides debugging capabilities.\n    \"\"\"\n    def __init__(self):\n        self.metrics = {}\n        self.logger = logging.getLogger(__name__)\n    def start_monitoring(self, kernel_name: str):\n        \"\"\"Start monitoring a kernel.\"\"\"",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "documentation": {}
    },
    {
        "label": "ScalableSystem",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "peekOfCode": "class ScalableSystem:\n    \"\"\"\n    🔥 SCALABLE SYSTEM\n    Implements scalable system architecture for production deployment.\n    \"\"\"\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.monitor = PerformanceMonitor()\n        self.logger = logging.getLogger(__name__)\n    def initialize(self):",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "documentation": {}
    },
    {
        "label": "explain_production_systems",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "peekOfCode": "def explain_production_systems():\n    \"\"\"\n    📚 PRODUCTION SYSTEMS & REAL-WORLD APPLICATIONS\n    Production systems and real-world application development.\n    \"\"\"\n    print(\"🧠 Production Systems & Real-World Applications:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🎯 Production System Components:\n    1. Production-Ready Kernels:",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "documentation": {}
    },
    {
        "label": "production_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "peekOfCode": "def production_kernel(\n    input_ptr, weight_ptr, bias_ptr, output_ptr,\n    batch_size, seq_len, hidden_dim,\n    stride_ib, stride_is, stride_id,\n    stride_wb, stride_ws, stride_wd,\n    stride_bb, stride_bs, stride_bd,\n    stride_ob, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "documentation": {}
    },
    {
        "label": "production_layer",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "peekOfCode": "def production_layer(input_tensor: torch.Tensor, weight: torch.Tensor, \n                    bias: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🔧 PRODUCTION LAYER\n    Wrapper function for production-ready layer.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda and weight.is_cuda and bias.is_cuda, \"Input tensors must be on GPU!\"\n    assert input_tensor.shape[2] == weight.shape[1], \"Hidden dimensions must match!\"\n    batch_size, seq_len, hidden_dim = input_tensor.shape",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "documentation": {}
    },
    {
        "label": "real_world_application_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "peekOfCode": "def real_world_application_kernel(\n    input_ptr, output_ptr,\n    batch_size, seq_len, hidden_dim,\n    stride_ib, stride_is, stride_id,\n    stride_ob, stride_os, stride_od,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"\n    🚀 REAL-WORLD APPLICATION KERNEL",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "documentation": {}
    },
    {
        "label": "real_world_application",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "peekOfCode": "def real_world_application(input_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🚀 REAL-WORLD APPLICATION\n    Wrapper function for real-world application optimization.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda, \"Input tensor must be on GPU!\"\n    batch_size, seq_len, hidden_dim = input_tensor.shape\n    # Create output tensor\n    output = torch.empty_like(input_tensor)",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "documentation": {}
    },
    {
        "label": "test_production_systems",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "peekOfCode": "def test_production_systems():\n    \"\"\"\n    🧪 TEST PRODUCTION SYSTEMS\n    Tests production systems and validates correctness.\n    \"\"\"\n    print(\"🧪 Testing Production Systems:\")\n    print(\"=\" * 50)\n    # Test configuration\n    batch_size, seq_len, hidden_dim = 2, 128, 512\n    # Create test data",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "documentation": {}
    },
    {
        "label": "benchmark_production_systems",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "peekOfCode": "def benchmark_production_systems():\n    \"\"\"\n    📊 BENCHMARK PRODUCTION SYSTEMS\n    Benchmarks production systems and compares performance.\n    \"\"\"\n    print(\"\\n📊 Benchmarking Production Systems:\")\n    print(\"=\" * 50)\n    # Test different configurations\n    configs = [\n        (2, 128, 512),   # batch_size, seq_len, hidden_dim",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "description": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the complete lesson 12 tutorial.\n    \"\"\"\n    print(\"🚀 LESSON 12: PRODUCTION SYSTEMS & REAL-WORLD APPLICATIONS\")\n    print(\"=\" * 70)\n    print(\"This lesson covers production systems and real-world applications.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.lessons.expert.lesson_12_production_systems",
        "documentation": {}
    },
    {
        "label": "explain_matrix_multiplication",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "peekOfCode": "def explain_matrix_multiplication():\n    \"\"\"\n    📚 MATRIX MULTIPLICATION FUNDAMENTALS\n    Matrix multiplication is one of the most important operations in deep learning.\n    Understanding the algorithms and optimization techniques is crucial.\n    \"\"\"\n    print(\"🧠 Matrix Multiplication Fundamentals:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🎯 What is Matrix Multiplication?",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "basic_matmul_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "peekOfCode": "def basic_matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "basic_matmul",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "peekOfCode": "def basic_matmul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 BASIC MATRIX MULTIPLICATION WRAPPER\n    Wrapper function for the basic matrix multiplication kernel.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape[1] == b.shape[0], \"Inner dimensions must match!\"\n    M, K = a.shape\n    _, N = b.shape",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "optimized_matmul_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "peekOfCode": "def optimized_matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "optimized_matmul",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "peekOfCode": "def optimized_matmul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 OPTIMIZED MATRIX MULTIPLICATION WRAPPER\n    Wrapper function for the optimized matrix multiplication kernel.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape[1] == b.shape[0], \"Inner dimensions must match!\"\n    M, K = a.shape\n    _, N = b.shape",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "batch_matmul_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "peekOfCode": "def batch_matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    B, M, N, K,\n    stride_ab, stride_am, stride_ak,\n    stride_bb, stride_bk, stride_bn,\n    stride_cb, stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "batch_matmul",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "peekOfCode": "def batch_matmul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 BATCH MATRIX MULTIPLICATION WRAPPER\n    Wrapper function for batch matrix multiplication.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape[0] == b.shape[0], \"Batch dimensions must match!\"\n    assert a.shape[2] == b.shape[1], \"Inner dimensions must match!\"\n    B, M, K = a.shape",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "matrix_transpose_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "peekOfCode": "def matrix_transpose_kernel(\n    input_ptr, output_ptr,\n    M, N,\n    stride_im, stride_in,\n    stride_om, stride_on,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"\n    🎯 MATRIX TRANSPOSE KERNEL",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "matrix_transpose",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "peekOfCode": "def matrix_transpose(a: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 MATRIX TRANSPOSE WRAPPER\n    Wrapper function for matrix transpose.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda, \"Input tensor must be on GPU!\"\n    assert a.dim() == 2, \"Input must be a 2D tensor!\"\n    M, N = a.shape\n    # Create output tensor",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "test_matrix_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "peekOfCode": "def test_matrix_operations():\n    \"\"\"\n    🧪 TEST MATRIX OPERATIONS\n    Tests various matrix operations and validates correctness.\n    \"\"\"\n    print(\"\\n🧪 Testing Matrix Operations:\")\n    print(\"=\" * 50)\n    # Test basic matrix multiplication\n    print(\"\\n📊 Test: Basic Matrix Multiplication\")\n    M, K, N = 256, 128, 192",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "benchmark_matrix_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "peekOfCode": "def benchmark_matrix_operations():\n    \"\"\"\n    📊 BENCHMARK MATRIX OPERATIONS\n    Compares performance between Triton and PyTorch for matrix operations.\n    \"\"\"\n    print(\"\\n📊 Benchmarking Matrix Operations:\")\n    print(\"=\" * 50)\n    sizes = [\n        (256, 256, 256),\n        (512, 512, 512),",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the complete lesson 4 tutorial.\n    \"\"\"\n    print(\"🔧 LESSON 4: MATRIX OPERATIONS & TILING STRATEGIES\")\n    print(\"=\" * 70)\n    print(\"This lesson covers matrix operations and optimization techniques in Triton.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_04_matrix_operations",
        "documentation": {}
    },
    {
        "label": "explain_shared_memory",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def explain_shared_memory():\n    \"\"\"\n    📚 SHARED MEMORY OPTIMIZATION\n    Shared memory is a fast, on-chip memory that's shared among threads in a block.\n    It's crucial for optimizing memory access patterns and reducing global memory traffic.\n    \"\"\"\n    print(\"🧠 Shared Memory Optimization:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🎯 What is Shared Memory?",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "shared_memory_matmul_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def shared_memory_matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "shared_memory_matmul",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def shared_memory_matmul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 SHARED MEMORY MATRIX MULTIPLICATION WRAPPER\n    Wrapper function for shared memory matrix multiplication.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape[1] == b.shape[0], \"Inner dimensions must match!\"\n    M, K = a.shape\n    _, N = b.shape",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "explain_memory_coalescing_patterns",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def explain_memory_coalescing_patterns():\n    \"\"\"\n    📚 MEMORY COALESCING PATTERNS\n    Different memory access patterns have different coalescing characteristics.\n    Understanding these patterns is crucial for optimization.\n    \"\"\"\n    print(\"\\n🎯 Memory Coalescing Patterns:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🧠 Coalescing Patterns:",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "coalesced_access_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def coalesced_access_kernel(\n    input_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 COALESCED ACCESS KERNEL\n    Demonstrates perfect memory coalescing.\n    \"\"\"\n    pid = tl.program_id(axis=0)",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "strided_access_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def strided_access_kernel(\n    input_ptr, output_ptr,\n    n_elements, stride,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 STRIDED ACCESS KERNEL\n    Demonstrates strided memory access.\n    \"\"\"\n    pid = tl.program_id(axis=0)",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "test_memory_coalescing",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def test_memory_coalescing():\n    \"\"\"\n    🧪 TEST MEMORY COALESCING PATTERNS\n    Tests different memory access patterns and measures performance.\n    \"\"\"\n    print(\"\\n🧪 Testing Memory Coalescing Patterns:\")\n    print(\"=\" * 50)\n    size = 1024 * 1024  # 1M elements\n    # Create test data\n    x = torch.randn(size, device='cuda', dtype=torch.float32)",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "explain_cache_friendly_algorithms",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def explain_cache_friendly_algorithms():\n    \"\"\"\n    📚 CACHE-FRIENDLY ALGORITHMS\n    Cache-friendly algorithms are designed to maximize cache utilization\n    and minimize cache misses.\n    \"\"\"\n    print(\"\\n🎯 Cache-Friendly Algorithms:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🧠 Cache-Friendly Design Principles:",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "cache_friendly_reduction_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def cache_friendly_reduction_kernel(\n    input_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 CACHE-FRIENDLY REDUCTION KERNEL\n    Implements cache-friendly reduction using shared memory.\n    \"\"\"\n    pid = tl.program_id(axis=0)",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "cache_friendly_reduction",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def cache_friendly_reduction(input_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 CACHE-FRIENDLY REDUCTION WRAPPER\n    Wrapper function for cache-friendly reduction.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda, \"Input tensor must be on GPU!\"\n    n_elements = input_tensor.numel()\n    output = torch.zeros(1, device=input_tensor.device, dtype=input_tensor.dtype)\n    # Define block size",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "explain_bandwidth_optimization",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def explain_bandwidth_optimization():\n    \"\"\"\n    📚 BANDWIDTH OPTIMIZATION\n    Memory bandwidth is often the bottleneck in GPU kernels.\n    Optimizing bandwidth utilization is crucial for performance.\n    \"\"\"\n    print(\"\\n🎯 Bandwidth Optimization:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🧠 Bandwidth Optimization Strategies:",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "bandwidth_optimized_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def bandwidth_optimized_kernel(\n    input_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 BANDWIDTH OPTIMIZED KERNEL\n    Demonstrates bandwidth optimization techniques.\n    \"\"\"\n    pid = tl.program_id(axis=0)",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "measure_memory_bandwidth",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def measure_memory_bandwidth():\n    \"\"\"\n    📊 MEASURE MEMORY BANDWIDTH\n    Measures achieved memory bandwidth for different configurations.\n    \"\"\"\n    print(\"\\n📊 Measuring Memory Bandwidth:\")\n    print(\"=\" * 50)\n    sizes = [1024, 4096, 16384, 65536, 262144, 1048576]\n    for size in sizes:\n        print(f\"\\n📈 Size: {size:,} elements\")",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "test_advanced_memory",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def test_advanced_memory():\n    \"\"\"\n    🧪 TEST ADVANCED MEMORY OPTIMIZATIONS\n    Tests various advanced memory optimization techniques.\n    \"\"\"\n    print(\"\\n🧪 Testing Advanced Memory Optimizations:\")\n    print(\"=\" * 50)\n    # Test shared memory matrix multiplication\n    print(\"\\n📊 Test: Shared Memory Matrix Multiplication\")\n    M, K, N = 256, 128, 192",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "benchmark_advanced_memory",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def benchmark_advanced_memory():\n    \"\"\"\n    📊 BENCHMARK ADVANCED MEMORY OPTIMIZATIONS\n    Compares performance between different memory optimization techniques.\n    \"\"\"\n    print(\"\\n📊 Benchmarking Advanced Memory Optimizations:\")\n    print(\"=\" * 50)\n    # Benchmark shared memory matrix multiplication\n    print(\"\\n📈 Benchmark: Shared Memory Matrix Multiplication\")\n    sizes = [",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the complete lesson 5 tutorial.\n    \"\"\"\n    print(\"🔧 LESSON 5: ADVANCED MEMORY PATTERNS & OPTIMIZATION\")\n    print(\"=\" * 70)\n    print(\"This lesson covers advanced memory optimization techniques in Triton.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_05_advanced_memory",
        "documentation": {}
    },
    {
        "label": "explain_kernel_fusion",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "peekOfCode": "def explain_kernel_fusion():\n    \"\"\"\n    📚 KERNEL FUSION FUNDAMENTALS\n    Kernel fusion combines multiple operations into a single kernel to reduce\n    memory traffic and kernel launch overhead.\n    \"\"\"\n    print(\"🧠 Kernel Fusion Fundamentals:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🎯 What is Kernel Fusion?",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "fused_add_multiply_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "peekOfCode": "def fused_add_multiply_kernel(\n    a_ptr, b_ptr, c_ptr, output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 FUSED ADD-MULTIPLY KERNEL\n    Fuses addition and multiplication operations:\n    output = (a + b) * c\n    This demonstrates basic horizontal fusion.",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "fused_add_multiply",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "peekOfCode": "def fused_add_multiply(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    🎯 FUSED ADD-MULTIPLY WRAPPER\n    Wrapper function for fused add-multiply kernel.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda and c.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape == b.shape == c.shape, \"Input tensors must have the same shape!\"\n    n_elements = a.numel()\n    output = torch.empty_like(a)",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "fused_matmul_activation_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "peekOfCode": "def fused_matmul_activation_kernel(\n    a_ptr, b_ptr, output_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    activation_type: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "fused_matmul_activation",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "peekOfCode": "def fused_matmul_activation(a: torch.Tensor, b: torch.Tensor, activation: str = \"relu\") -> torch.Tensor:\n    \"\"\"\n    🎯 FUSED MATRIX MULTIPLICATION + ACTIVATION WRAPPER\n    Wrapper function for fused matrix multiplication + activation.\n    \"\"\"\n    # Input validation\n    assert a.is_cuda and b.is_cuda, \"Input tensors must be on GPU!\"\n    assert a.shape[1] == b.shape[0], \"Inner dimensions must match!\"\n    M, K = a.shape\n    _, N = b.shape",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "fused_loop_kernel",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "peekOfCode": "def fused_loop_kernel(\n    input_ptr, output_ptr,\n    n_elements, num_iterations,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    🎯 FUSED LOOP KERNEL\n    Demonstrates loop fusion by combining multiple operations in a loop.\n    This reduces memory traffic and improves performance.\n    \"\"\"",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "fused_loop",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "peekOfCode": "def fused_loop(input_tensor: torch.Tensor, num_iterations: int = 10) -> torch.Tensor:\n    \"\"\"\n    🎯 FUSED LOOP WRAPPER\n    Wrapper function for fused loop kernel.\n    \"\"\"\n    # Input validation\n    assert input_tensor.is_cuda, \"Input tensor must be on GPU!\"\n    n_elements = input_tensor.numel()\n    output = torch.empty_like(input_tensor)\n    # Define block size",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "explain_performance_profiling",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "peekOfCode": "def explain_performance_profiling():\n    \"\"\"\n    📚 PERFORMANCE PROFILING\n    Performance profiling is crucial for identifying bottlenecks and\n    optimizing kernel performance.\n    \"\"\"\n    print(\"\\n🎯 Performance Profiling:\")\n    print(\"=\" * 50)\n    print(\"\"\"\n    🧠 Performance Profiling Techniques:",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "profile_kernel_performance",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "peekOfCode": "def profile_kernel_performance():\n    \"\"\"\n    📊 PROFILE KERNEL PERFORMANCE\n    Profiles different kernel implementations and compares performance.\n    \"\"\"\n    print(\"\\n📊 Profiling Kernel Performance:\")\n    print(\"=\" * 50)\n    # Test different kernel implementations\n    size = 1024 * 1024  # 1M elements\n    # Create test data",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "test_kernel_fusion",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "peekOfCode": "def test_kernel_fusion():\n    \"\"\"\n    🧪 TEST KERNEL FUSION\n    Tests various kernel fusion techniques and validates correctness.\n    \"\"\"\n    print(\"\\n🧪 Testing Kernel Fusion:\")\n    print(\"=\" * 50)\n    # Test fused add-multiply\n    print(\"\\n📊 Test: Fused Add-Multiply\")\n    size = 1024",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "description": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN FUNCTION\n    Runs the complete lesson 6 tutorial.\n    \"\"\"\n    print(\"🔧 LESSON 6: KERNEL FUSION & PERFORMANCE TUNING\")\n    print(\"=\" * 70)\n    print(\"This lesson covers kernel fusion and performance optimization techniques.\")\n    # Check CUDA availability\n    if not torch.cuda.is_available():",
        "detail": "qwen-llm.triton_tutorials.lessons.intermediate.lesson_06_kernel_fusion",
        "documentation": {}
    },
    {
        "label": "TestBeginnerLessons",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.tests.test_beginner",
        "description": "qwen-llm.triton_tutorials.tests.test_beginner",
        "peekOfCode": "class TestBeginnerLessons(unittest.TestCase):\n    \"\"\"\n    🧪 TEST SUITE FOR BEGINNER LESSONS\n    Tests for lessons 1-3 (GPU Fundamentals, Memory Management, Basic Operations)\n    \"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.dtype = torch.float32\n        # Test data sizes",
        "detail": "qwen-llm.triton_tutorials.tests.test_beginner",
        "documentation": {}
    },
    {
        "label": "TestBeginnerLessonsIntegration",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.tests.test_beginner",
        "description": "qwen-llm.triton_tutorials.tests.test_beginner",
        "peekOfCode": "class TestBeginnerLessonsIntegration(unittest.TestCase):\n    \"\"\"\n    🧪 INTEGRATION TESTS FOR BEGINNER LESSONS\n    Integration tests that combine multiple lessons.\n    \"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.dtype = torch.float32\n    def test_lesson_integration_vector_operations(self):",
        "detail": "qwen-llm.triton_tutorials.tests.test_beginner",
        "documentation": {}
    },
    {
        "label": "TestExamples",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.tests.test_examples",
        "description": "qwen-llm.triton_tutorials.tests.test_examples",
        "peekOfCode": "class TestExamples(unittest.TestCase):\n    \"\"\"\n    🧪 TEST SUITE FOR EXAMPLES\n    Tests for the example implementations.\n    \"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.dtype = torch.float32\n        # Test data for attention",
        "detail": "qwen-llm.triton_tutorials.tests.test_examples",
        "documentation": {}
    },
    {
        "label": "TestExamplesIntegration",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.tests.test_examples",
        "description": "qwen-llm.triton_tutorials.tests.test_examples",
        "peekOfCode": "class TestExamplesIntegration(unittest.TestCase):\n    \"\"\"\n    🧪 INTEGRATION TESTS FOR EXAMPLES\n    Integration tests that combine multiple examples.\n    \"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.dtype = torch.float32\n    def test_transformer_pipeline(self):",
        "detail": "qwen-llm.triton_tutorials.tests.test_examples",
        "documentation": {}
    },
    {
        "label": "TestIntermediateLessons",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.tests.test_intermediate",
        "description": "qwen-llm.triton_tutorials.tests.test_intermediate",
        "peekOfCode": "class TestIntermediateLessons(unittest.TestCase):\n    \"\"\"\n    🧪 TEST SUITE FOR INTERMEDIATE LESSONS\n    Tests for lessons 4-6 (Matrix Operations, Advanced Memory, Kernel Fusion)\n    \"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.dtype = torch.float32\n        # Test matrix sizes",
        "detail": "qwen-llm.triton_tutorials.tests.test_intermediate",
        "documentation": {}
    },
    {
        "label": "TestIntermediateLessonsIntegration",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.tests.test_intermediate",
        "description": "qwen-llm.triton_tutorials.tests.test_intermediate",
        "peekOfCode": "class TestIntermediateLessonsIntegration(unittest.TestCase):\n    \"\"\"\n    🧪 INTEGRATION TESTS FOR INTERMEDIATE LESSONS\n    Integration tests that combine multiple intermediate lessons.\n    \"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.dtype = torch.float32\n    def test_lesson_integration_matrix_operations(self):",
        "detail": "qwen-llm.triton_tutorials.tests.test_intermediate",
        "documentation": {}
    },
    {
        "label": "TestBenchmarking",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.tests.test_utils",
        "description": "qwen-llm.triton_tutorials.tests.test_utils",
        "peekOfCode": "class TestBenchmarking(unittest.TestCase):\n    \"\"\"\n    🧪 TEST SUITE FOR BENCHMARKING UTILITIES\n    Tests for the benchmarking module.\n    \"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.dtype = torch.float32\n        self.suite = BenchmarkSuite(warmup_runs=5, benchmark_runs=10)",
        "detail": "qwen-llm.triton_tutorials.tests.test_utils",
        "documentation": {}
    },
    {
        "label": "TestProfiling",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.tests.test_utils",
        "description": "qwen-llm.triton_tutorials.tests.test_utils",
        "peekOfCode": "class TestProfiling(unittest.TestCase):\n    \"\"\"\n    🧪 TEST SUITE FOR PROFILING UTILITIES\n    Tests for the profiling module.\n    \"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.dtype = torch.float32\n        self.profiler = PerformanceProfiler()",
        "detail": "qwen-llm.triton_tutorials.tests.test_utils",
        "documentation": {}
    },
    {
        "label": "TestValidation",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.tests.test_utils",
        "description": "qwen-llm.triton_tutorials.tests.test_utils",
        "peekOfCode": "class TestValidation(unittest.TestCase):\n    \"\"\"\n    🧪 TEST SUITE FOR VALIDATION UTILITIES\n    Tests for the validation module.\n    \"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.dtype = torch.float32\n        self.suite = ValidationSuite(rtol=1e-5, atol=1e-6)",
        "detail": "qwen-llm.triton_tutorials.tests.test_utils",
        "documentation": {}
    },
    {
        "label": "TestDataGeneration",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.tests.test_utils",
        "description": "qwen-llm.triton_tutorials.tests.test_utils",
        "peekOfCode": "class TestDataGeneration(unittest.TestCase):\n    \"\"\"\n    🧪 TEST SUITE FOR DATA GENERATION UTILITIES\n    Tests for the data generation module.\n    \"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.dtype = torch.float32\n        self.generator = DataGenerator(seed=42)",
        "detail": "qwen-llm.triton_tutorials.tests.test_utils",
        "documentation": {}
    },
    {
        "label": "TestPerformanceAnalysis",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.tests.test_utils",
        "description": "qwen-llm.triton_tutorials.tests.test_utils",
        "peekOfCode": "class TestPerformanceAnalysis(unittest.TestCase):\n    \"\"\"\n    🧪 TEST SUITE FOR PERFORMANCE ANALYSIS UTILITIES\n    Tests for the performance analysis module.\n    \"\"\"\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.dtype = torch.float32\n        self.analyzer = PerformanceAnalyzer()",
        "detail": "qwen-llm.triton_tutorials.tests.test_utils",
        "documentation": {}
    },
    {
        "label": "BenchmarkResult",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.utils.benchmarking",
        "description": "qwen-llm.triton_tutorials.utils.benchmarking",
        "peekOfCode": "class BenchmarkResult:\n    \"\"\"Results from a benchmark run.\"\"\"\n    name: str\n    triton_time: float\n    pytorch_time: float\n    speedup: float\n    memory_usage: Optional[float] = None\n    throughput: Optional[float] = None\n    error: Optional[str] = None\nclass BenchmarkSuite:",
        "detail": "qwen-llm.triton_tutorials.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "BenchmarkSuite",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.utils.benchmarking",
        "description": "qwen-llm.triton_tutorials.utils.benchmarking",
        "peekOfCode": "class BenchmarkSuite:\n    \"\"\"\n    📊 BENCHMARK SUITE\n    A comprehensive benchmarking suite for Triton kernels.\n    \"\"\"\n    def __init__(self, warmup_runs: int = 10, benchmark_runs: int = 100):\n        \"\"\"\n        Initialize the benchmark suite.\n        Args:\n            warmup_runs: Number of warmup runs",
        "detail": "qwen-llm.triton_tutorials.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "benchmark_vector_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.utils.benchmarking",
        "description": "qwen-llm.triton_tutorials.utils.benchmarking",
        "peekOfCode": "def benchmark_vector_operations():\n    \"\"\"Benchmark vector operations.\"\"\"\n    print(\"📊 Benchmarking Vector Operations:\")\n    print(\"=\" * 50)\n    suite = BenchmarkSuite()\n    # Test different sizes\n    sizes = [1024, 4096, 16384, 65536, 262144]\n    for size in sizes:\n        print(f\"\\n📈 Size: {size:,} elements\")\n        # Create test data",
        "detail": "qwen-llm.triton_tutorials.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "benchmark_matrix_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.utils.benchmarking",
        "description": "qwen-llm.triton_tutorials.utils.benchmarking",
        "peekOfCode": "def benchmark_matrix_operations():\n    \"\"\"Benchmark matrix operations.\"\"\"\n    print(\"\\n📊 Benchmarking Matrix Operations:\")\n    print(\"=\" * 50)\n    suite = BenchmarkSuite()\n    # Test different matrix sizes\n    sizes = [\n        (256, 256, 256),\n        (512, 512, 512),\n        (1024, 1024, 1024),",
        "detail": "qwen-llm.triton_tutorials.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "benchmark_memory_bandwidth",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.utils.benchmarking",
        "description": "qwen-llm.triton_tutorials.utils.benchmarking",
        "peekOfCode": "def benchmark_memory_bandwidth():\n    \"\"\"Benchmark memory bandwidth.\"\"\"\n    print(\"\\n📊 Benchmarking Memory Bandwidth:\")\n    print(\"=\" * 50)\n    suite = BenchmarkSuite()\n    # Test different sizes\n    sizes = [1024, 4096, 16384, 65536, 262144, 1048576]\n    for size in sizes:\n        print(f\"\\n📈 Size: {size:,} elements\")\n        # Benchmark memory copy",
        "detail": "qwen-llm.triton_tutorials.utils.benchmarking",
        "documentation": {}
    },
    {
        "label": "DataConfig",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.utils.data_generation",
        "description": "qwen-llm.triton_tutorials.utils.data_generation",
        "peekOfCode": "class DataConfig:\n    \"\"\"Configuration for data generation.\"\"\"\n    size: int\n    dtype: torch.dtype = torch.float32\n    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n    seed: Optional[int] = None\n    distribution: str = 'normal'  # 'normal', 'uniform', 'zeros', 'ones'\n    mean: float = 0.0\n    std: float = 1.0\n    min_val: float = 0.0",
        "detail": "qwen-llm.triton_tutorials.utils.data_generation",
        "documentation": {}
    },
    {
        "label": "DataGenerator",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.utils.data_generation",
        "description": "qwen-llm.triton_tutorials.utils.data_generation",
        "peekOfCode": "class DataGenerator:\n    \"\"\"\n    📊 DATA GENERATOR\n    A utility class for generating test data for Triton kernels.\n    \"\"\"\n    def __init__(self, seed: Optional[int] = None):\n        \"\"\"\n        Initialize the data generator.\n        Args:\n            seed: Random seed for reproducibility",
        "detail": "qwen-llm.triton_tutorials.utils.data_generation",
        "documentation": {}
    },
    {
        "label": "generate_test_data",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.utils.data_generation",
        "description": "qwen-llm.triton_tutorials.utils.data_generation",
        "peekOfCode": "def generate_test_data():\n    \"\"\"Generate test data for various scenarios.\"\"\"\n    print(\"📊 Generating Test Data:\")\n    print(\"=\" * 50)\n    generator = DataGenerator(seed=42)\n    # Generate vector data\n    print(\"\\n📈 Vector Data:\")\n    vector_config = DataConfig(size=1024, dtype=torch.float32, distribution='normal')\n    vector_data = generator.generate_vector(vector_config)\n    print(f\"  Generated vector: {vector_data.shape}, dtype: {vector_data.dtype}\")",
        "detail": "qwen-llm.triton_tutorials.utils.data_generation",
        "documentation": {}
    },
    {
        "label": "generate_validation_data",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.utils.data_generation",
        "description": "qwen-llm.triton_tutorials.utils.data_generation",
        "peekOfCode": "def generate_validation_data():\n    \"\"\"Generate data for validation tests.\"\"\"\n    print(\"\\n📊 Generating Validation Data:\")\n    print(\"=\" * 50)\n    generator = DataGenerator(seed=123)\n    # Generate edge case data\n    edge_cases = []\n    # Single element\n    single_config = DataConfig(size=1, dtype=torch.float32, distribution='normal')\n    edge_cases.append(generator.generate_vector(single_config))",
        "detail": "qwen-llm.triton_tutorials.utils.data_generation",
        "documentation": {}
    },
    {
        "label": "PerformanceMetrics",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "description": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "peekOfCode": "class PerformanceMetrics:\n    \"\"\"Performance metrics for a kernel.\"\"\"\n    name: str\n    execution_time: float\n    memory_bandwidth: float\n    arithmetic_intensity: float\n    gpu_utilization: float\n    cache_hit_rate: float\n    throughput: float\n    efficiency: float",
        "detail": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "documentation": {}
    },
    {
        "label": "PerformanceAnalyzer",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "description": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "peekOfCode": "class PerformanceAnalyzer:\n    \"\"\"\n    📈 PERFORMANCE ANALYZER\n    A comprehensive performance analyzer for Triton kernels.\n    \"\"\"\n    def __init__(self):\n        \"\"\"Initialize the performance analyzer.\"\"\"\n        self.metrics = []\n        self.gpu_available = torch.cuda.is_available()\n    def analyze_kernel(self,",
        "detail": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "documentation": {}
    },
    {
        "label": "analyze_vector_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "description": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "peekOfCode": "def analyze_vector_operations():\n    \"\"\"Analyze performance of vector operations.\"\"\"\n    print(\"📈 Analyzing Vector Operations:\")\n    print(\"=\" * 50)\n    analyzer = PerformanceAnalyzer()\n    # Test different sizes\n    sizes = [1024, 4096, 16384, 65536, 262144]\n    for size in sizes:\n        print(f\"\\n📊 Size: {size:,} elements\")\n        # Create test data",
        "detail": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "documentation": {}
    },
    {
        "label": "analyze_matrix_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "description": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "peekOfCode": "def analyze_matrix_operations():\n    \"\"\"Analyze performance of matrix operations.\"\"\"\n    print(\"\\n📈 Analyzing Matrix Operations:\")\n    print(\"=\" * 50)\n    analyzer = PerformanceAnalyzer()\n    # Test different matrix sizes\n    sizes = [\n        (256, 256, 256),\n        (512, 512, 512),\n        (1024, 1024, 1024),",
        "detail": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "documentation": {}
    },
    {
        "label": "analyze_scaling",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "description": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "peekOfCode": "def analyze_scaling():\n    \"\"\"Analyze scaling performance.\"\"\"\n    print(\"\\n📈 Analyzing Scaling Performance:\")\n    print(\"=\" * 50)\n    analyzer = PerformanceAnalyzer()\n    # Test scaling with vector addition\n    sizes = [1024, 4096, 16384, 65536, 262144, 1048576]\n    def vector_add(x, y):\n        return x + y\n    scaling_metrics = analyzer.analyze_scaling(vector_add, \"Vector Addition\", sizes)",
        "detail": "qwen-llm.triton_tutorials.utils.performance_analysis",
        "documentation": {}
    },
    {
        "label": "ProfileResult",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.utils.profiling",
        "description": "qwen-llm.triton_tutorials.utils.profiling",
        "peekOfCode": "class ProfileResult:\n    \"\"\"Results from a profiling run.\"\"\"\n    name: str\n    execution_time: float\n    memory_usage: float\n    gpu_memory_usage: float\n    gpu_utilization: float\n    cpu_utilization: float\n    throughput: Optional[float] = None\n    error: Optional[str] = None",
        "detail": "qwen-llm.triton_tutorials.utils.profiling",
        "documentation": {}
    },
    {
        "label": "PerformanceProfiler",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.utils.profiling",
        "description": "qwen-llm.triton_tutorials.utils.profiling",
        "peekOfCode": "class PerformanceProfiler:\n    \"\"\"\n    🔍 PERFORMANCE PROFILER\n    A comprehensive profiler for Triton kernels and PyTorch operations.\n    \"\"\"\n    def __init__(self):\n        \"\"\"Initialize the performance profiler.\"\"\"\n        self.results = []\n        self.gpu_available = torch.cuda.is_available()\n    def profile_function(self, ",
        "detail": "qwen-llm.triton_tutorials.utils.profiling",
        "documentation": {}
    },
    {
        "label": "profile_system_info",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.utils.profiling",
        "description": "qwen-llm.triton_tutorials.utils.profiling",
        "peekOfCode": "def profile_system_info():\n    \"\"\"Profile system information.\"\"\"\n    print(\"🔍 System Information:\")\n    print(\"=\" * 50)\n    # CPU information\n    print(f\"CPU: {psutil.cpu_count()} cores\")\n    print(f\"CPU Usage: {psutil.cpu_percent()}%\")\n    # Memory information\n    memory = psutil.virtual_memory()\n    print(f\"Memory: {memory.total / 1024**3:.1f} GB total, {memory.available / 1024**3:.1f} GB available\")",
        "detail": "qwen-llm.triton_tutorials.utils.profiling",
        "documentation": {}
    },
    {
        "label": "profile_kernel_performance",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.utils.profiling",
        "description": "qwen-llm.triton_tutorials.utils.profiling",
        "peekOfCode": "def profile_kernel_performance():\n    \"\"\"Profile kernel performance.\"\"\"\n    print(\"\\n🔍 Profiling Kernel Performance:\")\n    print(\"=\" * 50)\n    profiler = PerformanceProfiler()\n    # Test vector operations\n    size = 1024 * 1024  # 1M elements\n    a = torch.randn(size, device='cuda' if torch.cuda.is_available() else 'cpu', dtype=torch.float32)\n    b = torch.randn(size, device='cuda' if torch.cuda.is_available() else 'cpu', dtype=torch.float32)\n    # Profile vector addition",
        "detail": "qwen-llm.triton_tutorials.utils.profiling",
        "documentation": {}
    },
    {
        "label": "ValidationResult",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.utils.validation",
        "description": "qwen-llm.triton_tutorials.utils.validation",
        "peekOfCode": "class ValidationResult:\n    \"\"\"Results from a validation test.\"\"\"\n    name: str\n    passed: bool\n    max_error: float\n    mean_error: float\n    relative_error: float\n    error_message: Optional[str] = None\nclass ValidationSuite:\n    \"\"\"",
        "detail": "qwen-llm.triton_tutorials.utils.validation",
        "documentation": {}
    },
    {
        "label": "ValidationSuite",
        "kind": 6,
        "importPath": "qwen-llm.triton_tutorials.utils.validation",
        "description": "qwen-llm.triton_tutorials.utils.validation",
        "peekOfCode": "class ValidationSuite:\n    \"\"\"\n    ✅ VALIDATION SUITE\n    A comprehensive validation suite for Triton kernels.\n    \"\"\"\n    def __init__(self, rtol: float = 1e-5, atol: float = 1e-6):\n        \"\"\"\n        Initialize the validation suite.\n        Args:\n            rtol: Relative tolerance for comparison",
        "detail": "qwen-llm.triton_tutorials.utils.validation",
        "documentation": {}
    },
    {
        "label": "validate_vector_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.utils.validation",
        "description": "qwen-llm.triton_tutorials.utils.validation",
        "peekOfCode": "def validate_vector_operations():\n    \"\"\"Validate vector operations.\"\"\"\n    print(\"✅ Validating Vector Operations:\")\n    print(\"=\" * 50)\n    suite = ValidationSuite()\n    # Test different sizes\n    sizes = [1024, 4096, 16384]\n    for size in sizes:\n        print(f\"\\n📊 Size: {size:,} elements\")\n        # Create test data",
        "detail": "qwen-llm.triton_tutorials.utils.validation",
        "documentation": {}
    },
    {
        "label": "validate_matrix_operations",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.utils.validation",
        "description": "qwen-llm.triton_tutorials.utils.validation",
        "peekOfCode": "def validate_matrix_operations():\n    \"\"\"Validate matrix operations.\"\"\"\n    print(\"\\n✅ Validating Matrix Operations:\")\n    print(\"=\" * 50)\n    suite = ValidationSuite()\n    # Test different matrix sizes\n    sizes = [\n        (256, 256, 256),\n        (512, 512, 512),\n        (1024, 1024, 1024),",
        "detail": "qwen-llm.triton_tutorials.utils.validation",
        "documentation": {}
    },
    {
        "label": "validate_edge_cases",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.utils.validation",
        "description": "qwen-llm.triton_tutorials.utils.validation",
        "peekOfCode": "def validate_edge_cases():\n    \"\"\"Validate edge cases.\"\"\"\n    print(\"\\n✅ Validating Edge Cases:\")\n    print(\"=\" * 50)\n    suite = ValidationSuite()\n    # Test edge cases\n    edge_cases = [\n        # Single element\n        (torch.tensor([1.0], device='cuda' if torch.cuda.is_available() else 'cpu'),\n         torch.tensor([2.0], device='cuda' if torch.cuda.is_available() else 'cpu')),",
        "detail": "qwen-llm.triton_tutorials.utils.validation",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.cli",
        "description": "qwen-llm.triton_tutorials.cli",
        "peekOfCode": "def main():\n    \"\"\"\n    🎯 MAIN CLI FUNCTION\n    Provides a command-line interface for running tutorials.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"🚀 Triton Tutorials CLI - Learn Triton from beginner to expert\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:",
        "detail": "qwen-llm.triton_tutorials.cli",
        "documentation": {}
    },
    {
        "label": "run_lesson",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.cli",
        "description": "qwen-llm.triton_tutorials.cli",
        "peekOfCode": "def run_lesson(level: str, lesson: int, force_gpu: bool):\n    \"\"\"\n    🎯 RUN A SPECIFIC LESSON\n    Args:\n        level: Tutorial level (beginner, intermediate, advanced, expert)\n        lesson: Lesson number\n        force_gpu: Force GPU usage\n    \"\"\"\n    print(f\"🚀 Running {level} lesson {lesson}\")\n    # Check CUDA availability",
        "detail": "qwen-llm.triton_tutorials.cli",
        "documentation": {}
    },
    {
        "label": "list_lessons",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.cli",
        "description": "qwen-llm.triton_tutorials.cli",
        "peekOfCode": "def list_lessons(level_filter: str = None):\n    \"\"\"\n    📚 LIST ALL AVAILABLE LESSONS\n    Args:\n        level_filter: Optional level filter\n    \"\"\"\n    print(\"📚 Available Triton Tutorials:\")\n    print(\"=\" * 50)\n    lessons = {\n        'beginner': [",
        "detail": "qwen-llm.triton_tutorials.cli",
        "documentation": {}
    },
    {
        "label": "run_benchmarks",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.cli",
        "description": "qwen-llm.triton_tutorials.cli",
        "peekOfCode": "def run_benchmarks(lesson: int = None, run_all: bool = False):\n    \"\"\"\n    📊 RUN PERFORMANCE BENCHMARKS\n    Args:\n        lesson: Specific lesson to benchmark\n        run_all: Run all benchmarks\n    \"\"\"\n    print(\"📊 Running Performance Benchmarks:\")\n    print(\"=\" * 50)\n    if lesson:",
        "detail": "qwen-llm.triton_tutorials.cli",
        "documentation": {}
    },
    {
        "label": "install_dependencies",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.cli",
        "description": "qwen-llm.triton_tutorials.cli",
        "peekOfCode": "def install_dependencies(dev: bool = False):\n    \"\"\"\n    📦 INSTALL DEPENDENCIES\n    Args:\n        dev: Install development dependencies\n    \"\"\"\n    print(\"📦 Installing Dependencies:\")\n    print(\"=\" * 50)\n    import subprocess\n    import sys",
        "detail": "qwen-llm.triton_tutorials.cli",
        "documentation": {}
    },
    {
        "label": "show_system_info",
        "kind": 2,
        "importPath": "qwen-llm.triton_tutorials.cli",
        "description": "qwen-llm.triton_tutorials.cli",
        "peekOfCode": "def show_system_info():\n    \"\"\"\n    ℹ️ SHOW SYSTEM INFORMATION\n    Displays system information relevant to Triton development.\n    \"\"\"\n    print(\"ℹ️ System Information:\")\n    print(\"=\" * 50)\n    # Python version\n    import sys\n    print(f\"Python version: {sys.version}\")",
        "detail": "qwen-llm.triton_tutorials.cli",
        "documentation": {}
    },
    {
        "label": "convert_checkpoint",
        "kind": 2,
        "importPath": "convert_checkpoint",
        "description": "convert_checkpoint",
        "peekOfCode": "def convert_checkpoint(old_path: str, new_path: str):\n    \"\"\"\n    Convert old checkpoint to new format\n    \"\"\"\n    print(f\"🔄 Converting checkpoint from {old_path} to {new_path}\")\n    try:\n        # Try to load the old checkpoint\n        print(\"📦 Loading old checkpoint...\")\n        checkpoint = torch.load(old_path, map_location='cpu', weights_only=False)\n        # Extract only the essential data",
        "detail": "convert_checkpoint",
        "documentation": {}
    }
]