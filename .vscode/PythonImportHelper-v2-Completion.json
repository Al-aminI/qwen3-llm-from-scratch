[
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "fire",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fire",
        "description": "fire",
        "detail": "fire",
        "documentation": {}
    },
    {
        "label": "PaliGemmaProcessor",
        "importPath": "processing_paligemma",
        "description": "processing_paligemma",
        "isExtraImport": true,
        "detail": "processing_paligemma",
        "documentation": {}
    },
    {
        "label": "KVCache",
        "importPath": "modeling_gemma",
        "description": "modeling_gemma",
        "isExtraImport": true,
        "detail": "modeling_gemma",
        "documentation": {}
    },
    {
        "label": "PaliGemmaForConditionalGeneration",
        "importPath": "modeling_gemma",
        "description": "modeling_gemma",
        "isExtraImport": true,
        "detail": "modeling_gemma",
        "documentation": {}
    },
    {
        "label": "PaliGemmaForConditionalGeneration",
        "importPath": "modeling_gemma",
        "description": "modeling_gemma",
        "isExtraImport": true,
        "detail": "modeling_gemma",
        "documentation": {}
    },
    {
        "label": "PaliGemmaConfig",
        "importPath": "modeling_gemma",
        "description": "modeling_gemma",
        "isExtraImport": true,
        "detail": "modeling_gemma",
        "documentation": {}
    },
    {
        "label": "load_hf_model",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "SiglipVisionConfig",
        "importPath": "modeling_siglip",
        "description": "modeling_siglip",
        "isExtraImport": true,
        "detail": "modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipVisionModel",
        "importPath": "modeling_siglip",
        "description": "modeling_siglip",
        "isExtraImport": true,
        "detail": "modeling_siglip",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "safe_open",
        "importPath": "safetensors",
        "description": "safetensors",
        "isExtraImport": true,
        "detail": "safetensors",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "torch.utils.data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "GradScaler",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "psutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil",
        "description": "psutil",
        "detail": "psutil",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "Muon",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "config.qwen3_small_config",
        "description": "config.qwen3_small_config",
        "isExtraImport": true,
        "detail": "config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "train_model",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "resume_training",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "importPath": "qwen3_complete_model",
        "description": "qwen3_complete_model",
        "isExtraImport": true,
        "detail": "qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "QuantizationExpert",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationBenchmark",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALayer",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALayer",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALinear",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALayer",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALayer",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALinear",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "importPath": "quantization_tutorial",
        "description": "quantization_tutorial",
        "isExtraImport": true,
        "detail": "quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "FineTuneConfig",
        "importPath": "finetune_imdb",
        "description": "finetune_imdb",
        "isExtraImport": true,
        "detail": "finetune_imdb",
        "documentation": {}
    },
    {
        "label": "load_imdb_data",
        "importPath": "finetune_imdb",
        "description": "finetune_imdb",
        "isExtraImport": true,
        "detail": "finetune_imdb",
        "documentation": {}
    },
    {
        "label": "SentimentClassifier",
        "importPath": "finetune_imdb",
        "description": "finetune_imdb",
        "isExtraImport": true,
        "detail": "finetune_imdb",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "importPath": "qwen3_core_components",
        "description": "qwen3_core_components",
        "isExtraImport": true,
        "detail": "qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "move_inputs_to_device",
        "kind": 2,
        "importPath": "pytorch-paligemma.inference",
        "description": "pytorch-paligemma.inference",
        "peekOfCode": "def move_inputs_to_device(model_inputs: dict, device: str):\n    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n    return model_inputs\ndef get_model_inputs(\n    processor: PaliGemmaProcessor, prompt: str, image_file_path: str, device: str\n):\n    image = Image.open(image_file_path)\n    images = [image]\n    prompts = [prompt]\n    model_inputs = processor(text=prompts, images=images)",
        "detail": "pytorch-paligemma.inference",
        "documentation": {}
    },
    {
        "label": "get_model_inputs",
        "kind": 2,
        "importPath": "pytorch-paligemma.inference",
        "description": "pytorch-paligemma.inference",
        "peekOfCode": "def get_model_inputs(\n    processor: PaliGemmaProcessor, prompt: str, image_file_path: str, device: str\n):\n    image = Image.open(image_file_path)\n    images = [image]\n    prompts = [prompt]\n    model_inputs = processor(text=prompts, images=images)\n    model_inputs = move_inputs_to_device(model_inputs, device)\n    return model_inputs\ndef test_inference(",
        "detail": "pytorch-paligemma.inference",
        "documentation": {}
    },
    {
        "label": "test_inference",
        "kind": 2,
        "importPath": "pytorch-paligemma.inference",
        "description": "pytorch-paligemma.inference",
        "peekOfCode": "def test_inference(\n    model: PaliGemmaForConditionalGeneration,\n    processor: PaliGemmaProcessor,\n    device: str,\n    prompt: str,\n    image_file_path: str,\n    max_tokens_to_generate: int,\n    temperature: float,\n    top_p: float,\n    do_sample: bool,",
        "detail": "pytorch-paligemma.inference",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "pytorch-paligemma.inference",
        "description": "pytorch-paligemma.inference",
        "peekOfCode": "def main(\n    model_path: str = None,\n    prompt: str = None,\n    image_file_path: str = None,\n    max_tokens_to_generate: int = 100,\n    temperature: float = 0.8,\n    top_p: float = 0.9,\n    do_sample: bool = False,\n    only_cpu: bool = False,\n):",
        "detail": "pytorch-paligemma.inference",
        "documentation": {}
    },
    {
        "label": "KVCache",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class KVCache():\n    def __init__(self) -> None:\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n    def num_items(self) -> int:\n        if len(self.key_cache) == 0:\n            return 0\n        else:\n            # The shape of the key_cache is [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n            return self.key_cache[0].shape[-2]",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaConfig",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaConfig():\n    def __init__(\n        self,\n        vocab_size,\n        hidden_size,\n        intermediate_size,\n        num_hidden_layers,\n        num_attention_heads,\n        num_key_value_heads,\n        head_dim=256,",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "PaliGemmaConfig",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class PaliGemmaConfig():\n    def __init__(\n        self,\n        vision_config=None,\n        text_config=None,\n        ignore_index=-100,\n        image_token_index=256000,\n        vocab_size=257152,\n        projection_dim=2048,\n        hidden_size=2048,",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaRMSNorm",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaRMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.zeros(dim))\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n    def forward(self, x):\n        output = self._norm(x.float())\n        # Llama does x.to(float16) * w whilst Gemma is (x * w).to(float16)",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaRotaryEmbedding",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n        self.dim = dim # it is set to the head_dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        # Calculate the theta according to the formula theta_i = base^(-2i/dim) where i = 0, 1, 2, ..., dim // 2\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n    @torch.no_grad()",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaMLP",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n    def forward(self, x):",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaAttention",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaAttention(nn.Module):\n    def __init__(self, config: GemmaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = config.head_dim\n        self.num_key_value_heads = config.num_key_value_heads",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaDecoderLayer",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaDecoderLayer(nn.Module):\n    def __init__(self, config: GemmaConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = GemmaAttention(config=config, layer_idx=layer_idx)\n        self.mlp = GemmaMLP(config)\n        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n    def forward(\n        self,",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaModel",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaModel(nn.Module):\n    def __init__(self, config: GemmaConfig):\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList(\n            [GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "GemmaForCausalLM",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class GemmaForCausalLM(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.model = GemmaModel(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n    def tie_weights(self):",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "PaliGemmaMultiModalProjector",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class PaliGemmaMultiModalProjector(nn.Module):\n    def __init__(self, config: PaliGemmaConfig):\n        super().__init__()\n        self.linear = nn.Linear(config.vision_config.hidden_size, config.vision_config.projection_dim, bias=True)\n    def forward(self, image_features):\n        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Projection_Dim]\n        hidden_states = self.linear(image_features)\n        return hidden_states\nclass PaliGemmaForConditionalGeneration(nn.Module):\n    def __init__(self, config: PaliGemmaConfig):",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "PaliGemmaForConditionalGeneration",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "class PaliGemmaForConditionalGeneration(nn.Module):\n    def __init__(self, config: PaliGemmaConfig):\n        super().__init__()\n        self.config = config\n        self.vision_tower = SiglipVisionModel(config.vision_config)\n        self.multi_modal_projector = PaliGemmaMultiModalProjector(config)\n        self.vocab_size = config.vocab_size\n        language_model = GemmaForCausalLM(config.text_config)\n        self.language_model = language_model\n        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "rotate_half",
        "kind": 2,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "def rotate_half(x):\n    # Build the [-x2, x1, -x4, x3, ...] tensor for the sin part of the positional encoding.\n    x1 = x[..., : x.shape[-1] // 2] # Takes the first half of the last dimension\n    x2 = x[..., x.shape[-1] // 2 :] # Takes the second half of the last dimension\n    return torch.cat((-x2, x1), dim=-1)\ndef apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n    cos = cos.unsqueeze(unsqueeze_dim) # Add the head dimension\n    sin = sin.unsqueeze(unsqueeze_dim) # Add the head dimension\n    # Apply the formula (34) of the Rotary Positional Encoding paper.\n    q_embed = (q * cos) + (rotate_half(q) * sin)",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "apply_rotary_pos_emb",
        "kind": 2,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n    cos = cos.unsqueeze(unsqueeze_dim) # Add the head dimension\n    sin = sin.unsqueeze(unsqueeze_dim) # Add the head dimension\n    # Apply the formula (34) of the Rotary Positional Encoding paper.\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\nclass GemmaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "pytorch-paligemma.modeling_gemma",
        "description": "pytorch-paligemma.modeling_gemma",
        "peekOfCode": "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\nclass GemmaAttention(nn.Module):\n    def __init__(self, config: GemmaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config",
        "detail": "pytorch-paligemma.modeling_gemma",
        "documentation": {}
    },
    {
        "label": "SiglipVisionConfig",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipVisionConfig:\n    def __init__(\n        self,\n        hidden_size=768,\n        intermediate_size=3072,\n        num_hidden_layers=12,\n        num_attention_heads=12,\n        num_channels=3,\n        image_size=224,\n        patch_size=16,",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipVisionEmbeddings",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipVisionEmbeddings(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.image_size = config.image_size\n        self.patch_size = config.patch_size\n        self.patch_embedding = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=self.embed_dim,",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipAttention",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n        self.scale = self.head_dim**-0.5 # Equivalent to 1 / sqrt(self.head_dim)\n        self.dropout = config.attention_dropout",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipMLP",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Intermediate_Size]\n        hidden_states = self.fc1(hidden_states)\n        # hidden_states: [Batch_Size, Num_Patches, Intermediate_Size]",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipEncoderLayer",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipEncoderLayer(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.embed_dim = config.hidden_size\n        self.self_attn = SiglipAttention(config)\n        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n        self.mlp = SiglipMLP(config)\n        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n    # Ignore copy\n    def forward(",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipEncoder",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipEncoder(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.layers = nn.ModuleList(\n            [SiglipEncoderLayer(config) for _ in range(config.num_hidden_layers)]\n        )\n    # Ignore copy\n    def forward(\n        self,",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipVisionTransformer",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipVisionTransformer(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        embed_dim = config.hidden_size\n        self.embeddings = SiglipVisionEmbeddings(config)\n        self.encoder = SiglipEncoder(config)\n        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n        # pixel_values: [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "SiglipVisionModel",
        "kind": 6,
        "importPath": "pytorch-paligemma.modeling_siglip",
        "description": "pytorch-paligemma.modeling_siglip",
        "peekOfCode": "class SiglipVisionModel(nn.Module):\n    def __init__(self, config: SiglipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.vision_model = SiglipVisionTransformer(config)\n    def forward(self, pixel_values) -> Tuple:\n        # [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]\n        return self.vision_model(pixel_values=pixel_values)",
        "detail": "pytorch-paligemma.modeling_siglip",
        "documentation": {}
    },
    {
        "label": "PaliGemmaProcessor",
        "kind": 6,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "class PaliGemmaProcessor:\n    IMAGE_TOKEN = \"<image>\"\n    def __init__(self, tokenizer, num_image_tokens: int, image_size: int):\n        super().__init__()\n        self.image_seq_length = num_image_tokens\n        self.image_size = image_size\n        # Tokenizer described here: https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md#tokenizer\n        tokens_to_add = {\"additional_special_tokens\": [self.IMAGE_TOKEN]}\n        tokenizer.add_special_tokens(tokens_to_add)\n        EXTRA_TOKENS = [",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "add_image_tokens_to_prompt",
        "kind": 2,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "def add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token):\n    # Quoting from the blog (https://huggingface.co/blog/paligemma#detailed-inference-process):\n    #   The input text is tokenized normally.\n    #   A <bos> token is added at the beginning, and an additional newline token (\\n) is appended.\n    #   This newline token is an essential part of the input prompt the model was trained with, so adding it explicitly ensures it's always there.\n    #   The tokenized text is also prefixed with a fixed number of <image> tokens.\n    # NOTE: from the paper it looks like the `\\n` should be tokenized separately, but in the HF implementation this is not done.\n    #       ref to HF implementation: https://github.com/huggingface/transformers/blob/7f79a97399bb52aad8460e1da2f36577d5dccfed/src/transformers/models/paligemma/processing_paligemma.py#L55-L73\n    return f\"{image_token * image_seq_len}{bos_token}{prefix_prompt}\\n\"\ndef rescale(",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "rescale",
        "kind": 2,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "def rescale(\n    image: np.ndarray, scale: float, dtype: np.dtype = np.float32\n) -> np.ndarray:\n    rescaled_image = image * scale\n    rescaled_image = rescaled_image.astype(dtype)\n    return rescaled_image\ndef resize(\n    image: Image,\n    size: Tuple[int, int],\n    resample: Image.Resampling = None,",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 2,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "def resize(\n    image: Image,\n    size: Tuple[int, int],\n    resample: Image.Resampling = None,\n    reducing_gap: Optional[int] = None,\n) -> np.ndarray:\n    height, width = size\n    resized_image = image.resize(\n        (width, height), resample=resample, reducing_gap=reducing_gap\n    )",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "normalize",
        "kind": 2,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "def normalize(\n    image: np.ndarray,\n    mean: Union[float, Iterable[float]],\n    std: Union[float, Iterable[float]],\n) -> np.ndarray:\n    mean = np.array(mean, dtype=image.dtype)\n    std = np.array(std, dtype=image.dtype)\n    image = (image - mean) / std\n    return image\ndef process_images(",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "process_images",
        "kind": 2,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "def process_images(\n    images: List[Image.Image],\n    size: Dict[str, int] = None,\n    resample: Image.Resampling = None,\n    rescale_factor: float = None,\n    image_mean: Optional[Union[float, List[float]]] = None,\n    image_std: Optional[Union[float, List[float]]] = None,\n) -> List[np.ndarray]:\n    height, width = size[0], size[1]\n    images = [",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "IMAGENET_STANDARD_MEAN",
        "kind": 5,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "IMAGENET_STANDARD_MEAN = [0.5, 0.5, 0.5]\nIMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]\ndef add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token):\n    # Quoting from the blog (https://huggingface.co/blog/paligemma#detailed-inference-process):\n    #   The input text is tokenized normally.\n    #   A <bos> token is added at the beginning, and an additional newline token (\\n) is appended.\n    #   This newline token is an essential part of the input prompt the model was trained with, so adding it explicitly ensures it's always there.\n    #   The tokenized text is also prefixed with a fixed number of <image> tokens.\n    # NOTE: from the paper it looks like the `\\n` should be tokenized separately, but in the HF implementation this is not done.\n    #       ref to HF implementation: https://github.com/huggingface/transformers/blob/7f79a97399bb52aad8460e1da2f36577d5dccfed/src/transformers/models/paligemma/processing_paligemma.py#L55-L73",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "IMAGENET_STANDARD_STD",
        "kind": 5,
        "importPath": "pytorch-paligemma.processing_paligemma",
        "description": "pytorch-paligemma.processing_paligemma",
        "peekOfCode": "IMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]\ndef add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token):\n    # Quoting from the blog (https://huggingface.co/blog/paligemma#detailed-inference-process):\n    #   The input text is tokenized normally.\n    #   A <bos> token is added at the beginning, and an additional newline token (\\n) is appended.\n    #   This newline token is an essential part of the input prompt the model was trained with, so adding it explicitly ensures it's always there.\n    #   The tokenized text is also prefixed with a fixed number of <image> tokens.\n    # NOTE: from the paper it looks like the `\\n` should be tokenized separately, but in the HF implementation this is not done.\n    #       ref to HF implementation: https://github.com/huggingface/transformers/blob/7f79a97399bb52aad8460e1da2f36577d5dccfed/src/transformers/models/paligemma/processing_paligemma.py#L55-L73\n    return f\"{image_token * image_seq_len}{bos_token}{prefix_prompt}\\n\"",
        "detail": "pytorch-paligemma.processing_paligemma",
        "documentation": {}
    },
    {
        "label": "load_hf_model",
        "kind": 2,
        "importPath": "pytorch-paligemma.utils",
        "description": "pytorch-paligemma.utils",
        "peekOfCode": "def load_hf_model(model_path: str, device: str) -> Tuple[PaliGemmaForConditionalGeneration, AutoTokenizer]:\n    # Load the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n    assert tokenizer.padding_side == \"right\"\n    # Find all the *.safetensors files\n    safetensors_files = glob.glob(os.path.join(model_path, \"*.safetensors\"))\n    # ... and load them one by one in the tensors dictionary\n    tensors = {}\n    for safetensors_file in safetensors_files:\n        with safe_open(safetensors_file, framework=\"pt\", device=\"cpu\") as f:",
        "detail": "pytorch-paligemma.utils",
        "documentation": {}
    },
    {
        "label": "SmallModelConfig",
        "kind": 6,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "class SmallModelConfig:\n    \"\"\"\n     SMALL CONFIGURATION FOR FAST TRAINING\n    This configuration is optimized for:\n    - Fast training on CPU/limited GPU\n    - Learning the concepts without waiting hours\n    - Still demonstrating all key components\n    \"\"\"\n    # Model architecture - MUCH SMALLER\n    d_model: int = 128          # Reduced from 384 (3x smaller)",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "Muon",
        "kind": 6,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "class Muon(torch.optim.Optimizer):\n    \"\"\"\n     MUON OPTIMIZER: MomentUm Orthogonalized by Newton-schulz\n    This is a revolutionary optimizer that combines:\n    1. Momentum (like Adam) - remembers past gradients\n    2. Orthogonalization (Newton-Schulz) - makes gradients \"well-behaved\"\n    3. Adaptive learning rates - adjusts based on matrix dimensions\n     Why Muon is special:\n    - 30-50% faster convergence than Adam\n    - More stable training (fewer gradient explosions)",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "TextTokenDataset",
        "kind": 6,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "class TextTokenDataset(Dataset):\n    \"\"\"\n     CUSTOM DATASET FOR LANGUAGE MODELING\n    This creates training examples for our language model:\n     What it does:\n    - Takes a long sequence of tokens\n    - Creates sliding windows of fixed length\n    - Each example: input sequence + target sequence (shifted by 1)\n     Example:\n    Original text: \"The cat sat on the mat\"",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def set_seed(seed: int = 42):\n    \"\"\"Set all random seeds for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f\" Set all seeds to {seed}\")\n@dataclass",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n     KEY COMPONENT: Grouped-Query Attention (GQA)\n    GQA is a memory-efficient attention mechanism where:\n    - We have fewer Key-Value heads than Query heads\n    - Each KV head is \"repeated\" to match the number of Query heads\n    - This reduces memory usage while maintaining performance\n    Example:\n    - 4 Query heads, 2 KV heads  each KV head repeated 2 times\n    - Memory savings: 50% reduction in KV cache memory",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "zeropower_via_newtonschulz5",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n    \"\"\"\n     NEWTON-SCHULZ ORTHOGONALIZATION\n    This is the mathematical heart of the Muon optimizer:\n     What it does:\n    - Takes a matrix G (gradients)\n    - Makes it \"orthogonal\" (like rotating it to be perfectly aligned)\n    - Uses Newton-Schulz iteration (a fast numerical method)\n     Why orthogonalization helps:\n    - Orthogonal matrices preserve vector lengths and angles",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "load_and_cache_data",
        "kind": 2,
        "importPath": "qwen-llm.config.qwen3_small_config",
        "description": "qwen-llm.config.qwen3_small_config",
        "peekOfCode": "def load_and_cache_data(config: SmallModelConfig, cache_dir: str = \"data_cache\"):\n    \"\"\"\n     SMART DATA LOADING WITH CACHING\n    This function demonstrates modern ML data handling:\n     Key features:\n    1. Caching: Avoids reprocessing the same data\n    2. Streaming: Loads large datasets without memory issues\n    3. Tokenization: Converts text to numbers the model can understand\n    4. Efficient storage: Uses pickle for fast loading\n     The process:",
        "detail": "qwen-llm.config.qwen3_small_config",
        "documentation": {}
    },
    {
        "label": "BenchmarkConfig",
        "kind": 6,
        "importPath": "qwen-llm.benchmark_quantization",
        "description": "qwen-llm.benchmark_quantization",
        "peekOfCode": "class BenchmarkConfig:\n    \"\"\"\n     BENCHMARK CONFIGURATION\n    \"\"\"\n    # Model parameters\n    d_model: int = 128\n    n_layers: int = 3\n    vocab_size: int = 1000\n    # Benchmark parameters\n    num_samples: int = 100",
        "detail": "qwen-llm.benchmark_quantization",
        "documentation": {}
    },
    {
        "label": "QuantizationBenchmarker",
        "kind": 6,
        "importPath": "qwen-llm.benchmark_quantization",
        "description": "qwen-llm.benchmark_quantization",
        "peekOfCode": "class QuantizationBenchmarker:\n    \"\"\"\n     QUANTIZATION BENCHMARKER\n    Comprehensive benchmarking of different quantization techniques.\n    \"\"\"\n    def __init__(self, config: BenchmarkConfig):\n        self.config = config\n        self.results = {}\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        # Create test data",
        "detail": "qwen-llm.benchmark_quantization",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.benchmark_quantization",
        "description": "qwen-llm.benchmark_quantization",
        "peekOfCode": "def main():\n    \"\"\"Main benchmark function\"\"\"\n    print(\" QUANTIZATION BENCHMARK\")\n    print(\"=\" * 40)\n    # Create benchmark config\n    config = BenchmarkConfig()\n    config.d_model = 128\n    config.n_layers = 3\n    config.vocab_size = 1000\n    config.num_samples = 100",
        "detail": "qwen-llm.benchmark_quantization",
        "documentation": {}
    },
    {
        "label": "convert_checkpoint",
        "kind": 2,
        "importPath": "qwen-llm.convert_checkpoint",
        "description": "qwen-llm.convert_checkpoint",
        "peekOfCode": "def convert_checkpoint(old_path: str, new_path: str):\n    \"\"\"\n    Convert old checkpoint to new format\n    \"\"\"\n    print(f\" Converting checkpoint from {old_path} to {new_path}\")\n    try:\n        # Try to load the old checkpoint\n        print(\" Loading old checkpoint...\")\n        checkpoint = torch.load(old_path, map_location='cpu', weights_only=True)\n        # Extract only the essential data",
        "detail": "qwen-llm.convert_checkpoint",
        "documentation": {}
    },
    {
        "label": "demo_finetune",
        "kind": 2,
        "importPath": "qwen-llm.demo_finetune",
        "description": "qwen-llm.demo_finetune",
        "peekOfCode": "def demo_finetune():\n    \"\"\"\n     DEMO FINE-TUNING PROCESS\n    Demonstrates the fine-tuning process with a small subset of data.\n    \"\"\"\n    print(\" IMDB Sentiment Analysis Fine-tuning Demo\")\n    print(\"=\" * 50)\n    # Create config for demo (smaller dataset, fewer epochs)\n    config = FineTuneConfig()\n    config.batch_size = 8",
        "detail": "qwen-llm.demo_finetune",
        "documentation": {}
    },
    {
        "label": "FineTuneConfig",
        "kind": 6,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "class FineTuneConfig:\n    \"\"\"\n     FINE-TUNING CONFIGURATION\n    Configuration for fine-tuning the pre-trained model on IMDB sentiment analysis.\n    \"\"\"\n    # Model architecture\n    d_model: int = 128\n    n_heads: int = 4\n    n_layers: int = 3\n    d_ff: int = 512",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "IMDBDataset",
        "kind": 6,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "class IMDBDataset(Dataset):\n    \"\"\"\n     IMDB DATASET CLASS\n    Custom dataset class for IMDB sentiment analysis.\n    Handles tokenization and padding of movie reviews.\n    \"\"\"\n    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "SentimentClassifier",
        "kind": 6,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "class SentimentClassifier(nn.Module):\n    \"\"\"\n     SENTIMENT CLASSIFICATION MODEL\n    This model adds a classification head on top of our pre-trained Qwen3 model.\n    It freezes the pre-trained weights and only trains the classification layer.\n    \"\"\"\n    def __init__(self, pretrained_model: MinimalLLM, num_classes: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Freeze pre-trained model parameters",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "load_imdb_data",
        "kind": 2,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "def load_imdb_data(config: FineTuneConfig):\n    \"\"\"\n     LOAD IMDB DATASET\n    Loads and prepares the IMDB sentiment analysis dataset.\n    \"\"\"\n    print(\" Loading IMDB dataset...\")\n    # Load dataset\n    dataset = load_dataset(\"imdb\")\n    # Load tokenizer (same as used in pre-training)\n    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "def evaluate_model(model: SentimentClassifier, test_loader: DataLoader, config: FineTuneConfig):\n    \"\"\"\n     EVALUATE MODEL PERFORMANCE\n    Evaluates the model on the test set and returns accuracy and loss.\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "fine_tune_model",
        "kind": 2,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "def fine_tune_model(config: FineTuneConfig, pretrained_model_path: str):\n    \"\"\"\n     FINE-TUNE MODEL ON IMDB SENTIMENT ANALYSIS\n    Fine-tunes the pre-trained model on IMDB sentiment analysis task.\n    \"\"\"\n    print(\" Starting IMDB Sentiment Analysis Fine-tuning\")\n    print(\"=\" * 60)\n    # Set seed for reproducibility\n    set_seed(42)\n    # Load data",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "test_sentiment_classifier",
        "kind": 2,
        "importPath": "qwen-llm.finetune_imdb",
        "description": "qwen-llm.finetune_imdb",
        "peekOfCode": "def test_sentiment_classifier(model_path: str = \"models/imdb_sentiment_classifier.pt\"):\n    \"\"\"\n     TEST SENTIMENT CLASSIFIER\n    Tests the fine-tuned sentiment classifier on sample texts.\n    \"\"\"\n    print(\" Testing Sentiment Classifier\")\n    print(\"=\" * 40)\n    # Load model\n    checkpoint = torch.load(model_path, map_location='cpu')\n    config = checkpoint['config']",
        "detail": "qwen-llm.finetune_imdb",
        "documentation": {}
    },
    {
        "label": "LoRATrainingConfig",
        "kind": 6,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "class LoRATrainingConfig:\n    \"\"\"\n     LORA TRAINING CONFIGURATION\n    \"\"\"\n    # Model parameters\n    pretrained_model_path: str = \"models/final_model1.pt\"\n    # LoRA parameters\n    lora_rank: int = 16\n    lora_alpha: float = 32.0\n    lora_dropout: float = 0.1",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "LoRADataset",
        "kind": 6,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "class LoRADataset(Dataset):\n    \"\"\"\n     LORA DATASET CLASS\n    Custom dataset for LoRA fine-tuning on IMDB sentiment analysis.\n    \"\"\"\n    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 256):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "LoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "class LoRAClassifier(nn.Module):\n    \"\"\"\n     LORA CLASSIFIER\n    Combines pre-trained model with LoRA adaptation and classification head.\n    \"\"\"\n    def __init__(self, pretrained_model: MinimalLLM, num_classes: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Add classification head\n        self.classifier = nn.Sequential(",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "load_data_for_lora",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def load_data_for_lora(config: LoRATrainingConfig):\n    \"\"\"\n     LOAD DATA FOR LORA FINE-TUNING\n    \"\"\"\n    print(\" Loading data for LoRA fine-tuning...\")\n    # Load dataset\n    if config.dataset_name == \"imdb\":\n        dataset = load_dataset(\"imdb\")\n        train_texts = dataset['train']['text'][:config.num_samples]\n        train_labels = dataset['train']['label'][:config.num_samples]",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "setup_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def setup_lora_model(config: LoRATrainingConfig, tokenizer):\n    \"\"\"\n     SETUP LORA MODEL\n    Loads pre-trained model and applies LoRA adaptation.\n    \"\"\"\n    print(f\" Setting up LoRA model...\")\n    # Load pre-trained model\n    print(f\" Loading pre-trained model from {config.pretrained_model_path}\")\n    checkpoint = torch.load(config.pretrained_model_path, map_location='cpu')\n    # Create model config",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "train_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def train_lora_model(classifier: LoRAClassifier, lora_manager: LoRAManager, \n                    train_loader: DataLoader, test_loader: DataLoader, config: LoRATrainingConfig):\n    \"\"\"\n     TRAIN LORA MODEL\n    Fine-tunes the model using LoRA adaptation.\n    \"\"\"\n    print(f\" Starting LoRA fine-tuning...\")\n    # Setup optimizer (only for trainable parameters)\n    trainable_params = lora_manager.get_trainable_parameters()\n    trainable_params.extend([p for p in classifier.classifier.parameters()])",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "evaluate_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def evaluate_lora_model(classifier: LoRAClassifier, test_loader: DataLoader, config: LoRATrainingConfig):\n    \"\"\"\n     EVALUATE LORA MODEL\n    \"\"\"\n    classifier.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "save_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def save_lora_model(classifier: LoRAClassifier, lora_manager: LoRAManager, \n                   config: LoRATrainingConfig, final_accuracy: float):\n    \"\"\"\n     SAVE LORA MODEL\n    Saves the LoRA-adapted model and LoRA weights separately.\n    \"\"\"\n    print(f\" Saving LoRA model...\")\n    # Save LoRA weights\n    lora_weights = {}\n    for name, lora_layer in lora_manager.lora_layers.items():",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "test_lora_model",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def test_lora_model(model_path: str = \"models/lora_sentiment_classifier.pt\"):\n    \"\"\"\n     TEST LORA MODEL\n    Tests the LoRA fine-tuned model on sample texts.\n    \"\"\"\n    print(\" Testing LoRA Model\")\n    print(\"=\" * 40)\n    # Load model\n    checkpoint = torch.load(model_path, map_location='cpu')\n    config = checkpoint['config']",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.lora_finetune",
        "description": "qwen-llm.lora_finetune",
        "peekOfCode": "def main():\n    \"\"\"\n     MAIN LORA FINE-TUNING FUNCTION\n    \"\"\"\n    print(\" LORA FINE-TUNING FOR SENTIMENT ANALYSIS\")\n    print(\"=\" * 60)\n    # Set seed\n    set_seed(42)\n    # Create config\n    config = LoRATrainingConfig()",
        "detail": "qwen-llm.lora_finetune",
        "documentation": {}
    },
    {
        "label": "QLoRATrainingConfig",
        "kind": 6,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "class QLoRATrainingConfig:\n    \"\"\"\n     QLORA TRAINING CONFIGURATION\n    \"\"\"\n    # Model parameters\n    pretrained_model_path: str = \"models/final_model1.pt\"\n    # QLoRA parameters\n    lora_rank: int = 16\n    lora_alpha: float = 32.0\n    lora_dropout: float = 0.1",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "QLoRADataset",
        "kind": 6,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "class QLoRADataset(Dataset):\n    \"\"\"\n     QLORA DATASET CLASS\n    Custom dataset for QLoRA fine-tuning on IMDB sentiment analysis.\n    \"\"\"\n    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 256):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "QLoRAClassifier",
        "kind": 6,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "class QLoRAClassifier(nn.Module):\n    \"\"\"\n     QLORA CLASSIFIER\n    Combines pre-trained model with QLoRA adaptation and classification head.\n    \"\"\"\n    def __init__(self, pretrained_model: MinimalLLM, num_classes: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.pretrained_model = pretrained_model\n        # Add classification head\n        self.classifier = nn.Sequential(",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "load_data_for_qlora",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def load_data_for_qlora(config: QLoRATrainingConfig):\n    \"\"\"\n     LOAD DATA FOR QLORA FINE-TUNING\n    \"\"\"\n    print(\" Loading data for QLoRA fine-tuning...\")\n    # Load dataset\n    if config.dataset_name == \"imdb\":\n        dataset = load_dataset(\"imdb\")\n        train_texts = dataset['train']['text'][:config.num_samples]\n        train_labels = dataset['train']['label'][:config.num_samples]",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "setup_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def setup_qlora_model(config: QLoRATrainingConfig, tokenizer):\n    \"\"\"\n     SETUP QLORA MODEL\n    Loads pre-trained model and applies QLoRA adaptation.\n    \"\"\"\n    print(f\" Setting up QLoRA model...\")\n    # Load pre-trained model\n    print(f\" Loading pre-trained model from {config.pretrained_model_path}\")\n    checkpoint = torch.load(config.pretrained_model_path, map_location='cpu')\n    # Create model config",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "train_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def train_qlora_model(classifier: QLoRAClassifier, qlora_manager: QLoRAManager, \n                      train_loader: DataLoader, test_loader: DataLoader, config: QLoRATrainingConfig):\n    \"\"\"\n     TRAIN QLORA MODEL\n    Fine-tunes the model using QLoRA adaptation.\n    \"\"\"\n    print(f\" Starting QLoRA fine-tuning...\")\n    # Setup optimizer (only for trainable parameters)\n    trainable_params = qlora_manager.get_trainable_parameters()\n    trainable_params.extend([p for p in classifier.classifier.parameters()])",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "evaluate_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def evaluate_qlora_model(classifier: QLoRAClassifier, test_loader: DataLoader, config: QLoRATrainingConfig):\n    \"\"\"\n     EVALUATE QLORA MODEL\n    \"\"\"\n    classifier.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "save_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def save_qlora_model(classifier: QLoRAClassifier, qlora_manager: QLoRAManager, \n                     config: QLoRATrainingConfig, final_accuracy: float):\n    \"\"\"\n     SAVE QLORA MODEL\n    Saves the QLoRA-adapted model and QLoRA weights separately.\n    \"\"\"\n    print(f\" Saving QLoRA model...\")\n    # Save QLoRA weights\n    qlora_weights = {}\n    for name, qlora_layer in qlora_manager.qlora_layers.items():",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "test_qlora_model",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def test_qlora_model(model_path: str = \"models/qlora_sentiment_classifier.pt\"):\n    \"\"\"\n     TEST QLORA MODEL\n    Tests the QLoRA fine-tuned model on sample texts.\n    \"\"\"\n    print(\" Testing QLoRA Model\")\n    print(\"=\" * 40)\n    # Load model\n    checkpoint = torch.load(model_path, map_location='cpu')\n    config = checkpoint['config']",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.qlora_finetune",
        "description": "qwen-llm.qlora_finetune",
        "peekOfCode": "def main():\n    \"\"\"\n     MAIN QLORA FINE-TUNING FUNCTION\n    \"\"\"\n    print(\" QLORA FINE-TUNING FOR SENTIMENT ANALYSIS\")\n    print(\"=\" * 60)\n    # Set seed\n    set_seed(42)\n    # Create config\n    config = QLoRATrainingConfig()",
        "detail": "qwen-llm.qlora_finetune",
        "documentation": {}
    },
    {
        "label": "QuantizationConfig",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QuantizationConfig:\n    \"\"\"\n     QUANTIZATION CONFIGURATION\n    Configuration for different quantization techniques.\n    \"\"\"\n    # Basic quantization\n    bits: int = 8  # Number of bits for quantization (4, 8, 16)\n    symmetric: bool = True  # Symmetric vs asymmetric quantization\n    # LoRA parameters\n    lora_rank: int = 16  # Rank of LoRA adaptation",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationExpert",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QuantizationExpert:\n    \"\"\"\n     QUANTIZATION EXPERT CLASS\n    This class demonstrates different quantization techniques and their trade-offs.\n    \"\"\"\n    def __init__(self):\n        self.quantization_methods = {\n            'fp32': self._fp32_quantization,\n            'fp16': self._fp16_quantization,\n            'int8': self._int8_quantization,",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class LoRALayer(nn.Module):\n    \"\"\"\n     LORA LAYER IMPLEMENTATION\n    LoRA (Low-Rank Adaptation) decomposes weight updates into low-rank matrices.\n    Mathematical Foundation:\n    W = W + W = W + BA\n    Where:\n    - W: Original frozen weights\n    - B: Low-rank matrix (d  r)\n    - A: Low-rank matrix (r  k)",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class LoRALinear(nn.Module):\n    \"\"\"\n     LORA LINEAR LAYER\n    Combines original linear layer with LoRA adaptation.\n    \"\"\"\n    def __init__(self, original_layer: nn.Linear, rank: int = 16, alpha: float = 32.0, dropout: float = 0.1):\n        super().__init__()\n        self.original_layer = original_layer\n        self.lora = LoRALayer(\n            original_layer.in_features,",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "LoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class LoRAManager:\n    \"\"\"\n     LORA MANAGER\n    Manages LoRA adaptation for entire models.\n    \"\"\"\n    def __init__(self, model: nn.Module, config: QuantizationConfig):\n        self.model = model\n        self.config = config\n        self.lora_layers = {}\n        self.original_layers = {}",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALayer",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QLoRALayer(nn.Module):\n    \"\"\"\n     QLORA LAYER IMPLEMENTATION\n    QLoRA combines LoRA with 4-bit quantization for maximum efficiency.\n    Key Innovation:\n    - Quantizes base model to 4-bit\n    - Uses LoRA for adaptation\n    - Enables fine-tuning on consumer hardware\n    Memory Savings:\n    - 4-bit quantization: 8x reduction",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRALinear",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QLoRALinear(nn.Module):\n    \"\"\"\n     QLORA LINEAR LAYER\n    Combines quantized base weights with LoRA adaptation.\n    \"\"\"\n    def __init__(self, original_layer: nn.Linear, rank: int = 16, alpha: float = 32.0, \n                 dropout: float = 0.1, bits: int = 4):\n        super().__init__()\n        self.original_layer = original_layer\n        self.qlora = QLoRALayer(",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QLoRAManager",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QLoRAManager:\n    \"\"\"\n     QLORA MANAGER\n    Manages QLoRA adaptation for entire models.\n    \"\"\"\n    def __init__(self, model: nn.Module, config: QuantizationConfig):\n        self.model = model\n        self.config = config\n        self.qlora_layers = {}\n        self.original_layers = {}",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "QuantizationBenchmark",
        "kind": 6,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "class QuantizationBenchmark:\n    \"\"\"\n     QUANTIZATION BENCHMARK\n    Comprehensive benchmarking of different quantization techniques.\n    \"\"\"\n    def __init__(self):\n        self.results = {}\n    def benchmark_model(self, model: nn.Module, test_data: torch.Tensor, \n                       configs: List[QuantizationConfig]) -> Dict:\n        \"\"\"",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "demo_quantization",
        "kind": 2,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "def demo_quantization():\n    \"\"\"\n     QUANTIZATION DEMO\n    Demonstrates different quantization techniques on our Qwen3 model.\n    \"\"\"\n    print(\" QUANTIZATION EXPERT TUTORIAL\")\n    print(\"=\" * 50)\n    # Create a small model for demo\n    config = SmallModelConfig()\n    config.d_model = 64  # Smaller for demo",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "demo_lora",
        "kind": 2,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "def demo_lora():\n    \"\"\"\n     LORA DEMO\n    Demonstrates LoRA adaptation on our Qwen3 model.\n    \"\"\"\n    print(\"\\n LORA (LOW-RANK ADAPTATION) DEMO\")\n    print(\"=\" * 50)\n    # Create model\n    config = SmallModelConfig()\n    config.d_model = 128",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "demo_qlora",
        "kind": 2,
        "importPath": "qwen-llm.quantization_tutorial",
        "description": "qwen-llm.quantization_tutorial",
        "peekOfCode": "def demo_qlora():\n    \"\"\"\n     QLORA DEMO\n    Demonstrates QLoRA adaptation on our Qwen3 model.\n    \"\"\"\n    print(\"\\n QLORA (QUANTIZED LORA) DEMO\")\n    print(\"=\" * 50)\n    # Create model\n    config = SmallModelConfig()\n    config.d_model = 128",
        "detail": "qwen-llm.quantization_tutorial",
        "documentation": {}
    },
    {
        "label": "MinimalLLM",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "class MinimalLLM(nn.Module):\n    \"\"\"\n     COMPLETE QWEN3-STYLE LANGUAGE MODEL\n    This is the full language model that combines all components:\n     Architecture:\n    1. Token Embedding: Convert token IDs to vectors\n    2. Positional Dropout: Prevent overfitting on positions\n    3. Transformer Blocks: Stack of attention + feed-forward layers\n    4. Final Normalization: RMSNorm before output\n    5. Language Modeling Head: Convert vectors back to token probabilities",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def evaluate_model(model: nn.Module, val_loader: DataLoader, config: SmallModelConfig):\n    \"\"\"\n     MODEL EVALUATION FUNCTION\n    This function evaluates the model's performance on validation data:\n     Metrics Computed:\n    1. Loss: Cross-entropy loss (lower is better)\n    2. Accuracy: Percentage of correct next-token predictions\n    3. Perplexity: exp(loss) - measures model's \"surprise\" (lower is better)\n     Why these metrics matter:\n    - Loss: Direct measure of how well the model predicts",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "setup_muon_optimizer",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def setup_muon_optimizer(model: nn.Module, config: SmallModelConfig):\n    \"\"\"\n     HYBRID OPTIMIZER SETUP\n    This function sets up a hybrid optimization strategy:\n    - Muon optimizer for 2D parameters (attention and feed-forward weights)\n    - AdamW optimizer for other parameters (embeddings, norms, biases)\n     Why hybrid approach:\n    - Muon works best on 2D matrices (attention, feed-forward)\n    - AdamW is better for 1D parameters (embeddings, biases)\n    - This gives us the best of both worlds",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def load_checkpoint(model_path: str, config: SmallModelConfig):\n    \"\"\"\n     LOAD CHECKPOINT FOR RESUMING TRAINING\n    This function loads a previously trained model checkpoint and returns\n    the model, optimizers, schedulers, and training state.\n    \"\"\"\n    print(f\" Loading checkpoint from {model_path}\")\n    # Load checkpoint with safe loading to handle import path changes\n    try:\n        # Try loading with weights_only=False to handle custom classes",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "resume_training",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def resume_training(config: SmallModelConfig, train_loader: DataLoader, val_loader: DataLoader, checkpoint_path: str):\n    \"\"\"\n     RESUME TRAINING FROM CHECKPOINT\n    This function resumes training from a previously saved checkpoint.\n    It loads the model state and continues training from where it left off.\n    Args:\n        config: Model configuration\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        checkpoint_path: Path to the checkpoint file",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def train_model(config: SmallModelConfig, train_loader: DataLoader, val_loader: DataLoader):\n    \"\"\"\n     COMPLETE TRAINING LOOP\n    This is the heart of the training process, implementing:\n     Key Features:\n    1. Gradient Accumulation: Simulate larger batch sizes\n    2. Mixed Precision: Faster training with minimal accuracy loss\n    3. Learning Rate Scheduling: Warmup + cosine decay\n    4. Gradient Clipping: Prevent gradient explosions\n    5. Model Checkpointing: Save best model",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "generate_text",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_complete_model",
        "description": "qwen-llm.qwen3_complete_model",
        "peekOfCode": "def generate_text(model: nn.Module, tokenizer, prompt: str, max_length: int = 100,\n                 temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9):\n    \"\"\"\n     TEXT GENERATION FUNCTION\n    This function generates text using the trained model with advanced sampling:\n     Sampling Strategies:\n    1. Temperature Scaling: Controls randomness (0.1 = focused, 2.0 = random)\n    2. Top-k Sampling: Only consider top k most likely tokens\n    3. Top-p (Nucleus) Sampling: Consider tokens until cumulative probability reaches p\n     How it works:",
        "detail": "qwen-llm.qwen3_complete_model",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class Rotary(nn.Module):\n    \"\"\"\n     ROTARY POSITIONAL EMBEDDINGS (RoPE)\n    This is one of the most important innovations in modern transformers!\n     What RoPE does:\n    - Encodes position information by ROTATING vectors\n    - Unlike traditional positional embeddings that just ADD position info\n    - Allows the model to understand relative positions naturally\n     The Math:\n    - For each position i, we compute rotation angles based on i",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "Qwen3Attention",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class Qwen3Attention(nn.Module):\n    \"\"\"\n     GROUPED-QUERY ATTENTION (GQA) IMPLEMENTATION\n    This is the heart of modern transformer attention mechanisms!\n     Key Innovations:\n    1. Grouped-Query Attention: Fewer KV heads than Query heads\n    2. QK-Normalization: Normalizes queries and keys for stability\n    3. RoPE: Rotary positional embeddings\n    4. Scaled dot-product attention: The core attention mechanism\n     How it works:",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "SwiGLUFeedForward",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class SwiGLUFeedForward(nn.Module):\n    \"\"\"\n     SWIGLU FEED-FORWARD NETWORK\n    SwiGLU is a modern activation function that combines:\n    - Swish activation: x * sigmoid(x) (smooth, non-monotonic)\n    - GLU (Gated Linear Unit): element-wise multiplication with a gate\n     The Math:\n    SwiGLU(x) = Swish(W1(x))  W2(x)\n    Where:\n    - W1(x) is the \"gate\" (controls information flow)",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class TransformerBlock(nn.Module):\n    \"\"\"\n     TRANSFORMER BLOCK - THE BUILDING BLOCK OF MODERN LLMs\n    This combines all the components into a complete transformer layer:\n     Architecture:\n    1. Pre-norm attention (RMSNorm before attention)\n    2. Residual connection (x + attention(x))\n    3. Pre-norm feed-forward (RMSNorm before SwiGLU)\n    4. Residual connection (x + feedforward(x))\n     Pre-norm vs Post-norm:",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "class RMSNorm(nn.Module):\n    \"\"\"\n     RMSNorm - ROOT MEAN SQUARE NORMALIZATION\n    This is a modern alternative to LayerNorm that's more efficient:\n     The Math:\n    RMSNorm(x) = x / sqrt(mean(x) + ) * g\n    Where:\n    - x is the input\n    - mean(x) is the mean of squared values\n    -  is a small constant (1e-6)",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "qwen-llm.qwen3_core_components",
        "description": "qwen-llm.qwen3_core_components",
        "peekOfCode": "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n     GROUPED-QUERY ATTENTION HELPER\n    This implements the key innovation in GQA:\n    - Fewer Key-Value heads than Query heads\n    - Each KV head is \"shared\" across multiple Query heads\n    - Massive memory savings with minimal performance loss\n     Example:\n    - 8 Query heads, 2 KV heads\n    - KV head 1 is used by Query heads 1,2,3,4",
        "detail": "qwen-llm.qwen3_core_components",
        "documentation": {}
    },
    {
        "label": "QuantizedModelServer",
        "kind": 6,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "class QuantizedModelServer:\n    \"\"\"\n     QUANTIZED MODEL SERVER\n    Serves different types of quantized models for inference.\n    \"\"\"\n    def __init__(self, model_path: str, model_type: str = \"lora\"):\n        self.model_path = model_path\n        self.model_type = model_type\n        self.model = None\n        self.tokenizer = None",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "health_check",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return jsonify({'status': 'healthy', 'model_type': server.model_type})\n@app.route('/info', methods=['GET'])\ndef model_info():\n    \"\"\"Get model information\"\"\"\n    return jsonify(server.get_model_info())\n@app.route('/generate', methods=['POST'])\ndef generate():\n    \"\"\"Generate text endpoint\"\"\"",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "model_info",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def model_info():\n    \"\"\"Get model information\"\"\"\n    return jsonify(server.get_model_info())\n@app.route('/generate', methods=['POST'])\ndef generate():\n    \"\"\"Generate text endpoint\"\"\"\n    try:\n        data = request.get_json()\n        prompt = data.get('prompt', '')\n        max_length = data.get('max_length', 100)",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "generate",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def generate():\n    \"\"\"Generate text endpoint\"\"\"\n    try:\n        data = request.get_json()\n        prompt = data.get('prompt', '')\n        max_length = data.get('max_length', 100)\n        temperature = data.get('temperature', 0.8)\n        top_k = data.get('top_k', 50)\n        top_p = data.get('top_p', 0.9)\n        if not prompt:",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "classify",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def classify():\n    \"\"\"Classify sentiment endpoint\"\"\"\n    try:\n        data = request.get_json()\n        text = data.get('text', '')\n        if not text:\n            return jsonify({'error': 'Text is required'}), 400\n        start_time = time.time()\n        result = server.classify_sentiment(text)\n        classification_time = time.time() - start_time",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "benchmark",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def benchmark():\n    \"\"\"Benchmark model performance\"\"\"\n    try:\n        data = request.get_json()\n        num_samples = data.get('num_samples', 10)\n        max_length = data.get('max_length', 50)\n        # Benchmark text generation\n        prompts = [\n            \"The weather today is\",\n            \"I think that\",",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "def main():\n    \"\"\"Main function to start the server\"\"\"\n    global server\n    parser = argparse.ArgumentParser(description='Serve quantized models')\n    parser.add_argument('--model_path', required=True, help='Path to the quantized model')\n    parser.add_argument('--model_type', choices=['lora', 'qlora', 'quantized'], \n                       default='lora', help='Type of quantized model')\n    parser.add_argument('--host', default='0.0.0.0', help='Host to bind to')\n    parser.add_argument('--port', type=int, default=5000, help='Port to bind to')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "app = Flask(__name__)\nserver = None\n@app.route('/health', methods=['GET'])\ndef health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return jsonify({'status': 'healthy', 'model_type': server.model_type})\n@app.route('/info', methods=['GET'])\ndef model_info():\n    \"\"\"Get model information\"\"\"\n    return jsonify(server.get_model_info())",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "qwen-llm.serve_quantized",
        "description": "qwen-llm.serve_quantized",
        "peekOfCode": "server = None\n@app.route('/health', methods=['GET'])\ndef health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return jsonify({'status': 'healthy', 'model_type': server.model_type})\n@app.route('/info', methods=['GET'])\ndef model_info():\n    \"\"\"Get model information\"\"\"\n    return jsonify(server.get_model_info())\n@app.route('/generate', methods=['POST'])",
        "detail": "qwen-llm.serve_quantized",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"\n    global model, tokenizer, config\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file {model_path} not found!\")\n    print(f\" Loading model from {model_path}\")\n    # Load checkpoint",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "health_check",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def health_check():\n    \"\"\"\n     HEALTH CHECK ENDPOINT\n    Returns the health status of the model server.\n    \"\"\"\n    return jsonify({\n        'status': 'healthy',\n        'model_loaded': model is not None,\n        'device': str(next(model.parameters()).device) if model else None\n    })",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "generate",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def generate():\n    \"\"\"\n     TEXT GENERATION ENDPOINT\n    Generates text based on the provided prompt.\n    Expected JSON payload:\n    {\n        \"prompt\": \"Your text prompt here\",\n        \"max_length\": 100,\n        \"temperature\": 0.8,\n        \"top_k\": 50,",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "model_info",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def model_info():\n    \"\"\"\n     MODEL INFORMATION ENDPOINT\n    Returns information about the loaded model.\n    \"\"\"\n    if model is None:\n        return jsonify({'error': 'Model not loaded'}), 500\n    return jsonify({\n        'model_type': 'Qwen3-style Language Model',\n        'parameters': sum(p.numel() for p in model.parameters()),",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "home",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def home():\n    \"\"\"\n     HOME ENDPOINT\n    Returns a simple HTML interface for testing the model.\n    \"\"\"\n    return '''\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <title>Qwen3 Model Server</title>",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "def main():\n    \"\"\"\n     MAIN SERVER FUNCTION\n    Starts the Flask server to serve the Qwen3 model.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser(description='Serve Qwen3 model')\n    parser.add_argument('--model', default='models/final_model1.pt', help='Path to model file')\n    parser.add_argument('--host', default='0.0.0.0', help='Host to bind to')\n    parser.add_argument('--port', type=int, default=5003, help='Port to bind to')",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "app = Flask(__name__)\n# Global variables for model and tokenizer\nmodel = None\ntokenizer = None\nconfig = None\ndef load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "model = None\ntokenizer = None\nconfig = None\ndef load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"\n    global model, tokenizer, config\n    if not os.path.exists(model_path):",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "tokenizer = None\nconfig = None\ndef load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"\n    global model, tokenizer, config\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file {model_path} not found!\")",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "qwen-llm.serve_qwen3",
        "description": "qwen-llm.serve_qwen3",
        "peekOfCode": "config = None\ndef load_model(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     LOAD THE TRAINED MODEL\n    This function loads the trained model and tokenizer for serving.\n    \"\"\"\n    global model, tokenizer, config\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file {model_path} not found!\")\n    print(f\" Loading model from {model_path}\")",
        "detail": "qwen-llm.serve_qwen3",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "qwen-llm.train_qwen3",
        "description": "qwen-llm.train_qwen3",
        "peekOfCode": "def main(resume_from: str = None):\n    \"\"\"\n     MAIN TRAINING FUNCTION\n    This function orchestrates the entire training process:\n    1. System check and configuration\n    2. Data loading and preparation\n    3. Model training (from scratch or resume)\n    4. Results reporting\n    Args:\n        resume_from: Path to checkpoint to resume training from",
        "detail": "qwen-llm.train_qwen3",
        "documentation": {}
    },
    {
        "label": "demo_inference",
        "kind": 2,
        "importPath": "qwen-llm.train_qwen3",
        "description": "qwen-llm.train_qwen3",
        "peekOfCode": "def demo_inference(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     DEMO INFERENCE FUNCTION\n    This function demonstrates the trained model's capabilities\n    \"\"\"\n    print(\" Running inference demo\")\n    # Load model\n    if not os.path.exists(model_path):\n        print(f\" Model file {model_path} not found!\")\n        print(\" Please run training first with: python train_qwen3.py\")",
        "detail": "qwen-llm.train_qwen3",
        "documentation": {}
    },
    {
        "label": "interactive_inference",
        "kind": 2,
        "importPath": "qwen-llm.train_qwen3",
        "description": "qwen-llm.train_qwen3",
        "peekOfCode": "def interactive_inference(model_path: str = \"models/final_model1.pt\"):\n    \"\"\"\n     INTERACTIVE INFERENCE SESSION\n    This function allows you to interact with the trained model\n    \"\"\"\n    print(\" Starting interactive inference session\")\n    print(\"Type 'quit' to exit\")\n    # Load model\n    if not os.path.exists(model_path):\n        print(f\" Model file {model_path} not found!\")",
        "detail": "qwen-llm.train_qwen3",
        "documentation": {}
    },
    {
        "label": "convert_checkpoint",
        "kind": 2,
        "importPath": "convert_checkpoint",
        "description": "convert_checkpoint",
        "peekOfCode": "def convert_checkpoint(old_path: str, new_path: str):\n    \"\"\"\n    Convert old checkpoint to new format\n    \"\"\"\n    print(f\" Converting checkpoint from {old_path} to {new_path}\")\n    try:\n        # Try to load the old checkpoint\n        print(\" Loading old checkpoint...\")\n        checkpoint = torch.load(old_path, map_location='cpu', weights_only=False)\n        # Extract only the essential data",
        "detail": "convert_checkpoint",
        "documentation": {}
    }
]